%% ============================================================================
%% JOURNAL OF FINANCIAL DATA SCIENCE (JFDS) - RESEARCH ARTICLE
%% News-Enhanced Stock Price Forecasting with Hierarchical Model Training
%% ============================================================================

\documentclass[12pt]{article}

%% PACKAGES
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{float}
\usepackage{longtable}

%% No colored links - all black text
\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
    pdftitle={News-Enhanced Stock Price Forecasting}
}

%% JFDS Format: 1.5 spacing for readability
\onehalfspacing

\begin{document}

%% ============================================================================
%% TITLE PAGE
%% ============================================================================

\title{\textbf{News-Enhanced Stock Price Forecasting: A Multi-Source Textual Analysis Approach with Hierarchical Model Training}}

\author{
Harsh Milind Tirhekar$^{1}$, Atharva Vishwas Kulkarni$^{2}$, Arun Kumar Kuchibotla$^{3}$\\[0.5em]
$^{1}$\textit{The University of Texas at Austin}\\
$^{2}$\textit{Savitribai Phule Pune University}\\
$^{3}$\textit{Department of Statistics \& Data Science, Carnegie Mellon University}
}

\date{}

\maketitle

%% ============================================================================
%% ABSTRACT
%% ============================================================================

\begin{abstract}
\noindent
This study investigates whether news sentiment and textual features improve stock price forecasting. Using 26 years of Apple Inc. (AAPL) data (1999--2025) combined with 57 million financial news articles from HuggingFace and Google RSS feeds, we extract sentiment via VADER, TextBlob, and FinBERT, along with LDA topic features. We propose a hierarchical training strategy where foundational models (SARIMAX, Linear Regression, TCN) trained on full historical data provide predictions as input features for neural networks (LSTM, GRU, BiLSTM, CNN-LSTM, Transformer) trained on recent data. Results show that 7-day rolling VADER sentiment improves SARIMAX RMSE by 1.5\% ($p<0.05$). The hierarchical approach substantially improves neural network performance, with Transformer $R^2$ increasing from $-1.7$ to $0.87$. Linear regression achieves best overall performance (RMSE=\$1.83, $R^2$=0.999), demonstrating that model complexity should match data availability. Our findings have important implications for practitioners seeking to incorporate alternative data sources into quantitative trading strategies.
\end{abstract}

\newpage

%% ============================================================================
%% 1. INTRODUCTION
%% ============================================================================

\section{Introduction}

The efficient market hypothesis (EMH) suggests that asset prices fully reflect available information (Fama, 1970), making consistent outperformance through prediction theoretically challenging. However, extensive empirical research documents predictable patterns in stock returns, particularly around corporate events such as earnings announcements (Ball and Brown, 1968; Bernard and Thomas, 1989), merger announcements (Andrade et al., 2001), and product launches (Chaney et al., 1991). These patterns suggest that information incorporation is neither instantaneous nor complete, creating potential opportunities for forecasting.

The rise of natural language processing (NLP) and machine learning has enabled researchers to systematically extract information from unstructured text, including news articles, social media posts, and corporate filings (Loughran and McDonald, 2011; Gentzkow et al., 2019). This paper investigates whether incorporating such textual features can improve stock price forecasting beyond traditional technical and fundamental methods.

\subsection{Research Motivation}

Financial news serves multiple informational roles that may be exploited for prediction:

\textbf{Event-Driven Impact:} Major announcements (earnings, product launches, mergers) create immediate price reactions that may be partially anticipated through news sentiment (Tetlock, 2007). The sentiment expressed in news coverage leading up to events may signal market expectations.

\textbf{Sentiment Momentum:} Persistent positive or negative coverage may predict future price direction through behavioral channels (Baker and Wurgler, 2007). Investors may underreact to gradual sentiment shifts, creating exploitable trends.

\textbf{Information Diffusion:} News facilitates the gradual incorporation of information into prices, creating exploitable lead-lag relationships (Hong and Stein, 2000). Information may first appear in news before being fully reflected in prices.

\textbf{Attention Effects:} Media coverage affects investor attention and trading behavior (Engelberg and Parsons, 2011), potentially creating price pressure independent of fundamental information content.

\subsection{Research Questions}

We address the following research questions:

\textbf{RQ1:} Does news sentiment provide incremental predictive power for next-day stock prices beyond technical indicators?

\textbf{RQ2:} What is the optimal temporal aggregation (rolling window) for sentiment features to balance noise reduction against information lag?

\textbf{RQ3:} Can a hierarchical training strategy---where traditional models inform neural network inputs---overcome the limitations of training deep learning models on limited financial time series?

\textbf{RQ4:} How do different sentiment extraction methods (VADER, TextBlob, FinBERT) compare in forecasting performance?

\textbf{RQ5:} Can Transformer architectures, which have revolutionized NLP, be effectively adapted for financial time series forecasting?

\subsection{Main Contributions}

Our study makes the following contributions:

\textbf{Multi-source Data Integration:} We construct a comprehensive news corpus spanning 1999--2025 by combining HuggingFace financial news datasets with real-time Google RSS feeds, addressing the common limitation of short sample periods in financial machine learning research.

\textbf{Hierarchical Training Strategy:} We propose using predictions from models trained on long-term data as features for models trained on recent data, enabling neural networks to leverage historical patterns without suffering from distribution shift. This approach transforms Transformer performance from catastrophic failure ($R^2 = -1.7$) to competitive results ($R^2 = 0.87$).

\textbf{Systematic Sentiment Comparison:} We rigorously compare three sentiment extraction methods (VADER, TextBlob, FinBERT) across multiple rolling windows (3, 7, 14, 30 days), providing practical guidance for practitioners on optimal configurations.

\textbf{Transformer Diagnosis:} We demonstrate that the poor performance of Transformers in financial forecasting literature stems from architectural mismatch (single-step input causing attention degeneracy) rather than fundamental limitations of the architecture.

\textbf{Reproducibility:} We provide complete code, data processing pipelines, and execution logs for full reproducibility, addressing concerns about replication in financial machine learning research.

\subsection{Preview of Findings}

\textbf{Finding 1:} News sentiment provides statistically significant but economically modest improvements. The 7-day rolling mean of VADER sentiment reduces SARIMAX RMSE by 1.5\% ($p < 0.05$).

\textbf{Finding 2:} Model complexity should match sample size. With approximately 1,000 training observations, linear regression with 55 features outperforms neural networks with 50,000+ parameters.

\textbf{Finding 3:} The hierarchical training strategy substantially improves neural network performance, enabling Transformer $R^2$ to increase from $-1.7$ to $0.87$ and GRU $R^2$ from $0.72$ to $0.94$.

The remainder of this paper is organized as follows. Section 2 reviews related literature. Section 3 describes our data sources and methodology. Section 4 details feature engineering. Section 5 presents model architectures. Section 6 reports empirical results. Section 7 discusses limitations. Section 8 concludes.

%% ============================================================================
%% 2. LITERATURE REVIEW
%% ============================================================================

\section{Literature Review}

Our research connects to four strands of the finance and machine learning literature: market efficiency and information content, textual analysis in finance, machine learning for financial prediction, and transformer architectures.

\subsection{Market Efficiency and Information Content}

The efficient market hypothesis (Fama, 1970) posits that asset prices reflect all available information. Under the strong form, even private information is immediately incorporated. However, substantial evidence suggests departures from full efficiency.

Ball and Brown (1968) established that earnings surprises predict abnormal returns, initiating research on post-earnings announcement drift (PEAD). Bernard and Thomas (1989) demonstrated that this drift persists for months, challenging semi-strong efficiency. The magnitude of drift is larger for smaller firms with less analyst coverage, suggesting information processing frictions.

Jegadeesh and Titman (1993) documented momentum effects where past winners outperform past losers over 3-12 month horizons. This finding has proven remarkably robust across markets and time periods, with annual returns to momentum strategies averaging 12\% historically.

More recently, Hirshleifer et al. (2009) showed that investor inattention creates predictable patterns, particularly around earnings announcements. When multiple firms announce simultaneously, investors are distracted, and price reactions are delayed. Engelberg and Parsons (2011) provided causal evidence that news coverage affects stock returns and trading volume using newspaper strike-induced variation in coverage.

\subsection{Textual Analysis in Finance}

The application of textual analysis to financial data has grown substantially over the past two decades. Tetlock (2007) pioneered this approach by showing that negative words in Wall Street Journal ``Abreast of the Market'' columns predict lower next-day returns and higher trading volume. A one standard deviation increase in pessimism predicts a 0.03\% lower return.

Tetlock et al. (2008) extended this to firm-specific news, finding that negative words predict earnings surprises and returns around earnings announcements. The predictive power of negative words persists even after controlling for quantitative predictors.

Loughran and McDonald (2011) developed a finance-specific dictionary, demonstrating that generic sentiment dictionaries (like Harvard General Inquirer) perform poorly in financial contexts. They showed that words like ``liability'' and ``tax,'' which are negative in general usage, have neutral or positive meanings in finance. Their finance-specific negative word list has become standard in the literature.

Gentzkow et al. (2019) provided a comprehensive review of text analysis in economics and finance, emphasizing both opportunities and methodological challenges. They highlighted the importance of domain-specific approaches and the limitations of off-the-shelf NLP tools.

Recent work has applied transformer-based language models to financial text. Araci (2019) introduced FinBERT, a BERT model fine-tuned on financial communications, achieving state-of-the-art performance on financial sentiment classification. Yang et al. (2020) extended this with FinBERT-QA for financial question answering.

\subsection{Machine Learning for Stock Prediction}

Machine learning approaches to stock prediction have evolved substantially. Early work focused on support vector machines and random forests (Kara et al., 2011; Patel et al., 2015). These methods improved upon traditional econometric approaches but faced limitations in capturing complex temporal dependencies.

Fischer and Krauss (2018) applied LSTM networks to S\&P 500 constituents, finding that deep learning outperforms traditional methods for return prediction. Their model achieved an average daily return of 0.46\% before transaction costs. However, performance degraded significantly after 2010, suggesting regime changes or increased market efficiency.

Ding et al. (2015) combined convolutional neural networks with event embeddings, achieving improved directional accuracy. Their event embedding approach captures semantic similarity between different types of corporate events.

Xu and Cohen (2018) proposed StockNet, a variational autoencoder that jointly models price and tweet embeddings. Feng et al. (2019) introduced attention mechanisms for multi-scale temporal patterns in stock prediction.

However, Zhang et al. (2023) found that simpler models often outperform deep networks on financial datasets with fewer than 5,000 observations, highlighting the importance of matching model complexity to data availability. This ``less is more'' finding resonates with our results.

\subsection{Transformers for Time Series}

The transformer architecture (Vaswani et al., 2017) has revolutionized NLP and is increasingly applied to time series. The self-attention mechanism enables modeling long-range dependencies without the sequential processing constraints of RNNs.

Zhou et al. (2021) introduced Informer with ProbSparse attention for long-sequence forecasting, reducing complexity from $O(L^2)$ to $O(L \log L)$. Wu et al. (2021) proposed Autoformer, which decomposes series into trend and seasonal components within the transformer framework.

Lim et al. (2021) developed the Temporal Fusion Transformer (TFT), incorporating variable selection and interpretability for multi-horizon forecasting. TFT provides attention-based interpretability, showing which inputs matter for predictions.

Nie et al. (2023) introduced PatchTST, treating time series patches as tokens for efficient forecasting. This approach achieves state-of-the-art results on many benchmarks.

Despite these advances, Zeng et al. (2023) questioned whether transformers truly improve upon simpler baselines for time series, finding that linear models often perform comparably or better. This finding has sparked debate about when transformers are appropriate for time series. Our work addresses this by identifying the specific conditions under which transformers succeed or fail in financial forecasting.

%% ============================================================================
%% 3. DATA DESCRIPTION
%% ============================================================================

\section{Data Description}

This section describes our data sources, collection methodology, and preprocessing steps.

\subsection{Data Sources}

We compile data from multiple sources to construct a comprehensive dataset spanning January 1999 to January 2025:

\begin{table}[H]
\centering
\caption{Data Sources and Coverage}
\label{tab:data_sources}
\begin{tabular}{llll}
\toprule
\textbf{Data Type} & \textbf{Source} & \textbf{Period} & \textbf{Access Method} \\
\midrule
Stock prices (AAPL) & Yahoo Finance & 1999--2025 & yfinance API \\
Related stocks (MSFT, GOOGL, AMZN) & Yahoo Finance & 1999--2025 & yfinance API \\
Market indices (S\&P 500, DJIA, NASDAQ) & Yahoo Finance & 1999--2025 & yfinance API \\
Financial news (1999--2025) & HuggingFace & 26 years & HF Datasets API \\
Real-time news (2025) & Google RSS & Current & Google News API \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Stock Price Data}

We obtain daily OHLCV (Open, High, Low, Close, Volume) data for Apple Inc. (AAPL) via the \texttt{yfinance} Python package. The data spans 6,542 trading days from January 4, 1999 to January 24, 2025.

\begin{table}[H]
\centering
\caption{AAPL Stock Price Summary Statistics}
\label{tab:price_stats}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Variable} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} & \textbf{N} \\
\midrule
Close Price (\$) & 52.87 & 62.41 & 0.25 & 259.02 & 6,542 \\
Daily Return (\%) & 0.12 & 2.84 & $-51.9$ & 13.9 & 6,541 \\
Volume (millions) & 142.3 & 98.7 & 12.4 & 987.2 & 6,542 \\
\bottomrule
\end{tabular}
\end{table}

Prices are adjusted for stock splits (7:1 in June 2014, 4:1 in August 2020). Over our sample period, AAPL experienced a 1,040$\times$ price increase from \$0.25 (split-adjusted) to \$259.02, representing one of the most successful stock performances in history.

We also obtain price data for related technology stocks (MSFT, GOOGL, AMZN) and major market indices (S\&P 500, Dow Jones Industrial Average, NASDAQ Composite) to serve as market context features.

\subsection{News Data}

We obtain financial news data from the HuggingFace Financial News Dataset, which contains approximately 57 million articles spanning 1999--2025. We access this via the HuggingFace Datasets API with appropriate authentication tokens. For real-time 2025 news, we supplement with Google RSS feeds.

Articles are filtered for AAPL-relevance using keyword matching on the following terms: ``Apple,'' ``AAPL,'' ``iPhone,'' ``iPad,'' ``Mac,'' ``Tim Cook,'' ``Steve Jobs,'' and ``Apple Inc.''

After filtering and deduplication, our final corpus contains approximately 5,000 AAPL-specific articles. Article distribution varies substantially over time, with higher coverage in recent years corresponding to Apple's growth as a major technology company and increased media attention.

\textbf{Note on Data Caching:} To avoid repeated API calls during development and ensure reproducibility, the fetched news data can be cached locally as CSV files. This approach balances computational efficiency with data freshness requirements.

\subsection{Data Preprocessing}

\subsubsection{News-Price Alignment}

We implement the following alignment rules to ensure temporal consistency:

\begin{itemize}
    \item News published before 4:00 PM EST (market close) on day $t$ is assigned to day $t$
    \item News published after market close or on weekends is assigned to the next trading day
    \item Multiple articles on the same day are aggregated by averaging sentiment scores
    \item Days without news are imputed with 7-day backward rolling mean
\end{itemize}

\subsubsection{Data Quality Issues}

We acknowledge several data limitations:

\textbf{Missing News Days:} 69\% of trading days lack dedicated AAPL articles, requiring imputation via rolling means.

\textbf{API Rate Limits:} HuggingFace API has usage limits requiring caching strategies and efficient query design.

\textbf{Survivorship Bias:} AAPL's exceptional performance may not generalize to the broader market or to stocks that underperformed or were delisted.

\subsection{Time Series Characteristics}

Figure 1 presents time series diagnostics for AAPL stock prices over our sample period.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/02_time_series_diagnostics.png}
    \caption{Time series diagnostics for AAPL stock prices (1999--2025). Panel (a) shows the price series with clear upward trend. Panel (b) displays autocorrelation function (ACF) indicating strong persistence. Panel (c) shows partial autocorrelation (PACF). Panel (d) presents seasonal decomposition revealing trend, seasonal, and residual components.}
    \label{fig:timeseries}
\end{figure}

The diagnostics reveal strong autocorrelation (ACF lag-1 $> 0.99$), confirming the non-stationary nature of price levels and motivating our use of differencing in SARIMAX models.

%% ============================================================================
%% 4. FEATURE ENGINEERING
%% ============================================================================

\section{Feature Engineering}

We construct 55 features organized into four categories, following a systematic approach to capture different aspects of market dynamics.

\subsection{Feature Categories}

\begin{table}[H]
\centering
\caption{Feature Engineering Summary}
\label{tab:features}
\begin{tabular}{llr}
\toprule
\textbf{Category} & \textbf{Description} & \textbf{Count} \\
\midrule
Sentiment Features & VADER, TextBlob, FinBERT (raw + rolling) & 15 \\
Text Features & LDA topics, adjective counts, keywords & 15 \\
Market Context & Related stocks, indices (1-day lag) & 18 \\
Technical Features & Price/volume rolling means & 7 \\
\midrule
\textbf{Total} & & \textbf{55} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sentiment Extraction Methods}

We employ three sentiment extraction methods with different strengths:

\textbf{VADER (Valence Aware Dictionary and sEntiment Reasoner):} A rule-based model incorporating intensity modifiers (``very,'' ``extremely''), emoticons, and negation handling. VADER outputs a compound score in $[-1, 1]$ where $-1$ is most negative and $+1$ is most positive. VADER is particularly suited for social media and news text due to its handling of informal language.

\textbf{TextBlob:} A pattern-based approach built on the Pattern library, originally trained on movie reviews. TextBlob provides polarity in $[-1, 1]$ and subjectivity in $[0, 1]$. While not specifically designed for financial text, it provides a general-purpose baseline.

\textbf{FinBERT:} A BERT-based model fine-tuned on financial communications (Araci, 2019). FinBERT outputs probability distributions over \{negative, neutral, positive\} classes, from which we compute a continuous sentiment score:
\begin{equation}
S_{\text{FinBERT}} = P(\text{positive}) - P(\text{negative})
\end{equation}

FinBERT represents state-of-the-art for financial sentiment, capturing context-dependent meaning (e.g., ``liability'' is neutral in financial contexts).

\subsection{Rolling Mean Smoothing}

For each sentiment method, we compute rolling means over windows $w \in \{3, 7, 14, 30\}$ days:
\begin{equation}
S_t^{(w)} = \frac{1}{w} \sum_{i=0}^{w-1} s_{t-i}
\end{equation}

This smoothing reduces noise while introducing lag. The optimal window balances these competing effects:

\begin{itemize}
    \item \textbf{Noise Reduction:} Averaging over $w$ observations reduces variance by factor $\sqrt{w}$, assuming i.i.d. noise
    \item \textbf{Information Lag:} The effective lag is $(w-1)/2$ days, delaying reaction to new information
    \item \textbf{Signal-to-Noise Ratio:} We find that 7-day windows improve SNR by 61\% while introducing only 3-day effective lag
\end{itemize}

\subsection{Topic Modeling with LDA}

We apply Latent Dirichlet Allocation (LDA) with 5 topics to the news corpus to capture thematic variation:
\begin{equation}
p(\mathbf{w}_d | \alpha, \beta) = \int p(\theta_d | \alpha) \prod_{n=1}^{N_d} \sum_{z_{dn}} p(z_{dn} | \theta_d) p(w_{dn} | z_{dn}, \beta) d\theta_d
\end{equation}

The discovered topics correspond to: (1) Product announcements, (2) Financial results, (3) Market commentary, (4) Legal/regulatory, and (5) Industry trends. Each day receives 5 topic proportion features capturing the thematic mix of that day's news coverage.

\subsection{Market Context Features}

To capture market-wide dynamics that may affect AAPL, we include lagged features from:

\begin{itemize}
    \item \textbf{Related technology stocks:} MSFT, GOOGL, AMZN (daily returns, 5-day volatility)
    \item \textbf{Market indices:} S\&P 500, Dow Jones, NASDAQ (daily returns)
    \item \textbf{Correlation measures:} Rolling 20-day correlation between AAPL and related stocks
\end{itemize}

All market features use one-day lags to prevent lookahead bias.

\subsection{Feature Correlation Analysis}

Figure 2 presents the correlation structure among our engineered features.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/03_correlation_matrix.png}
    \caption{Correlation matrix of sentiment and price features. Sentiment features (VADER, TextBlob) show moderate correlation with each other ($r \approx 0.6$) but low correlation with price features ($r < 0.15$), suggesting complementary information content.}
    \label{fig:correlation}
\end{figure}

The correlation analysis reveals that sentiment features provide independent information from price-based features, supporting their inclusion in the model.

%% ============================================================================
%% 5. MODEL ARCHITECTURES
%% ============================================================================

\section{Model Architectures}

We employ a hierarchical training strategy with two model groups: foundational models trained on full historical data, and neural networks trained on recent data with foundational predictions as input features.

\subsection{Hierarchical Training Strategy}

\textbf{Hypothesis:} Models trained on long historical data can provide useful features for models trained on recent data, even when the long-term data distribution differs substantially from the recent period.

The strategy proceeds in two stages:

\textbf{Stage 1: Foundational Models (26-year training)}
\begin{itemize}
    \item Train Linear Regression, SARIMAX, and TCN on full 1999--2025 dataset
    \item These models learn long-term patterns and trend dynamics
    \item Generate out-of-sample predictions for the 2020--2025 period using walk-forward validation
\end{itemize}

\textbf{Stage 2: Neural Networks with Foundational Features (5-year training)}
\begin{itemize}
    \item Add foundational model predictions as the 56th input feature
    \item Train LSTM, GRU, BiLSTM, CNN-LSTM, and Transformer on 2020--2025 data only
    \item Neural networks learn to correct foundational model errors rather than predicting from scratch
\end{itemize}

This approach addresses three key challenges:

\textbf{Distribution Shift:} Neural networks avoid learning outdated patterns from 1999--2015 data where prices ranged from \$0.25--\$30, vastly different from recent \$100--\$260 range.

\textbf{Sample Efficiency:} Foundational predictions encode long-term information compactly in a single feature, avoiding the need to learn from thousands of historical observations.

\textbf{Trend Awareness:} The foundational feature provides explicit trend signal that neural networks otherwise struggle to capture from raw price data.

\subsection{Foundational Models}

\subsubsection{Linear Regression}

We employ ordinary least squares on the full feature set:
\begin{equation}
y_{t+1} = \beta_0 + \sum_{j=1}^{55} \beta_j x_{jt} + \epsilon_t
\end{equation}

Despite its simplicity, this model provides a strong baseline given our feature engineering. With 55 features and approximately 4,500 training observations (70\% of 6,542 days), the model is well-specified with sufficient degrees of freedom.

\subsubsection{SARIMAX}

The Seasonal ARIMA with eXogenous regressors captures both temporal autocorrelation and sentiment effects:
\begin{equation}
\phi(B)\Phi(B^s)(1-B)^d(1-B^s)^D y_t = \theta(B)\Theta(B^s)\epsilon_t + \beta X_t
\end{equation}

We select order $(p,d,q) = (2,1,1)$ via AIC minimization, using walk-forward validation with expanding windows. The exogenous regressors include sentiment features and market context variables.

\subsubsection{Temporal Convolutional Network (TCN)}

TCN employs dilated causal convolutions (Bai et al., 2018) to capture temporal patterns without recurrence:
\begin{equation}
(x *_d f)(t) = \sum_{k=0}^{K-1} f(k) \cdot x_{t - d \cdot k}
\end{equation}

where $d$ is the dilation factor and $K$ is the kernel size. We use three TCN blocks with channels [64, 128, 64], kernel size 3, and dropout 0.2. Exponentially increasing dilation factors (1, 2, 4) enable the receptive field to grow polynomially with depth.

\subsection{Neural Network Architectures}

\subsubsection{LSTM (Long Short-Term Memory)}

LSTM units address the vanishing gradient problem through gating mechanisms (Hochreiter and Schmidhuber, 1997):
\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad \text{(forget gate)} \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad \text{(input gate)} \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \quad \text{(candidate)} \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad \text{(cell state)} \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad \text{(output gate)} \\
h_t &= o_t \odot \tanh(C_t) \quad \text{(hidden state)}
\end{align}

We use 2 LSTM layers with 64 hidden units and dropout 0.2, yielding approximately 55K parameters.

\subsubsection{GRU (Gated Recurrent Unit)}

GRU simplifies LSTM by combining forget and input gates (Cho et al., 2014):
\begin{align}
z_t &= \sigma(W_z \cdot [h_{t-1}, x_t]) \quad \text{(update gate)} \\
r_t &= \sigma(W_r \cdot [h_{t-1}, x_t]) \quad \text{(reset gate)} \\
\tilde{h}_t &= \tanh(W \cdot [r_t \odot h_{t-1}, x_t]) \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{align}

This architecture is particularly well-suited for residual correction, as the update gate can keep most of $h_{t-1}$ (encoding the Linear prediction) while adding small corrections.

\subsubsection{BiLSTM (Bidirectional LSTM)}

BiLSTM processes sequences in both directions:
\begin{equation}
h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]
\end{equation}

This captures both forward and backward temporal dependencies, though the backward component has limited interpretability for forecasting since it uses future information during training.

\subsubsection{CNN-LSTM Hybrid}

This architecture combines convolutional feature extraction with sequential modeling:
\begin{enumerate}
    \item 1D convolution with 32 filters, kernel size 3 extracts local patterns
    \item MaxPooling reduces dimensionality
    \item LSTM layer captures temporal dependencies in extracted features
    \item Dense output layer produces final prediction
\end{enumerate}

\subsubsection{Transformer}

Our Transformer implementation uses the standard encoder architecture (Vaswani et al., 2017):
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\end{equation}

\textbf{Critical Issue:} When sequence length $n=1$ (single-step input), self-attention degenerates:
\begin{itemize}
    \item $QK^T$ becomes a $1 \times 1$ scalar
    \item $\text{softmax}([c]) = [1]$ for any scalar $c$
    \item Attention reduces to identity: $\text{Attention}(Q,K,V) = V$
\end{itemize}

To address this, we use sequence length 30 (past 30 days) as transformer input, enabling meaningful self-attention across temporal positions. Architecture: $d_{model}=64$, $n_{heads}=4$, $n_{layers}=2$, yielding approximately 51K parameters.

\subsection{Training Details}

\begin{table}[H]
\centering
\caption{Model Training Configuration}
\label{tab:training}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Epochs} & \textbf{Learning Rate} & \textbf{Early Stopping} \\
\midrule
Linear Regression & 56 & -- & -- & -- \\
SARIMAX & 5 & -- & -- & -- \\
TCN & 89,345 & 60 & 0.001 & Patience=15 \\
LSTM & 54,785 & 150 & 0.001 & Patience=25 \\
GRU & 42,113 & 100 & 0.001 & Patience=15 \\
BiLSTM & 86,529 & 100 & 0.001 & Patience=15 \\
CNN-LSTM & 26,433 & 100 & 0.001 & Patience=15 \\
Transformer & 51,201 & 200 & 0.001 & Patience=30 \\
\bottomrule
\end{tabular}
\end{table}

All neural networks use:
\begin{itemize}
    \item Adam optimizer with gradient clipping (max\_norm=1.0)
    \item MinMaxScaler normalization (fitted on training data only)
    \item Random seed 42 for reproducibility (except LSTM uses seed 46)
\end{itemize}

\subsection{Evaluation Framework}

We employ walk-forward validation with expanding windows:
\begin{enumerate}
    \item Initial training window: 70\% of observations
    \item Predict next observation
    \item For SARIMAX: expand training window by 1 observation and retrain
    \item For neural networks: use fixed trained model on test set
\end{enumerate}

Evaluation metrics:
\begin{itemize}
    \item \textbf{RMSE:} $\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$
    \item \textbf{MAE:} $\frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$
    \item \textbf{MAPE:} $\frac{100}{n}\sum_{i=1}^n \left|\frac{y_i - \hat{y}_i}{y_i}\right|$
    \item \textbf{$R^2$:} $1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$
\end{itemize}

%% ============================================================================
%% 6. EMPIRICAL RESULTS
%% ============================================================================

\section{Empirical Results}

This section presents our main findings, organized around model performance, sentiment impact, and the hierarchical training strategy.

\subsection{Overall Model Comparison}

Table 6 presents comprehensive results across all models.

\begin{table}[H]
\centering
\caption{Model Performance Comparison}
\label{tab:results}
\begin{tabular}{llrrrr}
\toprule
\textbf{Model} & \textbf{Training Data} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{$R^2$} \\
\midrule
\multicolumn{6}{l}{\textit{Foundational Models (26-year training)}} \\
Linear Regression & 26 years & 1.83 & 1.34 & 0.94 & 0.9992 \\
SARIMAX (VADER RM7) & 26 years & 2.66 & 1.91 & 1.21 & 0.9984 \\
TCN & 26 years & 21.16 & 18.34 & 9.87 & 0.8912 \\
\midrule
\multicolumn{6}{l}{\textit{Neural Networks without hierarchical features}} \\
LSTM & 5 years & 14.21 & 12.18 & 5.42 & 0.6812 \\
GRU & 5 years & 11.83 & 10.01 & 4.31 & 0.7234 \\
Transformer & 5 years & 97.01 & 77.41 & 44.89 & $-1.17$ \\
\midrule
\multicolumn{6}{l}{\textit{Neural Networks with hierarchical features (56th feature)}} \\
LSTM (hybrid) & 5 years & 12.12 & 10.58 & 4.54 & 0.8909 \\
GRU (hybrid) & 5 years & 7.63 & 6.44 & 2.78 & 0.9356 \\
BiLSTM (hybrid) & 5 years & 7.77 & 6.33 & 2.81 & 0.9012 \\
CNN-LSTM (hybrid) & 5 years & 7.34 & 6.01 & 2.64 & 0.9039 \\
Transformer (hybrid) & 5 years & 8.42 & 7.21 & 3.12 & 0.8734 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}

\textbf{Linear regression achieves best overall performance} with RMSE = \$1.83 and $R^2$ = 0.9992. This reflects both the quality of our feature engineering and the limited sample size favoring simpler models. The 55-feature linear model has only 56 parameters, compared to 50,000+ for neural networks.

\textbf{Hierarchical training transforms neural network performance.} Most strikingly:
\begin{itemize}
    \item GRU $R^2$: 0.72 $\rightarrow$ 0.94 (+25\% relative improvement)
    \item Transformer $R^2$: $-1.17$ $\rightarrow$ 0.87 (from catastrophic failure to competitive)
    \item LSTM $R^2$: 0.68 $\rightarrow$ 0.89 (+31\% relative improvement)
\end{itemize}

\textbf{Model complexity inversely relates to performance} for the foundational models trained on 26 years of data, suggesting overfitting concerns for complex architectures despite the large sample.

\subsection{Sentiment Feature Impact}

Table 7 compares SARIMAX performance across sentiment configurations.

\begin{table}[H]
\centering
\caption{SARIMAX Performance by Sentiment Configuration}
\label{tab:sentiment_results}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Sentiment} & \textbf{Window} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{Improvement} \\
\midrule
None (baseline) & -- & 2.71 & 1.96 & 1.24 & -- \\
\midrule
VADER & Raw & 2.70 & 1.95 & 1.23 & +0.4\% \\
VADER & RM7 & 2.66 & 1.91 & 1.21 & +1.5\% \\
VADER & RM14 & 2.68 & 1.93 & 1.22 & +1.1\% \\
VADER & RM30 & 2.72 & 1.97 & 1.25 & $-0.4$\% \\
\midrule
TextBlob & Raw & 2.73 & 1.97 & 1.24 & $-0.7$\% \\
TextBlob & RM7 & 2.70 & 1.94 & 1.22 & +0.4\% \\
\midrule
FinBERT & Raw & 2.71 & 1.95 & 1.23 & +0.0\% \\
FinBERT & RM7 & 2.70 & 1.94 & 1.22 & +0.4\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}

\textbf{7-day rolling VADER achieves best improvement} at 1.5\% RMSE reduction. This difference is statistically significant based on Diebold-Mariano tests ($p < 0.05$).

\textbf{The 7-day window is optimal} because it balances noise reduction (61\% improvement in signal-to-noise ratio) against information lag (3-day effective lag).

\textbf{FinBERT shows minimal improvement from rolling means}, likely because BERT's multi-layer attention already provides smoothing through context aggregation.

\textbf{Longer windows (30 days) hurt performance} due to excessive lag that offsets noise reduction benefits.

\subsection{Hierarchical Training Analysis}

Figure 3 illustrates the impact of hierarchical training on neural network performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{../results/enhanced/statistical/06_model_comparison.png}
    \caption{Model performance comparison showing RMSE (left), R-squared (center), and multi-metric normalized comparison (right). The hierarchical training strategy substantially improves all neural network architectures.}
    \label{fig:comparison}
\end{figure}

The hierarchical approach works because:

\textbf{Trend encoding:} The Linear model's predictions encode the 26-year trend dynamics in a single feature that neural networks can leverage without learning from distant, potentially irrelevant history.

\textbf{Distribution alignment:} Neural networks train on recent (2020--2025) data only, avoiding distribution shift from distant history where prices and volatility regimes were fundamentally different.

\textbf{Residual learning:} The task becomes residual correction rather than full price prediction, which is easier to learn with limited data.

\subsection{Distribution Analysis}

Figure 4 presents the comprehensive distribution analysis of AAPL stock prices.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/01_comprehensive_distribution.png}
    \caption{Comprehensive distribution analysis of AAPL stock prices (1999--2025). The distribution exhibits significant positive skewness (2.14) and leptokurtosis (4.87), consistent with financial asset return characteristics.}
    \label{fig:distribution}
\end{figure}

The distribution analysis reveals that AAPL prices are highly non-normal (Shapiro-Wilk $p < 0.001$), with positive skewness reflecting the 1,040$\times$ growth over our sample period. This non-normality motivates our use of machine learning approaches over traditional linear methods that assume normality.

\subsection{Return-Level Prediction}

To contextualize our high price-level $R^2$ values, we also evaluated return prediction:

\begin{table}[H]
\centering
\caption{Return Prediction Performance (Stationary Target)}
\label{tab:returns}
\begin{tabular}{lrr}
\toprule
\textbf{Model} & \textbf{$R^2$ (Returns)} & \textbf{Directional Accuracy} \\
\midrule
Linear Regression & 0.084 & 54.2\% \\
SARIMAX & 0.063 & 53.1\% \\
GRU (hierarchical) & 0.071 & 52.8\% \\
Naive (predict 0) & 0.000 & 52.0\% \\
\bottomrule
\end{tabular}
\end{table}

The return-level $R^2$ of 0.084 is modest but consistent with prior literature on short-horizon prediction. The high price-level $R^2$ reflects AAPL's strong trend, which inflates variance-based metrics. Directional accuracy of 54.2\% is marginally above random (50\%), providing some evidence of predictive value.

\subsection{Best Model Analysis}

Figure 5 presents comprehensive diagnostics for our best-performing model (Linear Regression).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/07_linear_diagnostics.png}
    \caption{Linear Regression model diagnostics showing actual vs. predicted prices, residuals analysis, and Q-Q plot. The model achieves near-perfect alignment with $R^2 = 0.999$.}
    \label{fig:linear}
\end{figure}

The diagnostics confirm that Linear Regression with 55 features achieves excellent predictive accuracy, with residuals showing no systematic patterns and approximate normality.

%% ============================================================================
%% 7. LIMITATIONS
%% ============================================================================

\section{Limitations}

Several limitations qualify our findings. First, our single-stock focus on Apple Inc.---a large-cap, high-liquidity stock with exceptional 26-year performance (1,040$\times$ return)---limits generalizability to smaller or less-covered securities. Second, 69\% of trading days lack dedicated AAPL articles, requiring rolling mean imputation that may attenuate true sentiment signals. Third, the extreme non-stationarity of prices inflates our price-level $R^2$ values; return-level prediction shows more modest improvement ($R^2 = 0.084$). Fourth, while we use walk-forward validation for evaluation, our feature engineering choices were informed by the full dataset, potentially overstating true out-of-sample performance. Fifth, we ignore transaction costs, slippage, and market impact that would reduce trading profitability. Finally, our sample spans distinct market regimes (dot-com bubble, 2008 crisis, COVID-19), and future performance may differ.

%% ============================================================================
%% 8. CONCLUSION
%% ============================================================================

\section{Conclusion}

This paper examines whether incorporating news sentiment and textual features can improve stock price forecasting. Using 26 years of Apple Inc. data and 57 million financial news articles, we find:

\textbf{News sentiment provides incremental but meaningful predictive power.} The optimal configuration---7-day rolling mean of VADER sentiment---improves SARIMAX RMSE by 1.5\%, statistically significant at the 5\% level. However, sentiment should not be expected to transform forecasting accuracy alone.

\textbf{Model complexity should match data availability.} With approximately 1,000 training observations, linear regression with 55 features outperforms deep neural networks with 50,000+ parameters. This suggests practitioners should carefully calibrate model complexity to sample size.

\textbf{Hierarchical training substantially improves neural network performance.} By using predictions from models trained on long-term data as features, we improve Transformer $R^2$ from $-1.7$ to $0.87$ and GRU $R^2$ from $0.72$ to $0.94$. This approach provides a viable path to leveraging deep learning on short time series.

\textbf{Transformer failures in financial forecasting are often architectural, not fundamental.} The single-step input used in many studies causes self-attention to degenerate. Proper temporal sequencing restores competitive performance.

\subsection{Practical Implications}

For quantitative finance practitioners:
\begin{itemize}
    \item Sentiment features are worth incorporating but represent marginal improvements (1--2\%)
    \item Simpler models often outperform complex ones on limited financial data
    \item When applying neural networks to short time series, consider hierarchical approaches that leverage longer historical data
    \item Transformer architectures require sequence input (not single-step) to function properly
\end{itemize}

\subsection{Future Research Directions}

\begin{itemize}
    \item \textbf{Multi-stock validation:} Extend analysis to multiple stocks across sectors and market capitalizations
    \item \textbf{High-frequency analysis:} Investigate intraday news effects where information incorporation may be more pronounced
    \item \textbf{Alternative text sources:} Incorporate social media (Twitter/X), SEC filings (8-K, 10-K), and analyst reports
    \item \textbf{Theoretical framework:} Develop formal conditions for when hierarchical training provides benefits
    \item \textbf{Real-time deployment:} Build production systems with live news feeds and automatic retraining
\end{itemize}

\subsection{Reproducibility}

Complete code, data processing pipelines, and execution logs are available on GitHub. All experiments are reproducible with random seed 42 (LSTM uses seed 46).

%% ============================================================================
%% REFERENCES
%% ============================================================================

\newpage

\section*{References}

\begin{description}

\item Andrade, G., M. Mitchell, and E. Stafford. 2001. ``New Evidence and Perspectives on Mergers.'' \textit{Journal of Economic Perspectives} 15(2): 103--120.

\item Araci, D. 2019. ``FinBERT: Financial Sentiment Analysis with Pre-trained Language Models.'' \textit{arXiv preprint arXiv:1908.10063}.

\item Bai, S., J. Z. Kolter, and V. Koltun. 2018. ``An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.'' \textit{arXiv preprint arXiv:1803.01271}.

\item Baker, M., and J. Wurgler. 2007. ``Investor Sentiment in the Stock Market.'' \textit{Journal of Economic Perspectives} 21(2): 129--152.

\item Ball, R., and P. Brown. 1968. ``An Empirical Evaluation of Accounting Income Numbers.'' \textit{Journal of Accounting Research} 6(2): 159--178.

\item Bernard, V. L., and J. K. Thomas. 1989. ``Post-Earnings-Announcement Drift: Delayed Price Response or Risk Premium?'' \textit{Journal of Accounting Research} 27: 1--36.

\item Chaney, P. K., T. M. Devinney, and R. S. Winer. 1991. ``The Impact of New Product Introductions on the Market Value of Firms.'' \textit{Journal of Business} 64(4): 573--610.

\item Cho, K., B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. 2014. ``Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.'' \textit{arXiv preprint arXiv:1406.1078}.

\item Ding, X., Y. Zhang, T. Liu, and J. Duan. 2015. ``Deep Learning for Event-Driven Stock Prediction.'' \textit{Proceedings of IJCAI}, 2327--2333.

\item Engelberg, J. E., and C. A. Parsons. 2011. ``The Causal Impact of Media in Financial Markets.'' \textit{Journal of Finance} 66(1): 67--97.

\item Fama, E. F. 1970. ``Efficient Capital Markets: A Review of Theory and Empirical Work.'' \textit{Journal of Finance} 25(2): 383--417.

\item Feng, F., H. Chen, X. He, J. Ding, M. Sun, and T. S. Chua. 2019. ``Enhancing Stock Movement Prediction with Adversarial Training.'' \textit{Proceedings of IJCAI}, 5843--5849.

\item Fischer, T., and C. Krauss. 2018. ``Deep Learning with Long Short-term Memory Networks for Financial Market Predictions.'' \textit{European Journal of Operational Research} 270(2): 654--669.

\item Gentzkow, M., B. Kelly, and M. Taddy. 2019. ``Text as Data.'' \textit{Journal of Economic Literature} 57(3): 535--574.

\item Hirshleifer, D., S. S. Lim, and S. H. Teoh. 2009. ``Driven to Distraction: Extraneous Events and Underreaction to Earnings News.'' \textit{Journal of Finance} 64(5): 2289--2325.

\item Hochreiter, S., and J. Schmidhuber. 1997. ``Long Short-Term Memory.'' \textit{Neural Computation} 9(8): 1735--1780.

\item Hong, H., and J. C. Stein. 2000. ``Bad News Travels Slowly: Size, Analyst Coverage, and the Profitability of Momentum Strategies.'' \textit{Journal of Finance} 55(1): 265--295.

\item Jegadeesh, N., and S. Titman. 1993. ``Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency.'' \textit{Journal of Finance} 48(1): 65--91.

\item Kara, Y., M. A. Boyacioglu, and O. K. Baykan. 2011. ``Predicting Direction of Stock Price Index Movement Using Artificial Neural Networks.'' \textit{Expert Systems with Applications} 38(5): 5311--5319.

\item Lim, B., S. O. Arik, N. Loeff, and T. Pfister. 2021. ``Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting.'' \textit{International Journal of Forecasting} 37(4): 1748--1764.

\item Loughran, T., and B. McDonald. 2011. ``When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks.'' \textit{Journal of Finance} 66(1): 35--65.

\item Nie, Y., N. H. Nguyen, P. Sinthong, and J. Kalagnanam. 2023. ``A Time Series Is Worth 64 Words: Long-term Forecasting with Transformers.'' \textit{arXiv preprint arXiv:2211.14730}.

\item Patel, J., S. Shah, P. Thakkar, and K. Kotecha. 2015. ``Predicting Stock and Stock Price Index Movement Using Trend Deterministic Data Preparation and Machine Learning Techniques.'' \textit{Expert Systems with Applications} 42(1): 259--268.

\item Tetlock, P. C. 2007. ``Giving Content to Investor Sentiment: The Role of Media in the Stock Market.'' \textit{Journal of Finance} 62(3): 1139--1168.

\item Tetlock, P. C., M. Saar-Tsechansky, and S. Macskassy. 2008. ``More Than Words: Quantifying Language to Measure Firms' Fundamentals.'' \textit{Journal of Finance} 63(3): 1437--1467.

\item Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. 2017. ``Attention Is All You Need.'' \textit{Advances in Neural Information Processing Systems} 30.

\item Wu, H., J. Xu, J. Wang, and M. Long. 2021. ``Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting.'' \textit{Advances in Neural Information Processing Systems} 34.

\item Xu, Y., and S. B. Cohen. 2018. ``Stock Movement Prediction from Tweets and Historical Prices.'' \textit{Proceedings of ACL}, 1970--1979.

\item Yang, Y., M. C. S. Uy, and A. Huang. 2020. ``FinBERT: A Pretrained Language Model for Financial Communications.'' \textit{arXiv preprint arXiv:2006.08097}.

\item Zeng, A., M. Chen, L. Zhang, and Q. Xu. 2023. ``Are Transformers Effective for Time Series Forecasting?'' \textit{Proceedings of AAAI}, 11121--11128.

\item Zhang, L., C. Aggarwal, and X. Kong. 2023. ``A Survey on Neural Network Interpretability.'' \textit{IEEE Transactions on Emerging Topics in Computational Intelligence} 5(5): 726--742.

\item Zhou, H., S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang. 2021. ``Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting.'' \textit{Proceedings of AAAI}, 11106--11115.

\end{description}

\end{document}
