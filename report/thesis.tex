\documentclass[12pt,a4paper]{report}

%% ============================================================================
%% PACKAGES
%% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage{longtable}
\usepackage{array}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{hyperref}
\usepackage{fancyhdr}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Stock Price Forecasting with Advanced Features},
}

% Code listing style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single
}
\lstset{style=mystyle}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\fancyhead[L]{\nouppercase{\leftmark}}
\renewcommand{\headrulewidth}{0.4pt}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[section]

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

%% ============================================================================
%% TITLE PAGE
%% ============================================================================
\begin{titlepage}
    \centering
    \vspace*{0.5cm}
    
    {\LARGE\textsc{Carnegie Mellon University}\par}
    \vspace{0.3cm}
    {\large Department of Statistics \& Data Science\par}
    
    \vspace{2cm}
    
    {\Huge\bfseries Stock Price Forecasting with Advanced Features\par}
    
    \vspace{1cm}
    
    {\Large Complete Self-Contained Research Documentation\par}
    \vspace{0.5cm}
    {\large Methodology, Implementation, Results, and Analysis\par}
    
    \vspace{2.5cm}
    
    {\Large\textbf{Harsh Milind Tirhekar \& Atharva Vishwas Kulkarni}\par}
    
    \vspace{1.5cm}
    
    {\large Under the guidance of\par}
    \vspace{0.3cm}
    {\Large\textbf{Prof. Arun Kuchibhotla}\par}
    
    \vspace{2cm}
    
    \begin{tabular}{rl}
        \textbf{Dataset:} & 6,542 Trading Days (1999--2025) \\
        \textbf{News Corpus:} & 57+ Million Financial Articles \\
        \textbf{Models Evaluated:} & 9 Architectures + 4 Baselines \\
        \textbf{Evaluation Framework:} & Predictive Accuracy + Trading Performance \\
    \end{tabular}
    
    \vfill
    
    {\large January 2026\par}
\end{titlepage}

%% ============================================================================
%% ABSTRACT
%% ============================================================================
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This research develops and rigorously compares traditional time series methods (SARIMAX) with modern deep learning approaches for stock price forecasting of Apple Inc. (AAPL). We incorporate sentiment analysis from 57+ million financial news articles, rich text features extracted via NLP techniques, and market context from related stocks while maintaining strict temporal causality to prevent lookahead bias.

Our methodology employs a hybrid approach where foundational models---Linear Regression, SARIMAX, and Temporal Convolutional Networks (TCN)---trained on the full 26-year dataset serve as the basis for neural network models. Specifically, predictions from the Linear model are incorporated as an additional input feature for recurrent neural networks (LSTM, BiLSTM, GRU, CNN-LSTM), enabling these models to learn residual corrections rather than predicting prices from scratch. This strategy improved GRU performance from $R^2 = 0.64$ to $R^2 = 0.89$.

We systematically address six research requirements: (1) comparing raw versus rolling mean sentiment across multiple windows, (2) engineering higher-dimensional text features using LDA topic modeling and keyword tracking, (3) incorporating market context from related stocks with proper lagging, (4) implementing five neural network architectures for direct comparison with SARIMAX, (5) developing a foundational model strategy for improved neural network performance, and (6) analyzing Transformer architecture efficiency with reduced parameters.

Key results include: Linear Regression achieves RMSE = \$1.83 and $R^2 = 0.9992$; SARIMAX with sentiment achieves RMSE = \$2.66; CNN-LSTM achieves the best neural network performance with $R^2 = 0.8939$. Transformer models fail catastrophically ($R^2 = -1.17$) due to sequence-length architectural mismatch, with smaller model variants performing even worse, ruling out overfitting as the cause.

\textbf{Keywords:} Stock Forecasting, Sentiment Analysis, SARIMAX, LSTM, Transformer, Feature Engineering

\clearpage
%% ============================================================================
%% TABLE OF CONTENTS
%% ============================================================================
\tableofcontents
\clearpage
\listoffigures
\addcontentsline{toc}{chapter}{List of Figures}
\clearpage
\listoftables
\addcontentsline{toc}{chapter}{List of Tables}
\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}
\onehalfspacing

%% ============================================================================
%% CHAPTER 1: INTRODUCTION AND PROBLEM STATEMENT
%% ============================================================================
\chapter{Introduction and Problem Statement}
\label{ch:introduction}

\section{Research Objectives}
\label{sec:objectives}

\textbf{Primary Goal:} Develop and rigorously compare traditional time series methods (SARIMAX) with modern deep learning approaches for stock price forecasting, incorporating sentiment analysis from news articles, rich text features, and market context from related stocks.

\vspace{0.5cm}
\textbf{Specific Aims:}

\begin{description}
    \item[Aim 1.] Quantify the performance difference between raw daily sentiment scores and rolling mean sentiment features using multiple window sizes (3, 7, 14, 30 days)
    
    \item[Aim 2.] Engineer higher-dimensional text features beyond simple sentiment scores using NLP techniques (topic modeling, adjective analysis, keyword tracking)
    
    \item[Aim 3.] Incorporate market context from related stocks and indices while maintaining temporal causality (no lookahead bias)
    
    \item[Aim 4.] Implement and compare multiple neural network architectures (LSTM, Bidirectional LSTM, GRU, Transformer, CNN-LSTM hybrid) against traditional baselines
    
    \item[Aim 5.] Document all methodologies, results, and analyses for complete reproducibility
    
    \item[Aim 6.] Ensure all experiments are temporally valid with proper validation protocols
\end{description}

\section{Professor's Requirements}
\label{sec:requirements}

This research was conducted under the guidance of Prof. Arun Kuchibhotla, who specified six concrete requirements. Below, we document each requirement and our implementation approach.

\subsection{Requirement 1: Raw vs. Rolling Mean Sentiment Comparison}

\textbf{Requirement:} ``Directly compare model performance using raw daily sentiment scores versus your current 7-day rolling mean sentiment scores. Report MAE, RMSE and MAPE for each, on the same dataset.''

\textbf{Our Implementation:} We extend this requirement significantly by testing ALL rolling window sizes (3, 7, 14, 30 days) and all three sentiment methods (TextBlob, VADER, FinBERT), providing comprehensive comparison beyond the original requirement. This systematic comparison enables us to identify optimal window sizes for each sentiment method and understand the trade-offs between noise reduction and information lag.

Results show that 7-day rolling VADER sentiment achieves the best performance, with SARIMAX RMSE of \$2.66 compared to \$2.70 for raw VADER---a statistically significant 1.5\% improvement. The complete comparison across all window sizes and sentiment methods is presented in Chapter 3 (Table \ref{tab:window_comparison}).

\subsection{Requirement 2: Rich Text Feature Engineering}

\textbf{Requirement:} ``Try building richer text features instead of a single daily sentiment number. For example, create higher-dimensional features using bag-of-words, topic modeling or tracking specific adjective frequencies.''

\textbf{Our Implementation:} We implement a comprehensive text feature pipeline consisting of:

\begin{itemize}
    \item \textbf{Bag-of-Words Vectorization:} TF-IDF weighted term frequencies with vocabulary size 500
    \item \textbf{LDA Topic Modeling:} 5-topic model identifying latent themes in financial news (earnings, products, market conditions, management, macroeconomics)
    \item \textbf{Adjective Frequency Analysis:} 6 features capturing positive/negative adjective counts, ratios, and net sentiment
    \item \textbf{Financial Keyword Tracking:} 18 domain-specific keywords across positive (earnings, growth, profit, beat, upgrade, bullish), negative (loss, decline, miss, downgrade, bearish, warning), and neutral (announced, reported, quarterly, guidance, forecast, outlook) categories
\end{itemize}

In total, we create 29 text features beyond simple sentiment scores. The complete mathematical foundations for each feature type are presented in Chapter 3.

\subsection{Requirement 3: Related Stock Features}

\textbf{Requirement:} ``Bring in related stocks' previous day price data as features (never the same day's to avoid lookahead bias). Start with just prices and in the future maybe add their news or sentiment, too.''

\textbf{Our Implementation:} We fetch data from 3 related technology stocks (MSFT, GOOGL, AMZN) selected based on sector similarity and historical correlation with AAPL (correlations $> 0.7$). We also incorporate 3 major market indices (S\&P 500, DJIA, NASDAQ). For each, we create:

\begin{itemize}
    \item Lagged returns at lags 1, 2, and 3 days (12 features)
    \item 21-day rolling correlations with AAPL (3 features)
    \item Index return features at lag 1, 2, 3 (9 features)
    \item Relative performance metrics (3 features)
\end{itemize}

All features use \textbf{strictly lag $\geq$ 1} to prevent any lookahead bias. The mathematical formulation and lookahead prevention proof are provided in Section \ref{sec:lookahead_proof}.

\subsection{Requirement 4: Neural Network Comparison}

\textbf{Requirement:} ``Try modern modeling approaches like neural networks (LSTM, GRU) and transformers. Use your richer text features and extra covariates, then compare the results directly to your SARIMAX Baseline.''

\textbf{Our Implementation:} We implement 5 neural network architectures, each with complete mathematical specification:

\begin{enumerate}
    \item \textbf{LSTM (Long Short-Term Memory):} 2-layer architecture with 64 hidden units, dropout 0.2
    \item \textbf{BiLSTM (Bidirectional LSTM):} Forward and backward processing with concatenated outputs
    \item \textbf{GRU (Gated Recurrent Unit):} Simplified 2-gate architecture, same configuration as LSTM
    \item \textbf{Transformer:} Multi-head self-attention with 4 heads, d\_model=64, 2 encoder layers
    \item \textbf{CNN-LSTM Hybrid:} 1D convolution for local pattern extraction followed by LSTM for sequence modeling
\end{enumerate}

All models use the complete feature set (55 base features + 1 hybrid feature) and are trained with Adam optimizer, MSE loss, and early stopping (patience=15). Complete architectural details and gate equations are provided in Chapter 6.

\subsection{Requirement 5: Foundational Model Strategy}

\textbf{Requirement:} ``Even if the data is outdated (2020), can we still use it to check how the method developed works on that? Or perhaps it can be used to develop a foundation model.''

\textbf{Our Implementation:} We develop a \textit{foundational model strategy} that addresses the challenge of training neural networks on non-stationary long-term data. The key innovation is:

\begin{enumerate}
    \item \textbf{Train foundational models on full 26-year data:} SARIMAX, Linear Regression, and TCN are trained on the complete historical dataset (1999--2025). These models can handle non-stationarity through differencing (SARIMAX), adaptive features (Linear), or dilated convolutions (TCN).
    
    \item \textbf{Use foundational predictions as the ``16th feature'':} Predictions from the Linear model (which achieves $R^2 = 0.9992$) are added as an additional input feature for neural networks.
    
    \item \textbf{Train neural networks on recent 5-year data:} LSTM, BiLSTM, GRU, and CNN-LSTM are trained on 2020--2025 data, where price distributions are more homogeneous (\$100--\$260) compared to the full history (\$0.25--\$260).
\end{enumerate}

\textbf{Why this works:} Instead of learning to predict absolute prices (a hard problem with non-stationary data), neural networks learn to predict \textit{corrections} to the foundational model's predictions (a much easier problem). Mathematically:

\begin{equation}
    y = \hat{y}_{\text{Linear}} + \underbrace{(y - \hat{y}_{\text{Linear}})}_{\text{residual learned by RNN}}
\end{equation}

Since the Linear model explains 99.92\% of variance, the RNN only needs to model the remaining 0.08\%.

\textbf{Results:} GRU improved from $R^2 = 0.64$ (without foundational feature) to $R^2 = 0.89$ (with foundational feature)---an improvement of +0.25 in $R^2$. CNN-LSTM similarly improved. LSTM showed minimal improvement, suggesting architectural differences in how models utilize the additional feature.

\subsection{Requirement 6: Transformer Size Reduction}

\textbf{Requirement:} ``Cut down the transformer size, try fewer attention heads, smaller feed-forward layers, and note the impact on training time and accuracy.''

\textbf{Our Implementation:} We conducted systematic Transformer ablation experiments:

\begin{table}[H]
\centering
\caption{Transformer Size Ablation Study}
\begin{tabular}{lrrrrr}
\hline
\textbf{Configuration} & \textbf{d\_model} & \textbf{Heads} & \textbf{Parameters} & \textbf{Train Time} & \textbf{Test $R^2$} \\
\hline
Original & 64 & 4 & 52,000 & 45s & -1.17 \\
Reduced & 32 & 2 & 6,000 & 28s & -1.45 \\
Minimal & 16 & 1 & 2,500 & 18s & -1.88 \\
\hline
\end{tabular}
\end{table}

\textbf{Critical Finding:} Reducing Transformer size made performance \textit{worse}, not better. This counter-intuitive result rules out overfitting and reveals a fundamental architectural mismatch:

\begin{itemize}
    \item Our input has sequence length = 1 (one day's features reshaped as a sequence)
    \item Self-attention between a single position and itself is trivially the identity operation
    \item The Transformer's power comes from modeling relationships \textit{between} positions---with one position, there are no relationships to model
\end{itemize}

This is a \textbf{design error} in applying Transformers to this task, not a scientific result about Transformer capabilities. Properly configured Transformers using multiple time steps as sequence positions could potentially perform well.

\section{Contributions}
\label{sec:contributions}

This research makes several contributions to the literature on machine learning for financial forecasting:

\subsection{Contribution 1: Hybrid Residual-Learning Framework}

We propose and validate a meta-learning approach where predictions from foundational models (trained on long historical data) serve as input features for neural networks (trained on recent data). This transforms the learning task from direct prediction to residual correction, substantially improving RNN performance. The framework is generalizable to other forecasting contexts where data non-stationarity challenges neural network training.

\subsection{Contribution 2: Quantitative Transformer Failure Analysis}

We provide systematic ablation evidence that vanilla Transformer architectures fail for single-step regression tasks due to sequence-length mismatch, not model capacity or overfitting. This finding has practical implications for researchers considering Transformer architectures for tabular regression tasks and contributes to the growing literature on understanding when attention mechanisms provide value.

\subsection{Contribution 3: Trading Strategy Evaluation}

Unlike prior academic work that reports only predictive accuracy metrics ($R^2$, RMSE), we translate forecasts into a complete trading strategy with realistic transaction costs and evaluate using economic metrics (Sharpe ratio, maximum drawdown, regime-specific performance). This bridges the gap between academic forecasting research and practical trading applications.

\subsection{Contribution 4: Complete Baseline Framework}

We establish a rigorous baseline comparison including naive persistence, random walk, and no-sentiment model variants. This enables fair assessment of the incremental contribution of sentiment features and advanced modeling techniques.

\subsection{Contribution 5: Comprehensive Documentation}

All methodologies, code, hyperparameters, and results are documented for complete reproducibility. Mathematical formulas are accompanied by component explanations and practical interpretations accessible to readers unfamiliar with time series modeling.

\section{Literature Review}
\label{sec:literature}

\subsection{Sentiment Analysis in Financial Markets}

The application of textual sentiment to financial forecasting has evolved substantially since Tetlock's (2007) foundational work demonstrating that media pessimism predicts market movements.

\textbf{Tetlock (2007)} analyzed the ``Abreast of the Market'' column in the Wall Street Journal using the Harvard IV-4 psychosocial dictionary. Key findings:
\begin{itemize}
    \item High media pessimism predicts downward pressure on market prices
    \item High or low pessimism predicts high trading volume
    \item Return predictability is short-lived, consistent with sentiment causing temporary mispricing
\end{itemize}

\textbf{Bollen, Mao, and Zeng (2011)} extended sentiment analysis to social media, analyzing 9.7 million tweets over 10 months. Using OpinionFinder and Google-Profile of Mood States (GPOMS), they found:
\begin{itemize}
    \item Twitter mood states (especially ``calm'') improve prediction of the Dow Jones Industrial Average
    \item The improvement is statistically significant with Granger causality at lag 3--4 days
    \item Accuracy improves from 73.3\% to 86.7\% when mood dimensions are included
\end{itemize}

\textbf{Ding, Zhang, Liu, and Duan (2015)} developed neural network models using event embeddings extracted from Reuters and Bloomberg news:
\begin{itemize}
    \item Proposed a convolutional neural network for extracting events from news
    \item Achieved state-of-the-art performance on S\&P 500 directional prediction
    \item Demonstrated that structured event representations outperform bag-of-words
\end{itemize}

\textbf{Xu and Cohen (2018)} developed StockNet, combining social media sentiment with technical indicators using variational autoencoders:
\begin{itemize}
    \item Achieved 58.2\% directional accuracy on the ACL18 dataset
    \item Demonstrated benefits of modeling temporal dependencies in sentiment
\end{itemize}

\subsection{Deep Learning for Financial Time Series}

\textbf{Hochreiter and Schmidhuber (1997)} introduced Long Short-Term Memory networks, solving the vanishing gradient problem through gating mechanisms. LSTMs have become the standard architecture for sequence modeling in many domains, including financial forecasting.

\textbf{Cho et al. (2014)} proposed Gated Recurrent Units as a simplified alternative to LSTMs:
\begin{itemize}
    \item Two gates (update, reset) instead of three (input, forget, output)
    \item Comparable performance with fewer parameters
    \item Faster training on smaller datasets
\end{itemize}

\textbf{Bai, Kolter, and Koltun (2018)} introduced Temporal Convolutional Networks (TCN):
\begin{itemize}
    \item Dilated causal convolutions capture long-range dependencies
    \item Parallel processing enables faster training than recurrent models
    \item Competitive with RNNs on sequence modeling benchmarks
\end{itemize}

\textbf{Vaswani et al. (2017)} introduced the Transformer architecture:
\begin{itemize}
    \item Self-attention mechanism models dependencies without recurrence
    \item Multi-head attention enables learning multiple relationship types
    \item State-of-the-art performance on NLP tasks
\end{itemize}

However, Transformers were designed for sequence-to-sequence tasks with multiple positions. Our research demonstrates that applying vanilla Transformers to single-step regression violates their architectural assumptions.

\subsection{Why News Coverage Improves Forecasts}

Pure price history captures the sequence of past values but misses the \textit{why} behind price movements. News coverage provides crucial complementary information through several mechanisms:

\textbf{Event-Driven Shocks:} Earnings announcements, product launches, regulatory changes, and management transitions create price discontinuities that historical price patterns cannot predict. News articles signal these events before they fully materialize in prices. For example, an article announcing a new iPhone model may precede the price impact by days as information diffuses through the investor population.

\textbf{Sentiment Momentum:} Persistent positive or negative media coverage creates buying or selling pressure that extends beyond initial price reactions. This is consistent with behavioral finance theories of attention-driven trading and sentiment-induced mispricing. Rolling sentiment means capture this momentum by smoothing day-to-day noise while preserving persistent trends.

\textbf{Information Diffusion:} Not all investors process information simultaneously. Institutional traders may react within minutes, while retail investors may take days to incorporate new information. News sentiment captures this gradual diffusion, explaining why markets take multiple days to fully incorporate new information and why lagged sentiment has predictive power.

\textbf{Narrative Framing:} The same fundamental information can be framed positively or negatively by journalists. A ``slower than expected'' growth rate could be framed as ``disappointing'' or as ``solid performance in a challenging environment.'' Sentiment analysis captures this framing effect, which influences investor perception and trading behavior.

\textbf{Empirical Evidence:} Our analysis confirms these mechanisms. The 7-day rolling VADER sentiment correlates with next-day returns at $\rho = 0.048$ ($p < 0.001$), demonstrating statistically significant predictive power beyond price history. While the correlation magnitude is small (consistent with efficient market theory), it translates to measurable improvements in forecast accuracy and trading performance.

\subsection{Comparison with Prior AAPL Studies}

Table \ref{tab:prior_work} compares our results with prior academic work on AAPL forecasting.

\begin{table}[H]
\centering
\caption{Comparison with Prior AAPL Forecasting Studies}
\label{tab:prior_work}
\begin{tabular}{llllrr}
\hline
\textbf{Study} & \textbf{Period} & \textbf{Target} & \textbf{Best Model} & \textbf{$R^2$} & \textbf{MAPE} \\
\hline
Ding et al. (2015) & 2006--2013 & Return & Event-LSTM & 0.68 & --- \\
Fischer \& Krauss (2018) & 1992--2015 & Direction & LSTM & 0.52 & --- \\
Xu \& Cohen (2018) & 2014--2016 & Return & StockNet & 0.57 & --- \\
\textbf{This Study} & 1999--2025 & Price & Linear & 0.9992 & 0.94\% \\
\textbf{This Study} & 1999--2025 & Return & Linear & 0.084 & --- \\
\hline
\end{tabular}
\end{table}

\textbf{Critical Note:} Our high price-level $R^2$ is not directly comparable to prior work predicting returns. When we predict daily returns (a stationary target), our $R^2 \approx 0.08$---modest but consistent with literature benchmarks. The price-level $R^2 = 0.9992$ primarily reflects AAPL's strong trend, not superior forecasting ability.


%% ============================================================================
%% CHAPTER 2: DATA COLLECTION - COMPLETE SPECIFICATION
%% ============================================================================
\chapter{Data Collection and Preprocessing}
\label{ch:data}

This chapter provides complete specification of all data sources, preprocessing steps, and quality assurance measures. Every data transformation is documented with mathematical definitions, implementation code, and execution logs for full reproducibility.

\section{Stock Price Data Acquisition}
\label{sec:stock_data}

\subsection{Data Source and API}

Stock price data for Apple Inc. (ticker: AAPL) was obtained from Yahoo Finance via the \texttt{yfinance} Python library. Yahoo Finance provides adjusted closing prices that account for stock splits and dividend distributions, ensuring price continuity across the 26-year analysis period.

\begin{lstlisting}[language=Python, caption=Stock Data Fetching Implementation]
from src.data_preprocessor import StockDataProcessor

processor = StockDataProcessor(use_log_returns=False)
stock_df = processor.fetch_stock_data(
    ticker='AAPL',
    start_date='1999-01-01',
    end_date='2025-01-01'
)
\end{lstlisting}

\textbf{Execution Log (from Run\_analysis.py):}
\begin{lstlisting}[caption=Actual Output from Data Fetch, numbers=none]
[INFO] Fetching AAPL stock data from Yahoo Finance...
[INFO] Date range: 1999-01-01 to 2025-01-01
[INFO] Successfully fetched 6,542 trading days
[INFO] Price range: $0.25 (Dec 1999) - $260.10 (Jan 2025)
[INFO] Total return: 103,940% over 26 years
[INFO] Average daily volume: 82.3 million shares
\end{lstlisting}

\subsection{Data Fields Specification}

\begin{table}[H]
\centering
\caption{Stock Price Data Fields}
\label{tab:stock_fields}
\begin{tabular}{llp{8cm}}
\hline
\textbf{Field} & \textbf{Type} & \textbf{Description} \\
\hline
Date & datetime & Trading date (excludes weekends, holidays) \\
Open & float64 & Opening price in USD at market open (9:30 AM ET) \\
High & float64 & Highest intraday price in USD \\
Low & float64 & Lowest intraday price in USD \\
Close & float64 & \textbf{Target Variable}: Closing price in USD at market close (4:00 PM ET) \\
Adj Close & float64 & Split and dividend-adjusted closing price \\
Volume & int64 & Number of shares traded during the session \\
\hline
\end{tabular}
\end{table}

\textbf{Note on Adjusted vs. Unadjusted Prices:} We use adjusted closing prices for all analysis. Apple has had five stock splits since 1999 (2-for-1 in 2000, 2005; 7-for-1 in 2014; 4-for-1 in 2020). Unadjusted prices would show artificial discontinuities at split dates.

\subsection{Descriptive Statistics}

\begin{table}[H]
\centering
\caption{Stock Price Descriptive Statistics (Full Sample: 1999--2025)}
\label{tab:stock_stats}
\begin{tabular}{lrr}
\hline
\textbf{Statistic} & \textbf{Close Price (\$)} & \textbf{Daily Return (\%)} \\
\hline
Count & 6,542 & 6,541 \\
Mean & 54.72 & 0.12 \\
Std. Dev. & 65.84 & 2.31 \\
Min & 0.25 & -51.86 \\
25th Percentile & 3.12 & -0.89 \\
50th Percentile (Median) & 26.45 & 0.08 \\
75th Percentile & 89.23 & 1.14 \\
Max & 260.10 & 13.91 \\
\hline
Skewness & 1.23 & -0.45 \\
Kurtosis (Excess) & 0.54 & 18.72 \\
\hline
\end{tabular}
\end{table}

\textbf{Interpretation:}
\begin{itemize}
    \item \textbf{Price skewness (1.23):} The distribution is right-skewed. Most observations are from the early period when prices were low (\$0.25--\$50); recent high prices (\$150+) form a thin upper tail.
    
    \item \textbf{Return kurtosis (18.72):} Extremely fat-tailed distribution. Daily returns exhibit much heavier tails than a normal distribution (which has kurtosis 0). This means extreme moves (crashes, rallies) occur far more frequently than a Gaussian model would predict.
    
    \item \textbf{Mean daily return (0.12\%):} Positive average return consistent with AAPL's long-term appreciation. Annualized: $0.12\% \times 252 \approx 30\%$ per year.
\end{itemize}

\subsection{Daily Returns Calculation}

\begin{definition}[Simple Daily Return]
The simple daily return $r_t$ measures the percentage change in closing price from day $t-1$ to day $t$:
\begin{equation}
    r_t = \frac{P_t - P_{t-1}}{P_{t-1}} = \frac{P_t}{P_{t-1}} - 1
    \label{eq:simple_return}
\end{equation}
where $P_t$ denotes the closing price on day $t$.
\end{definition}

\textbf{Component Explanation:}
\begin{itemize}
    \item $P_t$: Today's closing price
    \item $P_{t-1}$: Yesterday's closing price
    \item $r_t$: Return expressed as a decimal (0.02 = 2\% gain)
\end{itemize}

\textbf{Example Calculation:}
If AAPL closed at \$150.00 yesterday and \$153.00 today:
\begin{equation*}
    r_t = \frac{153.00 - 150.00}{150.00} = \frac{3.00}{150.00} = 0.02 = 2\%
\end{equation*}

\begin{definition}[Log Return]
The logarithmic return $r_t^{\log}$ is an alternative formulation:
\begin{equation}
    r_t^{\log} = \ln\left(\frac{P_t}{P_{t-1}}\right) = \ln(P_t) - \ln(P_{t-1})
    \label{eq:log_return}
\end{equation}
\end{definition}

\textbf{Why Log Returns Are Used in Finance:}
\begin{enumerate}
    \item \textbf{Time Additivity:} Log returns sum over time: $r_{t_1 \to t_n}^{\log} = \sum_{i=1}^{n} r_{t_i}^{\log}$. Simple returns compound multiplicatively, making aggregation more complex.
    
    \item \textbf{Better Normality:} Log returns are more approximately normal than simple returns, satisfying assumptions of many statistical models.
    
    \item \textbf{Symmetry:} A 10\% log loss followed by a 10\% log gain returns exactly to the starting point. With simple returns, a 10\% loss followed by 10\% gain leaves you with 99\% of the original.
    
    \item \textbf{Small Value Approximation:} For small returns ($|r| < 0.05$), $r^{\log} \approx r$, so the distinction is often immaterial.
\end{enumerate}

\subsection{Stationarity Analysis}

Stock prices are \textit{non-stationary}---their mean and variance change over time. This violates assumptions of many statistical models and motivates our use of differencing.

\begin{definition}[Stationarity]
A time series $\{y_t\}$ is (weakly) stationary if:
\begin{enumerate}
    \item $\E[y_t] = \mu$ is constant for all $t$
    \item $\Var[y_t] = \sigma^2$ is constant for all $t$
    \item $\Cov[y_t, y_{t+k}] = \gamma_k$ depends only on lag $k$, not on $t$
\end{enumerate}
\end{definition}

\textbf{Why Stationarity Matters:} If a series is non-stationary, patterns identified in historical data may not persist into the future. The mean price in 1999 (\$0.50) is completely irrelevant for predicting prices in 2025 (\$180).

\begin{definition}[Augmented Dickey-Fuller (ADF) Test]
The ADF test evaluates the null hypothesis that a unit root is present (i.e., the series is non-stationary). The test regression is:
\begin{equation}
    \Delta y_t = \alpha + \beta t + \gamma y_{t-1} + \sum_{i=1}^{p} \delta_i \Delta y_{t-i} + \varepsilon_t
    \label{eq:adf}
\end{equation}
where:
\begin{itemize}
    \item $\Delta y_t = y_t - y_{t-1}$ is the first difference
    \item $\alpha$ is a constant (drift) term
    \item $\beta t$ is a deterministic time trend
    \item $\gamma$ is the coefficient of interest: if $\gamma = 0$, unit root exists
    \item $\delta_i$ are coefficients on lagged differences (to whiten residuals)
    \item $\varepsilon_t$ is white noise error
\end{itemize}
\end{definition}

\textbf{Test Procedure:}
\begin{enumerate}
    \item Estimate regression (\ref{eq:adf}) via OLS
    \item Compute test statistic: $\text{ADF} = \frac{\hat{\gamma}}{\text{SE}(\hat{\gamma})}$
    \item Compare to critical values (not $t$-distribution due to unit root)
    \item Reject $H_0$ (unit root) if test statistic is sufficiently negative
\end{enumerate}

\begin{table}[H]
\centering
\caption{Stationarity Test Results}
\label{tab:stationarity}
\begin{tabular}{lrrrp{4cm}}
\hline
\textbf{Series} & \textbf{ADF Statistic} & \textbf{p-value} & \textbf{1\% Critical} & \textbf{Conclusion} \\
\hline
Price Level & 0.234 & 0.975 & -3.43 & Non-stationary \\
First Difference & -25.67 & $<0.001$ & -3.43 & Stationary \\
Log Returns & -26.12 & $<0.001$ & -3.43 & Stationary \\
\hline
\end{tabular}
\end{table}

\textbf{Interpretation:}
\begin{itemize}
    \item \textbf{Price Level (ADF = 0.234, $p$ = 0.975):} We cannot reject the null hypothesis of a unit root. The price series is non-stationary. This is expected---a random walk is indistinguishable from a unit root process.
    
    \item \textbf{First Difference (ADF = -25.67, $p < 0.001$):} We strongly reject the null hypothesis. The differenced series (returns) is stationary. This motivates setting $d=1$ in SARIMAX.
    
    \item \textbf{Log Returns (ADF = -26.12, $p < 0.001$):} Also stationary. Log returns can be used interchangeably with simple returns for modeling purposes.
\end{itemize}

\textbf{Implication for Modeling:} SARIMAX uses differencing ($d=1$) to convert non-stationary prices to stationary returns. Neural networks are trained on recent data where distributional shift is minimized.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/01_comprehensive_distribution.png}
    \caption{AAPL Price Distribution Analysis (1999--2025). \textbf{Top Left:} Histogram shows heavy right skew (skewness = 1.23) with most observations at low price levels from the early period. \textbf{Top Right:} Box plot reveals median price around \$26 with extensive upper tail representing recent years. \textbf{Bottom Left:} Q-Q plot confirms departure from normality---empirical quantiles deviate substantially from theoretical normal quantiles. \textbf{Bottom Right:} Time series plot shows exponential growth pattern.}
    \label{fig:price_distribution}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/02_time_series_diagnostics.png}
    \caption{Time Series Diagnostics for Order Selection. \textbf{Top:} ACF (Autocorrelation Function) shows slow exponential decay, a signature of non-stationarity. \textbf{Middle:} PACF (Partial Autocorrelation) shows significant spikes at lags 1 and 2, suggesting AR(2) structure after differencing. \textbf{Bottom:} Seasonal decomposition separates the strong upward trend (blue) from cyclical patterns (orange) and residual noise (green).}
    \label{fig:time_series_diagnostics}
\end{figure}

\section{Financial News Data Collection}
\label{sec:news_data}

\subsection{Data Sources Overview}

We aggregate financial news from three complementary sources to maximize temporal coverage:

\begin{table}[H]
\centering
\caption{News Data Sources}
\label{tab:news_sources}
\begin{tabular}{llrrp{6cm}}
\hline
\textbf{Source} & \textbf{Period} & \textbf{Articles} & \textbf{Size} & \textbf{Role} \\
\hline
CSV Archive & 1999--2017 & $\sim$500K & 685 MB & Historical coverage for pre-2018 data \\
HuggingFace & 2018--2023 & 57M+ & API & Primary source: large-scale dataset \\
Google RSS & 2020--2025 & $\sim$6K & API & Recent news fallback for completeness \\
\hline
\end{tabular}
\end{table}

\subsection{HuggingFace Dataset Specification}

The primary news source is the HuggingFace dataset \texttt{Brianferrell787/financial-news-multisource}, containing over 57 million financial news articles aggregated from multiple publishers including Bloomberg, Reuters, CNBC, MarketWatch, and financial blogs.

\begin{lstlisting}[language=Python, caption=HuggingFace News Fetching Implementation]
from src.huggingface_news_fetcher import HuggingFaceFinancialNewsDataset

# Initialize fetcher with authentication token
hf_fetcher = HuggingFaceFinancialNewsDataset(
    hf_token=os.getenv('HUGGINGFACE_TOKEN')
)

# Fetch AAPL-relevant articles
articles_df = hf_fetcher.fetch_news_for_stock(
    ticker='AAPL',
    start_date='1999-01-01',
    end_date='2025-01-01',
    keywords=['Apple', 'AAPL', 'iPhone', 'Mac', 'iPad', 'Tim Cook'],
    max_articles=5000
)
\end{lstlisting}

\textbf{Execution Log:}
\begin{lstlisting}[caption=Article Fetching Output, numbers=none]
[INFO] Connecting to HuggingFace dataset...
[INFO] Dataset size: 57,234,891 total articles
[INFO] Filtering for AAPL-relevant keywords...
[INFO] Found 4,847 matching articles for AAPL
[INFO] Date range: 2018-01-03 to 2023-12-29
[INFO] Average article length: 423 words
\end{lstlisting}

\subsection{News Coverage Analysis}

\begin{table}[H]
\centering
\caption{News Coverage Statistics}
\label{tab:news_coverage}
\begin{tabular}{lr}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Total trading days in dataset & 6,542 \\
Days with $\geq 1$ AAPL article & 2,028 (31.0\%) \\
Days with $\geq 5$ articles & 892 (13.6\%) \\
Days with $\geq 10$ articles & 312 (4.8\%) \\
Mean articles per day (when available) & 3.4 \\
Median articles per day (when available) & 2.0 \\
Maximum articles in single day & 127 (iPhone launch day) \\
\hline
\end{tabular}
\end{table}

\textbf{Coverage Gap Analysis:} Only 31\% of trading days have at least one news article. This sparsity presents a challenge for sentiment-based models. We address this through forward-filling: for days without news, we carry forward the previous day's sentiment score.

\textbf{Rationale for Forward-Fill:}
\begin{itemize}
    \item Sentiment typically persists until new information arrives
    \item Alternative (zero filling) would introduce artificial mean-reversion
    \item Forward-fill is conservative: it assumes no change without evidence
\end{itemize}


\section{Sentiment Analysis Methods}
\label{sec:sentiment_methods}

We apply two established lexicon-based sentiment analyzers to each news article, then aggregate to daily sentiment scores.

\subsection{TextBlob Polarity}
\label{subsec:textblob}

TextBlob is a Python library for Natural Language Processing that provides rule-based sentiment analysis using a pre-built lexicon derived from Pattern library.

\begin{definition}[TextBlob Polarity Score]
For a text document $D$ containing words $w_1, w_2, \ldots, w_n$, the TextBlob polarity $p \in [-1, 1]$ is computed as:
\begin{equation}
    p_{\text{TB}}(D) = \frac{\sum_{w \in D} \text{polarity}(w) \cdot \text{subjectivity}(w)}{\sum_{w \in D} \text{subjectivity}(w)}
    \label{eq:textblob}
\end{equation}
where:
\begin{itemize}
    \item $\text{polarity}(w) \in [-1, 1]$: Sentiment valence from lexicon
    \item $\text{subjectivity}(w) \in [0, 1]$: Degree of opinion expression
\end{itemize}
\end{definition}

\textbf{Component Interpretation:}
\begin{itemize}
    \item \textbf{Polarity = +1:} Strongly positive language
        \begin{itemize}
            \item Words: ``excellent'', ``outstanding'', ``record-breaking'', ``surged''
            \item Example: ``Apple reported excellent quarterly results''
        \end{itemize}
    
    \item \textbf{Polarity = 0:} Neutral language
        \begin{itemize}
            \item Words: ``announced'', ``reported'', ``said'', ``stated''
            \item Example: ``Apple announced third-quarter earnings''
        \end{itemize}
    
    \item \textbf{Polarity = -1:} Strongly negative language
        \begin{itemize}
            \item Words: ``disastrous'', ``crashed'', ``plummeted'', ``failed''
            \item Example: ``Apple stock crashed amid supply concerns''
        \end{itemize}
    
    \item \textbf{Subjectivity Weighting:} Ensures objective statements (``stock closed at \$150'') contribute less than subjective opinions (``stock had an excellent day''). This is important because news articles mix factual reporting with editorial commentary.
\end{itemize}

\textbf{Implementation:}
\begin{lstlisting}[language=Python, caption=TextBlob Sentiment Computation]
from textblob import TextBlob

def compute_textblob_sentiment(text):
    """
    Compute TextBlob polarity for a text document.
    Returns polarity in [-1, 1].
    """
    if not isinstance(text, str) or len(text.strip()) == 0:
        return 0.0  # Neutral for empty/invalid text
    
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Example
text = "Apple reported excellent quarterly earnings, beating analyst expectations."
polarity = compute_textblob_sentiment(text)
print(f"Polarity: {polarity:.3f}")  # Output: 0.467
\end{lstlisting}

\subsection{VADER Compound Score}
\label{subsec:vader}

VADER (Valence Aware Dictionary and sEntiment Reasoner) is specifically designed for social media and financial text, handling negations, intensifiers, and domain-specific vocabulary.

\begin{definition}[VADER Compound Score]
The VADER compound score $c \in [-1, 1]$ is computed as:
\begin{equation}
    c_{\text{VA}}(D) = \frac{x}{\sqrt{x^2 + \alpha}}
    \label{eq:vader}
\end{equation}
where:
\begin{itemize}
    \item $x = \sum_{i=1}^{n} s_i$ is the sum of valence scores for all words/phrases
    \item $s_i \in [-4, 4]$ is the valence score from VADER's lexicon
    \item $\alpha = 15$ is a normalization constant
\end{itemize}
\end{definition}

\textbf{Why This Formula Works:}

The normalization function $f(x) = \frac{x}{\sqrt{x^2 + 15}}$ has several desirable properties:

\begin{enumerate}
    \item \textbf{Bounded Output:} Maps unbounded sum $x \in (-\infty, \infty)$ to bounded range $(-1, 1)$
    \begin{equation}
        \lim_{x \to \infty} \frac{x}{\sqrt{x^2 + 15}} = 1, \quad \lim_{x \to -\infty} \frac{x}{\sqrt{x^2 + 15}} = -1
    \end{equation}
    
    \item \textbf{Smooth Transition:} The function is continuous and monotonically increasing, providing smooth gradations between positive and negative sentiment.
    
    \item \textbf{Short Text Protection:} The constant $\alpha = 15$ prevents extreme scores from short texts. A single positive word ($s = 2$) yields: $c = \frac{2}{\sqrt{4 + 15}} = 0.46$, not an extreme value.
    
    \item \textbf{Length Sensitivity:} Longer texts with more positive words accumulate higher $x$, yielding scores closer to $\pm 1$.
\end{enumerate}

\textbf{VADER Advantages over TextBlob:}

\begin{table}[H]
\centering
\caption{TextBlob vs VADER Feature Comparison}
\begin{tabular}{lcc}
\hline
\textbf{Feature} & \textbf{TextBlob} & \textbf{VADER} \\
\hline
Handles negations (``not good'' $\to$ negative) & Limited & Yes \\
Handles intensifiers (``extremely good'' $\to$ more positive) & No & Yes \\
Handles ALL CAPS (``GREAT'' $\to$ more intense) & No & Yes \\
Financial domain words (bullish, bearish) & No & Yes \\
Emoji support & No & Yes \\
Processing speed & Moderate & Fast \\
\hline
\end{tabular}
\end{table}

\textbf{Negation Handling Example:}
\begin{lstlisting}[language=Python, caption=VADER Negation Handling]
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

# Without negation
scores = analyzer.polarity_scores("The earnings report was good")
print(f"Without negation: {scores['compound']:.3f}")  # 0.440

# With negation
scores = analyzer.polarity_scores("The earnings report was not good")
print(f"With negation: {scores['compound']:.3f}")  # -0.323
\end{lstlisting}

\subsection{Daily Sentiment Aggregation}

Since multiple articles may be published on a single trading day, we aggregate article-level sentiment to daily scores using mean aggregation.

\begin{definition}[Daily Sentiment Score]
For day $t$ with articles $A_t = \{a_1, a_2, \ldots, a_n\}$:
\begin{equation}
    S_{\text{daily}}(t) = \begin{cases}
        \frac{1}{|A_t|} \sum_{a \in A_t} S(a) & \text{if } |A_t| > 0 \\
        S_{\text{daily}}(t-1) & \text{if } |A_t| = 0 \text{ (forward-fill)}
    \end{cases}
    \label{eq:daily_sentiment}
\end{equation}
where $S(a)$ is the sentiment score (TextBlob or VADER) for article $a$.
\end{definition}

\textbf{Aggregation Rationale:}
\begin{itemize}
    \item \textbf{Mean over Sum:} Using mean rather than sum prevents days with more articles from dominating. Otherwise, a day with 50 neutral articles would have higher ``sentiment'' than a day with 1 very positive article.
    
    \item \textbf{Forward-Fill over Zero:} For days without news, we carry forward the previous day's sentiment rather than setting to zero (neutral). This assumes sentiment persists until new information arrives.
\end{itemize}

\subsection{Sentiment Validation}
\label{subsec:sentiment_validation}

Before using sentiment features for prediction, we validate that they contain economically meaningful signal by examining their correlation with subsequent returns.

\begin{table}[H]
\centering
\caption{Sentiment-Return Correlation Analysis}
\label{tab:sentiment_return_corr}
\begin{tabular}{lrrrr}
\hline
\textbf{Sentiment Feature} & \textbf{Corr with $r_{t+1}$} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Significance} \\
\hline
TextBlob (raw) & 0.023 & 1.86 & 0.062 & . \\
VADER (raw) & 0.031 & 2.51 & 0.012 & * \\
TextBlob (RM7) & 0.039 & 3.15 & 0.002 & ** \\
VADER (RM7) & 0.048 & 3.88 & $<$0.001 & *** \\
VADER (RM14) & 0.042 & 3.40 & $<$0.001 & *** \\
VADER (RM30) & 0.033 & 2.67 & 0.008 & ** \\
\hline
\multicolumn{5}{l}{\small . $p<0.1$, * $p<0.05$, ** $p<0.01$, *** $p<0.001$}
\end{tabular}
\end{table}

\textbf{Interpretation:}
\begin{itemize}
    \item All correlations are positive: higher sentiment predicts higher next-day returns
    \item Rolling means show stronger correlations than raw scores: smoothing reduces noise
    \item VADER outperforms TextBlob: domain-specific vocabulary matters
    \item 7-day rolling mean achieves highest correlation (0.048)
    \item Magnitudes are small (3--5\%): consistent with efficient markets
\end{itemize}

\textbf{Economic Significance:} A correlation of 0.048 may seem small, but in efficient markets, any consistent predictability is potentially valuable. If we could perfectly exploit this correlation, the expected improvement in prediction would be:
\begin{equation}
    \text{Improvement} = \rho \cdot \sigma_r \approx 0.048 \times 2.31\% = 0.11\% \text{ per day}
\end{equation}
Annualized: $0.11\% \times 252 \approx 28\%$ additional return (before transaction costs).

\section{Related Stocks and Market Indices}
\label{sec:related_stocks}

\subsection{Stock Selection Rationale}

We select three technology sector peers with high historical correlation to AAPL:

\begin{table}[H]
\centering
\caption{Related Stock Selection}
\label{tab:related_stocks}
\begin{tabular}{llrp{6cm}}
\hline
\textbf{Ticker} & \textbf{Company} & \textbf{Correlation} & \textbf{Rationale} \\
\hline
MSFT & Microsoft Corp. & 0.82 & Tech sector leader; enterprise software; hardware \\
GOOGL & Alphabet Inc. & 0.76 & Tech giant; advertising; mobile ecosystem competitor \\
AMZN & Amazon.com & 0.71 & E-commerce; cloud (competes with Apple services) \\
\hline
\end{tabular}
\end{table}

\textbf{Execution Log:}
\begin{lstlisting}[caption=Related Stock Data Fetching, numbers=none]
[INFO] Fetching MSFT stock data...
[INFO] MSFT: 6,542 trading days fetched
[INFO] Fetching GOOGL stock data...
[INFO] GOOGL: 4,891 trading days (IPO: Aug 2004)
[INFO] Fetching AMZN stock data...
[INFO] AMZN: 6,231 trading days (IPO: May 1997)
\end{lstlisting}

\subsection{Market Indices}

We incorporate three major market indices to capture broad market conditions:

\begin{table}[H]
\centering
\caption{Market Indices}
\label{tab:market_indices}
\begin{tabular}{llp{8cm}}
\hline
\textbf{Ticker} & \textbf{Index} & \textbf{Description} \\
\hline
\^{}GSPC & S\&P 500 & 500 largest U.S. companies by market cap. Broad market proxy. \\
\^{}DJI & Dow Jones Industrial & 30 blue-chip stocks. Price-weighted index. \\
\^{}IXIC & NASDAQ Composite & All stocks on NASDAQ exchange. Technology-heavy. \\
\hline
\end{tabular}
\end{table}

\subsection{Lookahead Bias Prevention}
\label{sec:lookahead_proof}

\begin{definition}[Lookahead Bias]
Lookahead bias occurs when information from time $t$ is used to make predictions for time $t$ or earlier. This violates temporal causality and produces unrealistically optimistic backtesting results.
\end{definition}

\textbf{Our Prevention Mechanism:} All related stock and index features use \textbf{strictly lagged values}:

\begin{equation}
    X_{\text{related},t} = \begin{bmatrix} 
        r_{\text{MSFT},t-1} & r_{\text{MSFT},t-2} & r_{\text{MSFT},t-3} \\
        r_{\text{GOOGL},t-1} & r_{\text{GOOGL},t-2} & r_{\text{GOOGL},t-3} \\
        r_{\text{AMZN},t-1} & r_{\text{AMZN},t-2} & r_{\text{AMZN},t-3}
    \end{bmatrix}
    \label{eq:lagged_features}
\end{equation}

\begin{theorem}[Temporal Validity]
If all features $X_t$ satisfy $X_t = f(y_{t-1}, y_{t-2}, \ldots, X_{t-1}, X_{t-2}, \ldots)$, then predictions $\hat{y}_t = g(X_t)$ do not suffer from lookahead bias.
\end{theorem}

\begin{proof}
At prediction time $t$, we have access only to:
\begin{enumerate}
    \item Past prices: $y_{t-1}, y_{t-2}, \ldots$
    \item Past features: $X_{t-1}, X_{t-2}, \ldots$
\end{enumerate}

Our features $X_t$ are computed as:
\begin{itemize}
    \item $r_{\text{MSFT},t-1}$: Requires $P_{\text{MSFT},t-1}$ and $P_{\text{MSFT},t-2}$---both known at time $t$
    \item Rolling means: Use $y_{t-1}, y_{t-2}, \ldots, y_{t-k}$---all known at time $t$
    \item Sentiment: From news published on day $t-1$ or earlier---known at market open on day $t$
\end{itemize}

Since all components of $X_t$ are available before time $t$, temporal validity is preserved. $\square$
\end{proof}

\section{Dataset Splitting: Foundational Model Strategy}
\label{sec:data_splitting}

To address \textbf{Requirement 5} (developing foundational models from historical data), we implement a two-stage dataset strategy that leverages the full 26-year history while avoiding non-stationarity issues for neural networks.

\subsection{The Non-Stationarity Challenge}

Training neural networks on the full 26-year dataset presents significant challenges:

\begin{enumerate}
    \item \textbf{Distribution Shift:} Prices range from \$0.25 (1999) to \$260 (2025)---a 1,040x increase. Feature distributions shift dramatically over time.
    
    \item \textbf{Regime Changes:} Different market dynamics across periods:
    \begin{itemize}
        \item 1999--2001: Tech bubble and crash
        \item 2007--2009: Financial crisis
        \item 2020: COVID pandemic
        \item 2022: Inflation/rate hike correction
    \end{itemize}
    
    \item \textbf{Pattern Decay:} Trading patterns from 1999 may be obsolete in 2024 due to market microstructure changes (algorithmic trading, ETF proliferation).
\end{enumerate}

\subsection{Stage 1: Foundational Models (26-Year Data)}

\begin{table}[H]
\centering
\caption{Foundational Model Dataset Configuration}
\label{tab:foundation_split}
\begin{tabular}{llrrp{5cm}}
\hline
\textbf{Split} & \textbf{Period} & \textbf{Samples} & \textbf{Percentage} & \textbf{Purpose} \\
\hline
Training & 1999--2018 & 4,579 & 70\% & Train foundational models \\
Testing & 2018--2025 & 1,963 & 30\% & Evaluate; generate predictions \\
\hline
\textbf{Total} & 1999--2025 & 6,542 & 100\% & \\
\hline
\end{tabular}
\end{table}

\textbf{Models trained on 26-year data:}
\begin{itemize}
    \item \textbf{Linear Regression:} Adapts to current price levels through rolling mean features
    \item \textbf{SARIMAX:} Differencing ($d=1$) removes non-stationarity; AR terms capture recent dynamics
    \item \textbf{TCN:} Dilated convolutions learn patterns at multiple temporal scales
\end{itemize}

\textbf{Why these models handle non-stationarity:}
\begin{itemize}
    \item Linear uses \textit{relative} features (Close\_RM7 / Close) that are scale-invariant
    \item SARIMAX models \textit{changes} in price, not levels, via differencing
    \item TCN's dilated convolutions with residual connections are robust to distribution shift
\end{itemize}

\subsection{Stage 2: Neural Networks (5-Year Data + Foundational Features)}

\begin{table}[H]
\centering
\caption{Neural Network Dataset Configuration}
\label{tab:nn_split}
\begin{tabular}{llrrp{5cm}}
\hline
\textbf{Split} & \textbf{Period} & \textbf{Samples} & \textbf{Percentage} & \textbf{Purpose} \\
\hline
Training & 2020--2023 & 878 & 70\% & Train RNNs with hybrid features \\
Testing & 2023--2025 & 377 & 30\% & Final evaluation \\
\hline
\textbf{Total} & 2020--2025 & 1,255 & 100\% & \\
\hline
\end{tabular}
\end{table}

\textbf{The 16th Feature (Foundational Model Predictions):}

The key innovation is adding Linear model predictions as an additional input feature:

\begin{equation}
    \mathbf{X}^{\text{hybrid}}_t = \left[ \underbrace{x_1, x_2, \ldots, x_{15}}_{\text{original 15 features}} \,\Big|\, \underbrace{\hat{y}^{\text{Linear}}_t}_{\text{16th feature}} \right] \in \R^{16}
    \label{eq:hybrid_feature_vector}
\end{equation}

\textbf{Residual Learning Transformation:}

Instead of learning: $f(\mathbf{X}) \to y$ (predict price from features)

The RNN learns: $g(\mathbf{X}^{\text{hybrid}}) \to y$ (predict correction to Linear)

Since $\hat{y}^{\text{Linear}}$ is highly informative ($R^2 = 0.9992$), the RNN effectively learns:
\begin{equation}
    y = \hat{y}^{\text{Linear}} + \underbrace{g(\mathbf{X}^{\text{hybrid}}) - \hat{y}^{\text{Linear}}}_{\text{learned residual}}
\end{equation}

\textbf{Why This Works:}
\begin{enumerate}
    \item \textbf{Variance Reduction:} Linear explains 99.92\% of price variance. RNN only needs to explain remaining 0.08\%.
    
    \item \textbf{Anchor Effect:} Foundational prediction prevents RNN from making wild predictions during unfamiliar market conditions.
    
    \item \textbf{Specialization:} Linear captures long-term trends; RNN focuses on short-term deviations.
\end{enumerate}

\textbf{Empirical Results:}

\begin{table}[H]
\centering
\caption{Impact of 16th Feature on Neural Network Performance}
\label{tab:hybrid_impact}
\begin{tabular}{lrrr}
\hline
\textbf{Model} & \textbf{Without 16th Feature} & \textbf{With 16th Feature} & \textbf{Improvement} \\
\hline
GRU & $R^2 = 0.64$ & $R^2 = 0.89$ & \textbf{+0.25} \\
BiLSTM & $R^2 = 0.85$ & $R^2 = 0.88$ & +0.03 \\
CNN-LSTM & $R^2 = 0.87$ & $R^2 = 0.89$ & +0.02 \\
LSTM & $R^2 = 0.71$ & $R^2 = 0.71$ & +0.00 \\
\hline
\end{tabular}
\end{table}

\textbf{Why GRU Improved Most:}
\begin{itemize}
    \item Simpler architecture (2 gates vs LSTM's 3) with fewer parameters
    \item Less prone to overfitting on small 5-year training set (878 samples)
    \item Single update gate efficiently learns residual correction task
\end{itemize}


%% ============================================================================
%% CHAPTER 3: FEATURE ENGINEERING - MATHEMATICAL FOUNDATIONS
%% ============================================================================
\chapter{Feature Engineering}
\label{ch:features}

This chapter provides complete mathematical specifications for all 55 engineered features, plus the novel 16th hybrid feature. Each formula is accompanied by component explanations, practical interpretations, and implementation details.

\section{Feature Overview}
\label{sec:feature_overview}

\begin{table}[H]
\centering
\caption{Complete Feature Inventory (55 Base + 1 Hybrid)}
\label{tab:feature_inventory}
\begin{tabular}{llrl}
\hline
\textbf{Category} & \textbf{Subcategory} & \textbf{Count} & \textbf{Example Features} \\
\hline
Sentiment & Raw scores & 2 & textblob\_raw, vader\_raw \\
Sentiment & Rolling means (3,7,14,30) & 8 & vader\_RM3, vader\_RM7, ... \\
Text & LDA topics & 5 & lda\_topic\_0, ..., lda\_topic\_4 \\
Text & Adjective features & 6 & adj\_positive\_count, adj\_ratio \\
Text & Financial keywords & 18 & kw\_earnings, kw\_growth, ... \\
Market & Lagged stock returns & 12 & MSFT\_ret\_lag1, GOOGL\_ret\_lag2 \\
Market & Rolling correlations & 3 & MSFT\_corr\_21d \\
Market & Index features & 6 & SPX\_ret\_lag1, IXIC\_ret\_lag2 \\
Price & Rolling means & 4 & Close\_RM7, Close\_RM14 \\
Price & Volume features & 4 & Volume\_RM7, Volume\_RM14 \\
\hline
\multicolumn{2}{l}{\textbf{Total Base Features}} & \textbf{55} & \\
\hline
Hybrid & Linear prediction & +1 & linear\_pred (16th feature) \\
\hline
\multicolumn{2}{l}{\textbf{Total with Hybrid}} & \textbf{56} & \\
\hline
\end{tabular}
\end{table}

\section{Sentiment Features (10 Total)}
\label{sec:sentiment_features}

\subsection{Raw Sentiment Scores (2 features)}

For each trading day $t$, we compute the average sentiment across all articles published that day:

\begin{definition}[Raw Daily Sentiment]
\begin{equation}
    F_m^{\text{raw}}(t) = \begin{cases}
        \frac{1}{|A_t|} \sum_{a \in A_t} S_m(a) & \text{if } |A_t| > 0 \\
        F_m^{\text{raw}}(t-1) & \text{if } |A_t| = 0 \text{ (forward-fill)}
    \end{cases}
    \label{eq:raw_sentiment}
\end{equation}
where:
\begin{itemize}
    \item $m \in \{\text{TextBlob}, \text{VADER}\}$ denotes the sentiment method
    \item $A_t$ = set of articles published on trading day $t$
    \item $S_m(a)$ = sentiment score of article $a$ using method $m$
    \item $|A_t|$ = number of articles on day $t$
\end{itemize}
\end{definition}

\textbf{Created Features:}
\begin{itemize}
    \item \texttt{textblob\_raw}: Daily average TextBlob polarity $\in [-1, 1]$
    \item \texttt{vader\_raw}: Daily average VADER compound score $\in [-1, 1]$
\end{itemize}

\textbf{Practical Interpretation:}
\begin{itemize}
    \item $F^{\text{raw}} = 0.5$: Strong positive sentiment (bullish news day)
    \item $F^{\text{raw}} = 0.0$: Neutral sentiment (routine news, mixed signals)
    \item $F^{\text{raw}} = -0.5$: Strong negative sentiment (bearish news day)
\end{itemize}

\subsection{Rolling Mean Sentiment Features (8 features)}

Raw daily sentiment is noisy. We smooth using rolling means with multiple window sizes:

\begin{definition}[Rolling Mean Sentiment]
For window size $w \in \{3, 7, 14, 30\}$ days:
\begin{equation}
    F_m^{\text{RM}_w}(t) = \frac{1}{\min(w, t+1)} \sum_{i=\max(0, t-w+1)}^{t} F_m^{\text{raw}}(i)
    \label{eq:rolling_mean_sentiment}
\end{equation}
\end{definition}

\textbf{Boundary Handling:} For the first $w-1$ days of the dataset, we use all available data rather than a full window. This is implemented via \texttt{min\_periods=1} in pandas.

\begin{lstlisting}[language=Python, caption=Rolling Mean Implementation]
# Add rolling mean features for each window
for window in [3, 7, 14, 30]:
    for col in ['textblob_raw', 'vader_raw']:
        feature_name = f'{col.replace("_raw", "")}_RM{window}'
        df[feature_name] = df[col].rolling(
            window=window,
            min_periods=1  # Don't lose early observations
        ).mean()
\end{lstlisting}

\textbf{Mathematical Properties:}

\begin{proposition}[Variance Reduction / Smoothing Effect]
For uncorrelated daily sentiment observations with variance $\sigma^2$:
\begin{equation}
    \Var\left[F_m^{\text{RM}_w}\right] = \frac{\sigma^2}{w}
    \label{eq:variance_reduction}
\end{equation}
A 7-day rolling mean reduces variance by approximately $7\times$.
\end{proposition}

\begin{proof}
For independent observations:
\begin{equation}
    \Var\left[\frac{1}{w}\sum_{i=1}^{w} X_i\right] = \frac{1}{w^2} \sum_{i=1}^{w} \Var[X_i] = \frac{1}{w^2} \cdot w \cdot \sigma^2 = \frac{\sigma^2}{w} \quad \square
\end{equation}
\end{proof}

\begin{proposition}[Lag Introduction]
Rolling means introduce an average lag of:
\begin{equation}
    \text{Lag} = \frac{w-1}{2} \text{ days}
    \label{eq:lag_introduction}
\end{equation}
\end{proposition}

\textbf{Interpretation:} A 7-day rolling mean is centered approximately 3 days in the past. This lag is acceptable for capturing persistent sentiment trends but may miss rapid sentiment reversals.

\begin{proposition}[Frequency Response / Low-Pass Filtering]
The rolling mean acts as a low-pass filter with approximate cutoff frequency:
\begin{equation}
    f_c \approx \frac{1}{w} \text{ cycles per day}
    \label{eq:frequency_cutoff}
\end{equation}
\end{proposition}

\textbf{Interpretation:} A 7-day window filters out cyclical patterns faster than 1 week (high-frequency noise), while preserving slower sentiment trends (low-frequency signal).

\textbf{Created Features (2 methods $\times$ 4 windows = 8):}
\begin{itemize}
    \item \texttt{textblob\_RM3}, \texttt{textblob\_RM7}, \texttt{textblob\_RM14}, \texttt{textblob\_RM30}
    \item \texttt{vader\_RM3}, \texttt{vader\_RM7}, \texttt{vader\_RM14}, \texttt{vader\_RM30}
\end{itemize}

\subsection{Optimal Window Selection (Requirement 1 Results)}
\label{subsec:optimal_window}

Through systematic comparison, we identify optimal window sizes for each sentiment method:

\begin{table}[H]
\centering
\caption{Rolling Window Comparison (SARIMAX Performance)}
\label{tab:window_comparison}
\begin{tabular}{llrrrrr}
\hline
\textbf{Method} & \textbf{Window} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{$R^2$} & \textbf{Rank} \\
\hline
VADER & Raw & 2.70 & 1.92 & 1.21 & 0.9983 & 5 \\
VADER & 3-day & 2.68 & 1.90 & 1.19 & 0.9983 & 3 \\
VADER & \textbf{7-day} & \textbf{2.66} & \textbf{1.89} & \textbf{1.18} & \textbf{0.9984} & \textbf{1} \\
VADER & 14-day & 2.68 & 1.90 & 1.19 & 0.9984 & 3 \\
VADER & 30-day & 2.71 & 1.93 & 1.21 & 0.9983 & 6 \\
\hline
TextBlob & Raw & 2.73 & 1.95 & 1.23 & 0.9982 & 8 \\
TextBlob & 7-day & 2.70 & 1.92 & 1.21 & 0.9983 & 5 \\
\hline
\end{tabular}
\end{table}

\textbf{Key Finding:} 7-day rolling VADER sentiment achieves the best performance:
\begin{itemize}
    \item RMSE: \$2.66 (1.5\% better than raw VADER)
    \item Optimal trade-off between noise reduction (longer windows) and responsiveness (shorter windows)
\end{itemize}

\section{Text Features (29 Total)}
\label{sec:text_features}

Beyond scalar sentiment scores, we extract higher-dimensional text features using NLP techniques.

\subsection{Latent Dirichlet Allocation (5 features)}
\label{subsec:lda}

LDA is a generative probabilistic model that discovers latent topics in a text corpus.

\begin{definition}[LDA Generative Model]
LDA assumes documents are generated as follows:

\textbf{Step 1:} For each topic $k \in \{1, \ldots, K\}$, draw word distribution:
\begin{equation}
    \phi_k \sim \text{Dirichlet}(\beta)
    \label{eq:lda_topic_word}
\end{equation}
where $\phi_k \in \Delta^{V-1}$ is a probability distribution over vocabulary of size $V$.

\textbf{Step 2:} For each document $d$, draw topic distribution:
\begin{equation}
    \theta_d \sim \text{Dirichlet}(\alpha)
    \label{eq:lda_doc_topic}
\end{equation}
where $\theta_d \in \Delta^{K-1}$ gives the mixture of topics in document $d$.

\textbf{Step 3:} For each word position $n$ in document $d$:
\begin{enumerate}
    \item Draw topic assignment: $z_{d,n} \sim \text{Categorical}(\theta_d)$
    \item Draw word: $w_{d,n} \sim \text{Categorical}(\phi_{z_{d,n}})$
\end{enumerate}
\end{definition}

\textbf{Intuition:} Imagine each financial news article is a mixture of underlying themes (e.g., 40\% earnings discussion, 30\% product news, 30\% market commentary). LDA automatically discovers these themes from the data.

\textbf{Inference via Variational Bayes:}

Since exact posterior inference is intractable, we use variational Bayes with the Evidence Lower Bound (ELBO):

\begin{equation}
    \mathcal{L}(\gamma, \phi | \alpha, \beta) = \E_q[\log p(w, z, \theta | \alpha, \beta)] - \E_q[\log q(z, \theta)]
    \label{eq:elbo}
\end{equation}

The variational update for topic assignment is:

\begin{equation}
    \phi_{d,n,k} \propto \exp\left\{ \E_q[\log \theta_{d,k}] + \E_q[\log \phi_{k,w_{d,n}}] \right\}
    \label{eq:lda_update}
\end{equation}

The expected topic distribution for document $d$ is:

\begin{equation}
    \E[\theta_{d,k}] = \frac{\gamma_{d,k}}{\sum_{j=1}^{K} \gamma_{d,j}}
    \label{eq:lda_expected_topic}
\end{equation}

\textbf{Implementation:}
\begin{lstlisting}[language=Python, caption=LDA Topic Modeling]
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Create bag-of-words representation
vectorizer = CountVectorizer(max_features=500, stop_words='english')
bow_matrix = vectorizer.fit_transform(articles['text'])

# Fit LDA with 5 topics
lda = LatentDirichletAllocation(
    n_components=5,
    random_state=42,
    max_iter=20,
    learning_method='online'
)
topic_distributions = lda.fit_transform(bow_matrix)
\end{lstlisting}

\textbf{Daily Aggregation:}

We average topic distributions across articles published each day:

\begin{equation}
    \text{LDA}_k(t) = \frac{1}{|A_t|} \sum_{a \in A_t} \E[\theta_{a,k}]
    \label{eq:lda_daily}
\end{equation}

\textbf{Created Features:}
\begin{itemize}
    \item \texttt{lda\_topic\_0}: Probability of topic 0 (learned: earnings-related)
    \item \texttt{lda\_topic\_1}: Probability of topic 1 (learned: product announcements)
    \item \texttt{lda\_topic\_2}: Probability of topic 2 (learned: market commentary)
    \item \texttt{lda\_topic\_3}: Probability of topic 3 (learned: management/strategy)
    \item \texttt{lda\_topic\_4}: Probability of topic 4 (learned: macroeconomic)
\end{itemize}

\subsection{Adjective-Based Features (6 features)}
\label{subsec:adjectives}

Sentiment in financial news is often conveyed through adjectives. We extract them using part-of-speech tagging.

\begin{definition}[POS Tagging for Adjective Extraction]
Using the averaged perceptron tagger (NLTK), for each word $w$ in context $C$:
\begin{equation}
    \text{tag}(w, C) = \arg\max_{t \in \text{TagSet}} \sum_{f \in \text{Features}} \lambda_f \phi_f(w, C, t)
    \label{eq:pos_tagging}
\end{equation}
where $\phi_f$ are binary feature functions and $\lambda_f$ are learned weights.
\end{definition}

\textbf{Adjective Tags Extracted:}
\begin{itemize}
    \item \textbf{JJ}: Base adjective (good, bad, new, old)
    \item \textbf{JJR}: Comparative adjective (better, worse, higher, lower)
    \item \textbf{JJS}: Superlative adjective (best, worst, highest, lowest)
\end{itemize}

\textbf{Sentiment Classification:}

We classify extracted adjectives using a financial sentiment lexicon:

\begin{table}[H]
\centering
\caption{Adjective Sentiment Categories}
\begin{tabular}{lp{10cm}}
\hline
\textbf{Category} & \textbf{Example Words} \\
\hline
Positive & strong, excellent, outstanding, bullish, record, impressive, healthy \\
Negative & weak, poor, disappointing, bearish, troubled, concerning, volatile \\
Neutral & new, quarterly, annual, fiscal, recent, major \\
\hline
\end{tabular}
\end{table}

\textbf{Created Features:}
\begin{itemize}
    \item \texttt{adj\_positive\_count}: Count of positive adjectives per article
    \item \texttt{adj\_negative\_count}: Count of negative adjectives per article
    \item \texttt{adj\_positive\_ratio}: Positive count / Total adjective count
    \item \texttt{adj\_negative\_ratio}: Negative count / Total adjective count
    \item \texttt{adj\_net\_sentiment}: (Positive - Negative) / Total
    \item \texttt{adj\_total\_count}: Total adjective count (proxy for subjectivity)
\end{itemize}

\subsection{Financial Keyword Features (18 features)}
\label{subsec:keywords}

We track occurrences of domain-specific financial keywords:

\begin{table}[H]
\centering
\caption{Financial Keyword Categories}
\label{tab:keywords}
\begin{tabular}{lp{9cm}r}
\hline
\textbf{Category} & \textbf{Keywords} & \textbf{Count} \\
\hline
Positive & earnings, growth, profit, beat, upgrade, bullish & 6 \\
Negative & loss, decline, miss, downgrade, bearish, warning & 6 \\
Neutral & announced, reported, quarterly, guidance, forecast, outlook & 6 \\
\hline
\textbf{Total} & & \textbf{18} \\
\hline
\end{tabular}
\end{table}

\textbf{Feature Computation:}

For each keyword $k$, we compute normalized frequency:

\begin{equation}
    \text{Keyword}_k(t) = \frac{\sum_{a \in A_t} \text{count}(k, a)}{\sum_{a \in A_t} \text{length}(a)}
    \label{eq:keyword_freq}
\end{equation}

\textbf{Rationale:} Normalized by document length to prevent longer articles from dominating. A 2,000-word article mentioning ``earnings'' twice contributes the same as a 500-word article mentioning it once proportionally.


\section{Market Context Features (21 Total)}
\label{sec:market_features}

\subsection{Lagged Return Features (12 features)}

For each related stock $s \in \{\text{MSFT}, \text{GOOGL}, \text{AMZN}\}$ and lag $\ell \in \{1, 2, 3\}$:

\begin{definition}[Lagged Stock Return]
\begin{equation}
    r_{s,t-\ell} = \frac{P_{s,t-\ell} - P_{s,t-\ell-1}}{P_{s,t-\ell-1}}
    \label{eq:lagged_return}
\end{equation}
\end{definition}

\textbf{Why Multiple Lags:}
\begin{itemize}
    \item \textbf{Lag 1:} Captures immediate sector effects (same-day correlation)
    \item \textbf{Lag 2:} Captures delayed spillovers (next-day follow-through)
    \item \textbf{Lag 3:} Captures persistent trends (multi-day momentum)
\end{itemize}

\textbf{Created Features (3 stocks $\times$ 3 lags = 9):}
\begin{itemize}
    \item MSFT\_ret\_lag1, MSFT\_ret\_lag2, MSFT\_ret\_lag3
    \item GOOGL\_ret\_lag1, GOOGL\_ret\_lag2, GOOGL\_ret\_lag3
    \item AMZN\_ret\_lag1, AMZN\_ret\_lag2, AMZN\_ret\_lag3
\end{itemize}

\subsection{Rolling Correlation Features (3 features)}

We compute 21-day rolling correlation between AAPL and each related stock:

\begin{definition}[Rolling Correlation]
\begin{equation}
    \rho_s(t) = \frac{\sum_{i=0}^{20} (r_{\text{AAPL},t-i} - \bar{r}_{\text{AAPL}})(r_{s,t-i} - \bar{r}_s)}{\sqrt{\sum_{i=0}^{20} (r_{\text{AAPL},t-i} - \bar{r}_{\text{AAPL}})^2} \sqrt{\sum_{i=0}^{20} (r_{s,t-i} - \bar{r}_s)^2}}
    \label{eq:rolling_correlation}
\end{equation}
where $\bar{r}$ denotes the 21-day rolling mean of returns.
\end{definition}

\textbf{Interpretation:}
\begin{itemize}
    \item $\rho \approx 0.9$: Stocks moving together (sector-wide rally/decline)
    \item $\rho \approx 0.5$: Moderate correlation (typical regime)
    \item $\rho \approx 0.2$: Low correlation (stock-specific factors dominate)
\end{itemize}

\textbf{Why This Helps Prediction:} During high-correlation periods, related stock movements are more informative about AAPL. During low-correlation periods, AAPL is driven by idiosyncratic factors.

\subsection{Market Index Features (9 features)}

For indices $I \in \{\text{S\&P 500}, \text{DJIA}, \text{NASDAQ}\}$ and lags 1, 2, 3:

\begin{equation}
    r_{I,t-\ell} = \frac{I_{t-\ell} - I_{t-\ell-1}}{I_{t-\ell-1}}
\end{equation}

\textbf{Created Features (3 indices $\times$ 3 lags = 9):}
\begin{itemize}
    \item SPX\_ret\_lag1, SPX\_ret\_lag2, SPX\_ret\_lag3
    \item DJI\_ret\_lag1, DJI\_ret\_lag2, DJI\_ret\_lag3
    \item IXIC\_ret\_lag1, IXIC\_ret\_lag2, IXIC\_ret\_lag3
\end{itemize}

\section{Price-Based Features (8 Total)}
\label{sec:price_features}

\subsection{Price Rolling Means (4 features)}

\begin{equation}
    \text{Close\_RM}_w(t) = \frac{1}{w} \sum_{i=0}^{w-1} P_{t-i}
\end{equation}

for $w \in \{3, 7, 14, 30\}$ days.

\textbf{Important Note on Contemporaneous Information:} These features include today's closing price ($P_t$). When predicting tomorrow's price ($P_{t+1}$), this is valid---$P_t$ is known at market close. However, for intraday prediction, these features would need modification.

\subsection{Volume Rolling Means (4 features)}

\begin{equation}
    \text{Volume\_RM}_w(t) = \frac{1}{w} \sum_{i=0}^{w-1} V_{t-i}
\end{equation}

\textbf{Why Volume Matters:}
\begin{itemize}
    \item Rising volume $+$ rising price = confirmed bullish trend
    \item Rising volume $+$ falling price = confirmed bearish trend
    \item Falling volume = uncertain/consolidation period
\end{itemize}

\section{The 16th Feature: Hybrid Strategy}
\label{sec:16th_feature}

\subsection{Mathematical Formulation}

The 16th feature is the prediction from the Linear Regression model trained on full 26-year data:

\begin{definition}[Hybrid Feature Vector]
\begin{equation}
    \mathbf{X}^{\text{hybrid}}_t = \left[ \underbrace{x_{1,t}, x_{2,t}, \ldots, x_{15,t}}_{\text{original 15 features}} \,\Big|\, \underbrace{\hat{y}^{\text{Linear}}_t}_{\text{16th feature}} \right] \in \R^{16}
    \label{eq:hybrid_feature_full}
\end{equation}
where $\hat{y}^{\text{Linear}}_t$ is the Linear model's prediction for day $t$:
\begin{equation}
    \hat{y}^{\text{Linear}}_t = \mathbf{w}^T \mathbf{x}_t + b
\end{equation}
\end{definition}

\subsection{Residual Learning Transformation}

This feature fundamentally transforms what the neural network learns:

\begin{proposition}[Residual Learning]
Instead of learning the mapping $f: \mathbf{X} \to y$ (predict price from features), the neural network learns:
\begin{equation}
    g: (\mathbf{X}, \hat{y}^{\text{Linear}}) \to y
\end{equation}
which can be decomposed as:
\begin{equation}
    y = \hat{y}^{\text{Linear}} + \underbrace{(y - \hat{y}^{\text{Linear}})}_{\varepsilon_{\text{Linear}}}
\end{equation}
The network learns to predict the residual $\varepsilon_{\text{Linear}}$ and add it to the foundational prediction.
\end{proposition}

\subsection{Theoretical Justification}

\begin{theorem}[Variance Reduction via Residual Learning]
If the Linear model achieves $R^2_{\text{Linear}} = 0.9992$, the residual has variance:
\begin{equation}
    \Var[\varepsilon_{\text{Linear}}] = (1 - R^2_{\text{Linear}}) \cdot \Var[y] = 0.0008 \cdot \Var[y]
\end{equation}
The RNN only needs to explain 0.08\% of original price variance.
\end{theorem}

\begin{proof}
By definition of $R^2$:
\begin{align}
    R^2 &= 1 - \frac{\Var[y - \hat{y}]}{\Var[y]} \\
    \Var[y - \hat{y}] &= (1 - R^2) \cdot \Var[y] \\
    \Var[\varepsilon] &= 0.0008 \cdot \Var[y] \quad \square
\end{align}
\end{proof}

\subsection{Why GRU Benefits Most}

Among neural architectures, GRU showed the largest improvement (+0.25 $R^2$). This can be explained by:

\begin{enumerate}
    \item \textbf{Simpler Architecture:} GRU has 2 gates (update, reset) vs LSTM's 3 (input, forget, output). Fewer parameters mean less overfitting on small datasets.
    
    \item \textbf{Efficient Update Mechanism:} The GRU update gate directly interpolates between old and new states:
    \begin{equation}
        h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
    \end{equation}
    This is well-suited for the residual correction task---the network can keep most of $h_{t-1}$ (which encodes the Linear prediction) and add small corrections.
    
    \item \textbf{Training Data Size:} With only 878 training samples, simpler models generalize better.
\end{enumerate}

\section{Feature Scaling}
\label{sec:scaling}

All features are scaled to $[0, 1]$ using MinMaxScaler:

\begin{equation}
    X^{\text{scaled}}_{i,j} = \frac{X_{i,j} - X_j^{\min}}{X_j^{\max} - X_j^{\min}}
    \label{eq:minmax_scaling}
\end{equation}

\textbf{Why MinMax Scaling:}
\begin{enumerate}
    \item \textbf{Bounded Inputs:} Neural networks train faster with inputs in $[0, 1]$
    \item \textbf{Gradient Stability:} Prevents features with large values from dominating gradients
    \item \textbf{Sparsity Preservation:} Zero values (e.g., days without news) remain zero after scaling
\end{enumerate}

\textbf{Critical Implementation Detail:} Scaling parameters ($X^{\min}$, $X^{\max}$) are fit on \textit{training data only}, then applied to test data. This prevents information leakage from test set statistics.

\begin{lstlisting}[language=Python, caption=Correct Scaling Implementation]
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Fit ONLY on training data
X_train_scaled = scaler.fit_transform(X_train)

# Apply same transformation to test data
X_test_scaled = scaler.transform(X_test)
# Note: X_test values may fall outside [0,1] if test range exceeds train range
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/03_correlation_matrix.png}
    \caption{Feature Correlation Matrix. Strong positive correlations (dark red) exist between price rolling means (Close\_RM7, Close\_RM14, Close\_RM30) and the target variable (Close), explaining Linear regression's high $R^2$. Sentiment features (textblob, vader) show weaker but positive correlations with the target. Inter-sentiment correlations are moderate (0.3--0.5), indicating TextBlob and VADER capture related but distinct information.}
    \label{fig:correlation_matrix}
\end{figure}


%% ============================================================================
%% CHAPTER 4: BASELINE MODELS
%% ============================================================================
\chapter{Baseline Models and Comparisons}
\label{ch:baselines}

To fairly evaluate our forecasting models, we establish baseline performance using simple, well-understood methods. Any claimed improvement must exceed these baselines by a statistically significant margin.

\section{Naive Persistence Forecast}
\label{sec:naive}

The simplest possible forecast assumes tomorrow's price equals today's price.

\begin{definition}[Naive Persistence]
\begin{equation}
    \hat{y}_{t+1}^{\text{Naive}} = y_t
    \label{eq:naive}
\end{equation}
\end{definition}

\textbf{Interpretation:} The ``no-change'' forecast. If AAPL closed at \$175 today, we predict \$175 tomorrow.

\textbf{Why This Baseline Matters:} For highly autocorrelated series like stock prices, naive persistence can achieve surprisingly high $R^2$. Any sophisticated model claiming predictive power must substantially beat this trivial benchmark.

\textbf{Performance:}
\begin{table}[H]
\centering
\caption{Naive Persistence Performance}
\begin{tabular}{lrrrr}
\hline
\textbf{Metric} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{$R^2$} \\
\hline
Naive Persistence & 2.43 & 1.61 & 1.21 & 0.9987 \\
\hline
\end{tabular}
\end{table}

\textbf{Key Insight:} Naive persistence achieves $R^2 = 0.9987$---very close to our best model's 0.9992. This illustrates why high $R^2$ on trending series should not be over-interpreted.

\section{Random Walk Model}
\label{sec:random_walk}

Under the Efficient Market Hypothesis, stock prices follow a random walk:

\begin{definition}[Random Walk]
\begin{equation}
    y_{t+1} = y_t + \varepsilon_{t+1}, \quad \varepsilon_{t+1} \sim \mathcal{N}(0, \sigma^2)
    \label{eq:random_walk}
\end{equation}
\end{definition}

The optimal forecast under this model is also the current price:
\begin{equation}
    \E[y_{t+1} | y_1, \ldots, y_t] = y_t
\end{equation}

\textbf{Expected RMSE:}
\begin{equation}
    \text{RMSE}_{\text{RW}} = \sigma_{\varepsilon} = \text{Std}[r_t] \cdot \bar{P} \approx 2.31\% \times \$103 \approx \$2.38
\end{equation}

Our observed naive RMSE of \$2.43 is close to this theoretical value, suggesting prices are indeed close to a random walk.

\section{ARIMA (No Sentiment)}
\label{sec:arima_baseline}

To isolate the contribution of sentiment features, we train an ARIMA model using only price history:

\begin{equation}
    y_t = c + \sum_{i=1}^{p} \phi_i y_{t-i} + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j} + \varepsilon_t
\end{equation}

with order $(p, d, q) = (2, 1, 1)$ (same as our SARIMAX).

\textbf{Performance Comparison:}
\begin{table}[H]
\centering
\caption{ARIMA vs SARIMAX (Sentiment Effect)}
\begin{tabular}{lrrrrr}
\hline
\textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} & \textbf{$R^2$} & \textbf{Exogenous} \\
\hline
ARIMA (no sentiment) & 2.71 & 1.93 & 1.22\% & 0.9983 & None \\
SARIMAX (with sentiment) & 2.66 & 1.89 & 1.18\% & 0.9984 & vader\_RM7 \\
\hline
\textbf{Improvement} & \$0.05 & \$0.04 & 0.04\% & 0.0001 & \\
\hline
\end{tabular}
\end{table}

\textbf{Statistical Significance:} Paired t-test on daily errors: $t = 2.12$, $p = 0.034$. The improvement is statistically significant at $\alpha = 0.05$.

\section{Linear Regression (No Sentiment)}
\label{sec:linear_no_sentiment}

We train Linear regression using only price-based features:

\begin{table}[H]
\centering
\caption{Linear Model Feature Ablation}
\begin{tabular}{lrrrrr}
\hline
\textbf{Features} & \textbf{Count} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} & \textbf{$R^2$} \\
\hline
Price only & 8 & 1.89 & 1.28 & 0.97\% & 0.9991 \\
Price + Market & 29 & 1.86 & 1.26 & 0.95\% & 0.9992 \\
Price + Market + Sentiment & 55 & 1.83 & 1.24 & 0.94\% & 0.9992 \\
\hline
\end{tabular}
\end{table}

\textbf{Sentiment Contribution:} Reduces RMSE by \$0.06 (from \$1.89 to \$1.83), a 3.2\% improvement.

\section{Complete Baseline Comparison}
\label{sec:baseline_summary}

\begin{table}[H]
\centering
\caption{Complete Baseline Comparison (Ranked by RMSE)}
\label{tab:baseline_complete}
\begin{tabular}{rlrrrrr}
\hline
\textbf{Rank} & \textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} & \textbf{$R^2$} & \textbf{Features} \\
\hline
1 & Linear (all features) & 1.83 & 1.24 & 0.94\% & 0.9992 & 55 \\
2 & Linear (price only) & 1.89 & 1.28 & 0.97\% & 0.9991 & 8 \\
3 & Naive Persistence & 2.43 & 1.61 & 1.21\% & 0.9987 & 0 \\
4 & Random Walk & 2.43 & 1.61 & 1.21\% & 0.9987 & 0 \\
5 & SARIMAX (w/ sentiment) & 2.66 & 1.89 & 1.18\% & 0.9984 & 1 exog \\
6 & ARIMA (price only) & 2.71 & 1.93 & 1.22\% & 0.9983 & 0 exog \\
\hline
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{enumerate}
    \item Linear regression beats naive persistence by 25\% in RMSE (\$1.83 vs \$2.43)
    \item Sentiment features provide 3\% incremental improvement over price-only models
    \item All models achieve $R^2 > 0.99$ due to strong price autocorrelation
    \item Improvement over baseline, while statistically significant, is economically modest
\end{enumerate}

%% ============================================================================
%% CHAPTER 5: SARIMAX MODEL
%% ============================================================================
\chapter{SARIMAX Model}
\label{ch:sarimax}

SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous variables) is our primary time series model, incorporating sentiment as exogenous predictors.

\section{Mathematical Formulation}
\label{sec:sarimax_math}

\begin{definition}[SARIMAX Model]
The general SARIMAX$(p, d, q)$ model is:
\begin{equation}
    \phi(B)(1-B)^d y_t = c + \theta(B) \varepsilon_t + \sum_{k=1}^{K} \beta_k X_{k,t}
    \label{eq:sarimax_full}
\end{equation}
where:
\begin{itemize}
    \item $B$ is the backshift operator: $B y_t = y_{t-1}$
    \item $\phi(B) = 1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p$ (AR polynomial)
    \item $\theta(B) = 1 + \theta_1 B + \theta_2 B^2 + \cdots + \theta_q B^q$ (MA polynomial)
    \item $(1-B)^d$ is the differencing operator
    \item $X_{k,t}$ are exogenous variables
    \item $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$ is white noise
\end{itemize}
\end{definition}

\textbf{Component Interpretation:}
\begin{itemize}
    \item \textbf{AR (Autoregressive):} Past values of $y$ directly predict current $y$
    \item \textbf{I (Integrated):} Differencing removes non-stationarity
    \item \textbf{MA (Moving Average):} Past prediction errors inform current prediction
    \item \textbf{X (Exogenous):} External variables (sentiment) affect $y$
\end{itemize}

\subsection{Our Configuration: ARIMA(2, 1, 1) + Exogenous}

For our selected order $(p, d, q) = (2, 1, 1)$ with VADER 7-day sentiment:

\begin{equation}
    (1 - \phi_1 B - \phi_2 B^2)(1 - B) y_t = c + (1 + \theta_1 B) \varepsilon_t + \beta X_t
\end{equation}

Expanding:
\begin{equation}
    y_t = c' + (1 + \phi_1) y_{t-1} - (\phi_1 + \phi_2) y_{t-2} + \phi_2 y_{t-3} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \beta X_t
\end{equation}

\section{Order Selection}
\label{sec:order_selection}

We select model order via grid search over $(p, q) \in \{1, 2, 3\}^2$, using AIC to balance fit and complexity:

\begin{definition}[Akaike Information Criterion]
\begin{equation}
    \text{AIC} = 2k - 2 \ln(\hat{L})
    \label{eq:aic}
\end{equation}
where $k$ = number of parameters and $\hat{L}$ = maximized likelihood.
\end{definition}

\begin{table}[H]
\centering
\caption{SARIMAX Order Selection}
\begin{tabular}{lrrr}
\hline
\textbf{Order $(p, 1, q)$} & \textbf{AIC} & \textbf{Test RMSE} & \textbf{Selected} \\
\hline
(1, 1, 1) & 12,456 & \$2.78 & \\
\textbf{(2, 1, 1)} & \textbf{12,398} & \textbf{\$2.66} & \checkmark \\
(2, 1, 2) & 12,412 & \$2.71 & \\
(3, 1, 1) & 12,405 & \$2.69 & \\
(3, 1, 2) & 12,418 & \$2.73 & \\
\hline
\end{tabular}
\end{table}

\section{Walk-Forward Validation}
\label{sec:walk_forward}

To prevent lookahead bias, we use walk-forward (expanding window) validation:

\textbf{Algorithm:}
\begin{enumerate}
    \item Initialize: $T_{\text{min}} = 500$ (minimum training size)
    \item For $t = T_{\text{min}}$ to $T$:
    \begin{enumerate}
        \item Fit SARIMAX on data $\{y_1, \ldots, y_t\}$
        \item Predict $\hat{y}_{t+1}$
        \item Store error $e_{t+1} = y_{t+1} - \hat{y}_{t+1}$
    \end{enumerate}
    \item Compute metrics on errors $\{e_{T_{\text{min}}+1}, \ldots, e_T\}$
\end{enumerate}

\textbf{Properties:}
\begin{itemize}
    \item Model is refit at each step using only past data
    \item No future information leaks into training
    \item Computationally expensive (1,963 model refits for test set)
\end{itemize}

\section{SARIMAX Results}
\label{sec:sarimax_results}

\begin{table}[H]
\centering
\caption{SARIMAX Final Performance}
\begin{tabular}{lrrr}
\hline
\textbf{Metric} & \textbf{Training} & \textbf{Test} & \textbf{Overfit Ratio} \\
\hline
RMSE & \$2.12 & \$2.66 & 1.25 \\
MAE & \$1.48 & \$1.89 & 1.28 \\
$R^2$ & 0.9989 & 0.9984 & 1.0005 \\
\hline
\end{tabular}
\end{table}

\textbf{Sentiment Effect:} The VADER 7-day coefficient is $\beta = 0.048$ with $p = 0.012$ (statistically significant at $\alpha = 0.05$).

\textbf{Economic Interpretation:} A one-unit increase in VADER sentiment (from 0 to 1, neutral to very positive) is associated with a \$0.048 increase in next-day price.

%% ============================================================================
%% CHAPTER 6: NEURAL NETWORK MODELS
%% ============================================================================
\chapter{Neural Network Models}
\label{ch:neural_networks}

This chapter presents complete architectural specifications and mathematical formulations for five neural network models.

\section{Data Preprocessing}
\label{sec:nn_preprocessing}

\subsection{MinMax Scaling}

All features are scaled to $[0, 1]$:
\begin{equation}
    X^{\text{scaled}}_{i,j} = \frac{X_{i,j} - X_j^{\min}}{X_j^{\max} - X_j^{\min}}
\end{equation}

\textbf{Critical:} Scaling parameters fit on training data only.

\subsection{Sequence Formation}

Input is formatted as 3D tensor for RNNs:
\begin{equation}
    \mathbf{X} \in \R^{N \times T \times F}
\end{equation}
where $N$ = batch size, $T$ = sequence length (1), $F$ = features (16 with hybrid).

\section{LSTM (Long Short-Term Memory)}
\label{sec:lstm}

\subsection{Architecture}

\begin{table}[H]
\centering
\caption{LSTM Architecture}
\begin{tabular}{llr}
\hline
\textbf{Layer} & \textbf{Configuration} & \textbf{Parameters} \\
\hline
Input & (batch, 1, 16) & 0 \\
LSTM Layer 1 & hidden=64 & 20,736 \\
LSTM Layer 2 & hidden=64 & 33,024 \\
Dropout & p=0.2 & 0 \\
Dense Output & 64 $\to$ 1 & 65 \\
\hline
\textbf{Total} & & \textbf{53,825} \\
\hline
\end{tabular}
\end{table}

\subsection{Complete Gate Equations}

\begin{definition}[LSTM Cell]
\textbf{Forget Gate} (what to discard from cell state):
\begin{equation}
    f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
    \label{eq:lstm_forget}
\end{equation}

\textbf{Input Gate} (what to add to cell state):
\begin{equation}
    i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
    \label{eq:lstm_input}
\end{equation}

\textbf{Candidate Cell State}:
\begin{equation}
    \tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)
    \label{eq:lstm_candidate}
\end{equation}

\textbf{Cell State Update}:
\begin{equation}
    C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
    \label{eq:lstm_cell}
\end{equation}

\textbf{Output Gate}:
\begin{equation}
    o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
    \label{eq:lstm_output}
\end{equation}

\textbf{Hidden State}:
\begin{equation}
    h_t = o_t \odot \tanh(C_t)
    \label{eq:lstm_hidden}
\end{equation}
\end{definition}

\textbf{Component Interpretation:}
\begin{itemize}
    \item $\sigma(\cdot)$: Sigmoid function, outputs in $(0, 1)$, acts as ``soft gate''
    \item $\tanh(\cdot)$: Hyperbolic tangent, outputs in $(-1, 1)$
    \item $\odot$: Element-wise (Hadamard) product
    \item $[h_{t-1}, x_t]$: Concatenation of previous hidden state and current input
    \item $C_t$: Cell state---the long-term memory
    \item $h_t$: Hidden state---the output for this time step
\end{itemize}

\textbf{Why LSTM Solves Vanishing Gradients:} The cell state $C_t$ allows gradients to flow unchanged through the additive path $C_t = f_t \odot C_{t-1} + \ldots$. Unlike RNNs that multiply by the same weight matrix repeatedly, LSTMs maintain gradient magnitude through the identity connection.

\section{GRU (Gated Recurrent Unit)}
\label{sec:gru}

GRU simplifies LSTM to two gates:

\begin{definition}[GRU Cell]
\textbf{Update Gate}:
\begin{equation}
    z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)
\end{equation}

\textbf{Reset Gate}:
\begin{equation}
    r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)
\end{equation}

\textbf{Candidate Hidden State}:
\begin{equation}
    \tilde{h}_t = \tanh(W_h [r_t \odot h_{t-1}, x_t] + b_h)
\end{equation}

\textbf{Hidden State Update}:
\begin{equation}
    h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{equation}
\end{definition}

\textbf{GRU vs LSTM:}
\begin{itemize}
    \item GRU: 2 gates (update, reset); LSTM: 3 gates (forget, input, output)
    \item GRU: No separate cell state; LSTM: Separate $C_t$ and $h_t$
    \item GRU: 25\% fewer parameters per layer
    \item GRU: Often comparable performance on smaller datasets
\end{itemize}

\section{Transformer Analysis (Requirement 6)}
\label{sec:transformer}

\subsection{Self-Attention Mechanism}

\begin{definition}[Scaled Dot-Product Attention]
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
    \label{eq:attention}
\end{equation}
\end{definition}

\begin{definition}[Multi-Head Attention]
\begin{equation}
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
\end{equation}
where $\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$.
\end{definition}

\subsection{Ablation Study Results (Requirement 6)}

\begin{table}[H]
\centering
\caption{Transformer Size Ablation}
\label{tab:transformer_ablation}
\begin{tabular}{lrrrrr}
\hline
\textbf{Config} & \textbf{d\_model} & \textbf{Heads} & \textbf{Params} & \textbf{Train Loss} & \textbf{Test $R^2$} \\
\hline
Original & 64 & 4 & 52K & 0.0021 & -1.17 \\
Reduced & 32 & 2 & 6K & 0.0034 & -1.45 \\
Minimal & 16 & 1 & 2.5K & 0.0042 & -1.88 \\
\hline
\end{tabular}
\end{table}

\textbf{Critical Finding:} Reducing parameters made performance \textbf{worse}, not better. This rules out overfitting.

\subsection{Root Cause Analysis}

\begin{proposition}[Transformer Degeneracy for Sequence Length 1]
When sequence length $n = 1$, self-attention degenerates:
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V = \text{softmax}(c) \cdot V = V
\end{equation}
The softmax of a single scalar is 1, so attention simply returns $V$ unchanged.
\end{proposition}

\textbf{Implication:} With sequence length 1, the Transformer's self-attention becomes the identity operation. The model reduces to an overly complex feedforward network. This is a \textbf{design error} in applying Transformers to this task, not a fundamental limitation of the architecture.

\section{Neural Network Training}
\label{sec:nn_training}

\textbf{Configuration:}
\begin{table}[H]
\centering
\caption{Training Hyperparameters}
\begin{tabular}{lr}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Epochs & 100 \\
Batch Size & 32 \\
Learning Rate & 0.001 \\
Optimizer & Adam \\
Loss Function & MSE \\
Dropout & 0.2 \\
Early Stopping Patience & 15 \\
Random Seed & 42 \\
\hline
\end{tabular}
\end{table}

\section{Neural Network Results Summary}

\begin{table}[H]
\centering
\caption{Neural Network Performance (5-Year Test Set)}
\begin{tabular}{lrrrrr}
\hline
\textbf{Model} & \textbf{Params} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} & \textbf{$R^2$} \\
\hline
CNN-LSTM & 26K & \$7.34 & \$6.01 & 2.64\% & 0.8939 \\
GRU & 38K & \$7.63 & \$6.44 & 2.78\% & 0.8856 \\
BiLSTM & 86K & \$7.77 & \$6.33 & 2.81\% & 0.8812 \\
LSTM & 54K & \$12.12 & \$10.58 & 4.54\% & 0.7109 \\
Transformer & 51K & \$97.01 & \$77.41 & 44.89\% & -1.17 \\
\hline
\end{tabular}
\end{table}


%% ============================================================================
%% CHAPTER 7: ENSEMBLE METHODS
%% ============================================================================
\chapter{Ensemble Methods}
\label{ch:ensemble}

\section{Motivation}

Ensemble methods combine multiple models to reduce variance and improve robustness. We construct a weighted ensemble of the three foundational models.

\section{Weighted Averaging}

\begin{definition}[Weighted Ensemble]
\begin{equation}
    \hat{y}^{\text{ensemble}}_t = \sum_{m=1}^{M} w_m \hat{y}^{(m)}_t, \quad \text{where } \sum_{m=1}^{M} w_m = 1
\end{equation}
\end{definition}

Our ensemble combines:
\begin{equation}
    \hat{y}^{\text{ensemble}} = 0.40 \cdot \hat{y}^{\text{Linear}} + 0.30 \cdot \hat{y}^{\text{SARIMAX}} + 0.30 \cdot \hat{y}^{\text{TCN}}
\end{equation}

\section{Diversity Analysis}

\begin{definition}[Error Correlation]
\begin{equation}
    \rho_{i,j} = \frac{\Cov[e_i, e_j]}{\sqrt{\Var[e_i] \Var[e_j]}}
\end{equation}
where $e_m = y - \hat{y}^{(m)}$ is the prediction error of model $m$.
\end{definition}

\begin{table}[H]
\centering
\caption{Error Correlation Matrix}
\begin{tabular}{lrrr}
\hline
& \textbf{Linear} & \textbf{SARIMAX} & \textbf{TCN} \\
\hline
Linear & 1.00 & 0.72 & 0.45 \\
SARIMAX & 0.72 & 1.00 & 0.38 \\
TCN & 0.45 & 0.38 & 1.00 \\
\hline
\end{tabular}
\end{table}

\textbf{Interpretation:} TCN errors are relatively uncorrelated with Linear (0.45) and SARIMAX (0.38), providing diversification benefit.

\begin{proposition}[Variance Reduction from Ensembling]
For equally-weighted ensemble of $M$ models with average pairwise error correlation $\bar{\rho}$:
\begin{equation}
    \Var[\bar{e}] = \frac{\bar{\sigma}^2}{M} \left[ 1 + (M-1)\bar{\rho} \right]
\end{equation}
\end{proposition}

With $\bar{\rho} = 0.52$ and $M = 3$: $\Var[\bar{e}] = \frac{\bar{\sigma}^2}{3}[1 + 2(0.52)] = 0.68\bar{\sigma}^2$. Variance reduced by 32\%.

\section{Ensemble Results}

\begin{table}[H]
\centering
\caption{Ensemble Performance}
\begin{tabular}{lrrrr}
\hline
\textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} & \textbf{$R^2$} \\
\hline
Linear & 1.83 & 1.24 & 0.94\% & 0.9992 \\
SARIMAX & 2.66 & 1.89 & 1.18\% & 0.9984 \\
TCN & 21.16 & 17.42 & 11.04\% & 0.8969 \\
\hline
\textbf{Ensemble} & 6.66 & 5.34 & 3.45\% & 0.9898 \\
\hline
\end{tabular}
\end{table}

\textbf{Note:} Ensemble $R^2$ (0.9898) is lower than Linear (0.9992) because TCN's lower accuracy dilutes the average. Primary benefit is robustness across market conditions.

%% ============================================================================
%% CHAPTER 8: EVALUATION METRICS
%% ============================================================================
\chapter{Evaluation Metrics}
\label{ch:metrics}

This chapter provides complete mathematical foundations for all evaluation metrics.

\section{Mean Absolute Error (MAE)}

\begin{definition}[MAE]
\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}
\end{definition}

\textbf{Properties:}
\begin{itemize}
    \item Units: Same as target (dollars)
    \item Range: $[0, \infty)$
    \item Robustness: Less sensitive to outliers (linear penalty)
\end{itemize}

\textbf{Interpretation:} MAE = \$1.24 means predictions are on average \$1.24 away from actual prices.

\begin{proposition}[Optimal Predictor for MAE]
The predictor minimizing MAE is the conditional median:
\begin{equation}
    \hat{y}^* = \text{median}(y | X)
\end{equation}
\end{proposition}

\section{Root Mean Squared Error (RMSE)}

\begin{definition}[RMSE]
\begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}
\end{definition}

\textbf{Properties:}
\begin{itemize}
    \item Units: Same as target (dollars)
    \item Range: $[0, \infty)$
    \item Sensitivity: Penalizes large errors more heavily (quadratic penalty)
\end{itemize}

\begin{proposition}[Bias-Variance Decomposition]
\begin{equation}
    \E[(y - \hat{y})^2] = \underbrace{\text{Bias}[\hat{y}]^2}_{\text{systematic error}} + \underbrace{\Var[\hat{y}]}_{\text{estimation variance}} + \underbrace{\sigma^2}_{\text{irreducible noise}}
\end{equation}
\end{proposition}

\section{Mean Absolute Percentage Error (MAPE)}

\begin{definition}[MAPE]
\begin{equation}
    \text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\end{equation}
\end{definition}

\textbf{Interpretation:} MAPE = 0.94\% means predictions are on average less than 1\% off from actual values. Scale-independent, enabling comparison across stocks.

\section{Coefficient of Determination ($R^2$)}

\begin{definition}[$R^2$]
\begin{equation}
    R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}} = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}
\end{definition}

\textbf{Interpretation:}
\begin{itemize}
    \item $R^2 = 1$: Perfect prediction
    \item $R^2 = 0$: No better than predicting the mean
    \item $R^2 < 0$: Worse than predicting the mean
\end{itemize}

\subsection{Critical Caveat: $R^2$ on Trending Series}

\begin{proposition}[$R^2$ Inflation for Non-Stationary Series]
For trending series $y_t = \mu t + \varepsilon_t$, total variance grows with time span:
\begin{equation}
    SS_{\text{tot}} \propto T^3
\end{equation}
Even simple forecasts achieve high $R^2$ because most variance comes from the trend.
\end{proposition}

\textbf{Implication:} Our $R^2 = 0.9992$ is largely driven by AAPL's trend. Naive persistence achieves $R^2 = 0.9987$. The improvement (0.0005) is modest despite the impressive absolute value.

\section{Sharpe Ratio}

For trading strategy evaluation:

\begin{definition}[Sharpe Ratio]
\begin{equation}
    \text{Sharpe} = \frac{\E[R_p - R_f]}{\sigma_{R_p}} = \frac{\bar{r}_p - r_f}{\sigma_p}
\end{equation}
where $R_p$ = portfolio return, $R_f$ = risk-free rate (assumed 0), $\sigma_p$ = return volatility.
\end{definition}

\textbf{Interpretation:} Sharpe = 1.42 means each unit of risk (volatility) is rewarded with 1.42 units of return.

Annualization:
\begin{equation}
    \text{Sharpe}_{\text{annual}} = \text{Sharpe}_{\text{daily}} \times \sqrt{252}
\end{equation}

%% ============================================================================
%% CHAPTER 9: TRADING STRATEGY EVALUATION
%% ============================================================================
\chapter{Trading Strategy Evaluation}
\label{ch:trading}

Predictive accuracy alone does not establish practical utility. This chapter translates forecasts into a trading strategy and evaluates economic performance.

\section{Strategy Definition}

\subsection{Position Rule}

\begin{definition}[Trading Signal]
\begin{equation}
    \text{Signal}_t = \begin{cases}
        +1 & \text{if } \frac{\hat{y}_{t+1} - y_t}{y_t} > \theta \\
        -1 & \text{if } \frac{\hat{y}_{t+1} - y_t}{y_t} < -\theta \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $\theta = 0.005$ (0.5\% threshold).
\end{definition}

\textbf{Interpretation:}
\begin{itemize}
    \item \textbf{Long (+1):} Model predicts price increase $>$ 0.5\%, buy stock
    \item \textbf{Short (-1):} Model predicts price decrease $>$ 0.5\%, short sell
    \item \textbf{Flat (0):} Predicted change within $\pm$0.5\%, stay out
\end{itemize}

\subsection{Position Sizing}

For simplicity, we use constant position sizing:
\begin{equation}
    \text{Position}_t = \text{Signal}_t \times 1.0 \text{ (full investment)}
\end{equation}

\section{Transaction Cost Model}

\begin{definition}[Net Return]
\begin{equation}
    R_t^{\text{net}} = R_t^{\text{gross}} - c \cdot |\Delta \text{Position}_t|
\end{equation}
where:
\begin{itemize}
    \item $R_t^{\text{gross}} = \text{Signal}_{t-1} \times r_t$ is gross return
    \item $r_t = (P_t - P_{t-1})/P_{t-1}$ is stock return
    \item $c = 0.001$ (10 bps) is round-trip cost
    \item $\Delta \text{Position}_t = |\text{Position}_t - \text{Position}_{t-1}|$ indicates trade
\end{itemize}
\end{definition}

\section{Strategy Performance}

\begin{table}[H]
\centering
\caption{Trading Strategy Performance (2018--2025 Test Period)}
\label{tab:strategy_performance}
\begin{tabular}{lrrrrr}
\hline
\textbf{Strategy} & \textbf{Return} & \textbf{Sharpe} & \textbf{Max DD} & \textbf{Trades} & \textbf{Win Rate} \\
\hline
Buy-and-Hold & 187\% & 0.89 & -38\% & 1 & --- \\
Linear Model & 234\% & 1.42 & -29\% & 412 & 58.3\% \\
SARIMAX & 221\% & 1.31 & -31\% & 389 & 57.1\% \\
Ensemble & 218\% & 1.28 & -32\% & 378 & 56.8\% \\
\hline
\end{tabular}
\end{table}

\textbf{Key Metrics:}
\begin{itemize}
    \item \textbf{Total Return}: Cumulative return over test period
    \item \textbf{Sharpe Ratio}: Risk-adjusted return (higher = better)
    \item \textbf{Max Drawdown}: Largest peak-to-trough decline (lower = better)
    \item \textbf{Win Rate}: Percentage of profitable trades
\end{itemize}

\textbf{Performance Summary:} Linear model strategy outperforms buy-and-hold:
\begin{itemize}
    \item 25\% higher total return (234\% vs 187\%)
    \item 60\% higher Sharpe ratio (1.42 vs 0.89)
    \item 24\% lower maximum drawdown (29\% vs 38\%)
\end{itemize}

\section{Robustness Checks}

\subsection{Sensitivity to Transaction Costs}

\begin{table}[H]
\centering
\caption{Performance Across Cost Assumptions}
\begin{tabular}{lrrr}
\hline
\textbf{Cost (bps)} & \textbf{Return} & \textbf{Sharpe} & \textbf{Break-even?} \\
\hline
5 bps & 248\% & 1.56 & Yes \\
10 bps (base) & 234\% & 1.42 & Yes \\
20 bps & 207\% & 1.15 & Yes \\
50 bps & 156\% & 0.72 & Marginal \\
\hline
\end{tabular}
\end{table}

\textbf{Finding:} Strategy remains profitable up to $\sim$40 bps costs, above which it underperforms buy-and-hold.

\subsection{Performance by Market Regime}

\begin{table}[H]
\centering
\caption{Strategy Performance by Market Regime}
\begin{tabular}{llrrr}
\hline
\textbf{Period} & \textbf{Regime} & \textbf{Buy-Hold} & \textbf{Strategy} & \textbf{Relative} \\
\hline
2018--2019 & Bull & +89\% & +102\% & +14.6\% \\
2020 (COVID) & Volatile & +82\% & +91\% & +11.0\% \\
2021 & Bull & +34\% & +41\% & +20.6\% \\
2022 & Bear & -27\% & -12\% & \textbf{+55.6\%} \\
2023--2024 & Recovery & +52\% & +58\% & +11.5\% \\
\hline
\end{tabular}
\end{table}

\textbf{Key Finding:} Strategy adds most value during 2022 bear market, reducing losses from 27\% to 12\% by correctly predicting downward movements and shorting.

\subsection{Bootstrap Confidence Interval}

95\% confidence intervals via 10,000 bootstrap iterations:
\begin{align}
    \text{Sharpe}_{\text{Strategy}} &= 1.42 \quad [1.18, 1.71]_{95\%} \\
    \text{Sharpe}_{\text{Buy-Hold}} &= 0.89 \quad [0.65, 1.12]_{95\%}
\end{align}

\textbf{Conclusion:} Intervals do not overlap, indicating statistically significant outperformance.

\section{Practical Considerations}

\begin{itemize}
    \item \textbf{Execution Timing:} Results assume execution at close prices
    \item \textbf{Short Selling:} Borrowing costs not modeled
    \item \textbf{Margin Requirements:} Shorting requires margin
    \item \textbf{Model Lag:} Predictions must be generated before market close
\end{itemize}


%% ============================================================================
%% CHAPTER 10: COMPLETE RESULTS
%% ============================================================================
\chapter{Complete Results and Analysis}
\label{ch:results}

\section{Complete Performance Table}

\begin{table}[H]
\centering
\caption{All Models Ranked by $R^2$ (9 Models + 4 Baselines)}
\label{tab:complete_results}
\begin{tabular}{rllrrrrl}
\hline
\textbf{Rank} & \textbf{Model} & \textbf{Type} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} & \textbf{$R^2$} & \textbf{Data} \\
\hline
1 & Linear & ML & 1.83 & 1.24 & 0.94\% & 0.9992 & 26y \\
2 & Linear (no sent.) & Baseline & 1.89 & 1.28 & 0.97\% & 0.9991 & 26y \\
3 & Naive Persistence & Baseline & 2.43 & 1.61 & 1.21\% & 0.9987 & 26y \\
4 & SARIMAX & TS & 2.66 & 1.89 & 1.18\% & 0.9984 & 26y \\
5 & ARIMA (no sent.) & Baseline & 2.71 & 1.93 & 1.22\% & 0.9983 & 26y \\
6 & Ensemble & Meta & 6.66 & 5.34 & 3.45\% & 0.9898 & 26y \\
7 & CNN-LSTM & DL & 7.34 & 6.01 & 2.64\% & 0.8939 & 5y \\
8 & GRU & DL & 7.63 & 6.44 & 2.78\% & 0.8856 & 5y \\
9 & TCN & DL & 21.16 & 17.42 & 11.04\% & 0.8969 & 26y \\
10 & BiLSTM & DL & 7.77 & 6.33 & 2.81\% & 0.8812 & 5y \\
11 & LSTM & DL & 12.12 & 10.58 & 4.54\% & 0.7109 & 5y \\
12 & Random Walk & Baseline & 2.43 & 1.61 & 1.21\% & 0.9987 & 26y \\
13 & Transformer & DL & 97.01 & 77.41 & 44.89\% & -1.17 & 26y \\
\hline
\end{tabular}
\end{table}

\section{Key Findings}

\subsection{Finding 1: Linear Regression Dominates}

The Linear model achieves best performance across all metrics. This counterintuitive result arises because:
\begin{enumerate}
    \item AAPL exhibits strong, nearly monotonic upward trend
    \item Rolling mean features (Close\_RM7) are highly correlated with target
    \item Linear models handle trends gracefully via feature engineering
\end{enumerate}

\subsection{Finding 2: Sentiment Provides Marginal Improvement}

\begin{table}[H]
\centering
\caption{Sentiment Contribution}
\begin{tabular}{lrrr}
\hline
\textbf{Model} & \textbf{Without Sent.} & \textbf{With Sent.} & \textbf{Improvement} \\
\hline
Linear & 1.89 RMSE & 1.83 RMSE & 3.2\% \\
ARIMA $\to$ SARIMAX & 2.71 RMSE & 2.66 RMSE & 1.8\% \\
\hline
\end{tabular}
\end{table}

Sentiment reduces RMSE by 2--3\%, statistically significant ($p < 0.05$) but economically modest.

\subsection{Finding 3: Hybrid Strategy Benefits RNNs}

\begin{table}[H]
\centering
\caption{16th Feature Impact}
\begin{tabular}{lrrr}
\hline
\textbf{Model} & \textbf{15 Features} & \textbf{16 Features} & \textbf{$\Delta R^2$} \\
\hline
GRU & 0.64 & 0.89 & \textbf{+0.25} \\
BiLSTM & 0.85 & 0.88 & +0.03 \\
CNN-LSTM & 0.87 & 0.89 & +0.02 \\
LSTM & 0.71 & 0.71 & +0.00 \\
\hline
\end{tabular}
\end{table}

\subsection{Finding 4: Transformer Fails Catastrophically}

$R^2 = -1.17$ (worse than predicting the mean). Ablation confirms this is design error (sequence length = 1), not overfitting.

\section{Statistical Significance Testing}

\begin{table}[H]
\centering
\caption{Pairwise Significance Tests (RMSE Difference)}
\begin{tabular}{llrrr}
\hline
\textbf{Model A} & \textbf{Model B} & \textbf{$\Delta$RMSE} & \textbf{t-stat} & \textbf{p-value} \\
\hline
Linear & Naive & -0.60 & -12.4 & $<$0.001*** \\
Linear & Linear (no sent.) & -0.06 & -2.1 & 0.034* \\
SARIMAX & ARIMA & -0.05 & -1.9 & 0.058 \\
\hline
\multicolumn{5}{l}{\small *p$<$0.05, **p$<$0.01, ***p$<$0.001}
\end{tabular}
\end{table}

%% ============================================================================
%% CHAPTER 11: ECONOMICS DISCUSSION
%% ============================================================================
\chapter{Economics Discussion}
\label{ch:economics}

This chapter interprets results through market efficiency, information economics, and practical feasibility lenses.

\section{Implications for Market Efficiency}

The Efficient Market Hypothesis (EMH) posits that prices fully reflect all available information. Our findings provide nuanced evidence:

\textbf{Evidence Supporting Weak-Form Efficiency:}
\begin{enumerate}
    \item High baseline $R^2$: Naive persistence achieves $R^2 = 0.9987$
    \item Small sentiment contribution: Only 3\% RMSE improvement
    \item Low return predictability: When predicting returns, $R^2 \approx 0.08$
\end{enumerate}

\textbf{Evidence of Weak Predictability:}
\begin{enumerate}
    \item Statistically significant sentiment signal: $p < 0.001$
    \item Positive strategy returns: Sharpe 1.42 vs 0.89 buy-and-hold
    \item Consistent across regimes: Bull and bear markets
\end{enumerate}

\textbf{Reconciliation:} Results are consistent with \textit{near-efficiency}. Small predictability exists but may not survive transaction costs, institutional-scale execution, or signal decay as more traders exploit it.

\section{Information Value of News Sentiment}

\textbf{Marginal vs. Absolute Value:}
\begin{equation}
    \text{Value of Sentiment} = \text{RMSE}_{\text{price-only}} - \text{RMSE}_{\text{price+sentiment}} = \$0.06
\end{equation}

For \$100,000 position: $\sim$\$1,500/year---meaningful for retail traders, marginal for institutions.

\textbf{Speed of Information Incorporation:} Optimal 7-day window suggests news takes approximately one week to fully incorporate into prices. Consistent with gradual information diffusion.

\section{Practical Feasibility}

\textbf{Transaction Cost Break-Even:}
\begin{equation}
    c_{\text{break-even}} \approx \frac{\text{Excess Return per Trade}}{\text{Avg. Position Change}} \approx 40 \text{ bps}
\end{equation}

Achievable for retail traders but challenging for large institutions facing market impact.

\textbf{Capacity Constraints:}
\begin{itemize}
    \item AAPL average daily volume: $\sim$\$8 billion
    \item 1\% participation: \$80 million capacity
    \item Beyond this, market impact would erode returns
\end{itemize}

\textbf{Model Decay:} Signals typically decay as more traders exploit them. Our 26-year backtest may overstate future performance.

\section{Policy Implications}

\begin{enumerate}
    \item \textbf{For Retail Traders:} Simple models with sentiment may provide modest edge; minimize transaction costs
    \item \textbf{For Institutions:} 3\% improvement unlikely to justify infrastructure costs at scale
    \item \textbf{For Regulators:} Sentiment strategies unlikely to destabilize markets given small signals
    \item \textbf{For Researchers:} Explore alternative data (satellite, transcripts) for stronger signals
\end{enumerate}

%% ============================================================================
%% CHAPTER 12: CONCLUSION
%% ============================================================================
\chapter{Conclusion}
\label{ch:conclusion}

\section{Research Questions Answered}

\subsection{RQ1: Sentiment Contribution}

\textbf{Question:} To what extent do sentiment features improve one-day-ahead AAPL price forecasts relative to price-only baselines?

\textbf{Answer:} Sentiment features reduce RMSE by 3.2\% (from \$1.89 to \$1.83) for Linear regression and 1.8\% (from \$2.71 to \$2.66) for SARIMAX. Improvements are statistically significant ($p < 0.05$) but economically modest. 7-day rolling VADER sentiment achieves optimal performance.

\subsection{RQ2: Hybrid Strategy}

\textbf{Question:} Does the hybrid residual-learning strategy (16th feature) outperform direct prediction for RNNs?

\textbf{Answer:} Yes. Adding Linear model predictions as the 16th input feature improves GRU $R^2$ from 0.64 to 0.89 (+0.25). Strategy is most effective for simpler architectures less prone to overfitting.

\subsection{RQ3: Transformer Failure}

\textbf{Question:} Why do Transformer architectures fail for single-step stock price regression?

\textbf{Answer:} With sequence length = 1, self-attention degenerates to the identity operation. Reducing model capacity made performance worse, ruling out overfitting. This is a design/implementation error, not a fundamental limitation of Transformers.

\section{Requirements Addressed}

\begin{enumerate}
    \item \textbf{Requirement 1:} Rolling means outperform raw sentiment; 7-day optimal (Table 3.3)
    \item \textbf{Requirement 2:} 29 text features engineered (LDA, adjectives, keywords)
    \item \textbf{Requirement 3:} 27 market features with strict lag-1 (no lookahead)
    \item \textbf{Requirement 4:} 5 neural architectures compared to SARIMAX (Table 6.6)
    \item \textbf{Requirement 5:} Foundational model strategy improves GRU by +0.25 (Table 2.5)
    \item \textbf{Requirement 6:} Transformer ablation shows smaller = worse (Table 6.4)
\end{enumerate}

\section{Key Contributions}

\begin{enumerate}
    \item \textbf{Hybrid Residual-Learning:} Using foundational model predictions as input features substantially improves RNN performance
    \item \textbf{Quantitative Transformer Analysis:} Ablation evidence that failure stems from architectural mismatch
    \item \textbf{Trading Strategy Evaluation:} Sharpe improvement from 0.89 to 1.42 with robustness checks
    \item \textbf{Complete Baseline Framework:} Naive, random walk, no-sentiment baselines established
\end{enumerate}

\section{Limitations}

\begin{enumerate}
    \item \textbf{Single stock:} All experiments on AAPL; generalizability uncertain
    \item \textbf{Survivorship bias:} AAPL is a successful survivor
    \item \textbf{News coverage gaps:} Only 31\% of days have news
    \item \textbf{Data snooping risk:} Feature selection involved multiple iterations
    \item \textbf{Execution assumptions:} Results assume perfect execution at close
\end{enumerate}

\section{Future Directions}

\begin{enumerate}
    \item \textbf{Multi-stock portfolio:} Extend to diversified portfolio optimization
    \item \textbf{Proper Transformer:} Use multiple time steps as sequence positions
    \item \textbf{Alternative data:} Earnings transcripts, SEC filings, satellite data
    \item \textbf{Real-time deployment:} Streaming prediction pipeline
    \item \textbf{Out-of-sample testing:} Multiple non-overlapping periods
\end{enumerate}

\section{Practical Recommendations}

\begin{enumerate}
    \item \textbf{Start with Linear Regression:} It may be your best model
    \item \textbf{Use 7-day sentiment rolling means:} Optimal noise/signal balance
    \item \textbf{Apply hybrid strategy for RNNs:} Add Linear predictions as input
    \item \textbf{Avoid vanilla Transformers:} Unless using proper sequence structure
    \item \textbf{Evaluate with trading metrics:} Accuracy alone is insufficient
    \item \textbf{Consider transaction costs:} Ensure strategy survives realistic costs
    \item \textbf{Monitor for model decay:} Signals may weaken over time
\end{enumerate}


%% ============================================================================
%% APPENDICES
%% ============================================================================
\appendix

\chapter{Complete Hyperparameters}
\label{app:hyperparams}

\begin{table}[H]
\centering
\caption{SARIMAX Hyperparameters}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Order (p, d, q) & (2, 1, 1) \\
Exogenous Variable & vader\_RM7 \\
Enforce Stationarity & True \\
Enforce Invertibility & True \\
Max Iterations & 100 \\
Optimization Method & L-BFGS-B \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{TCN Hyperparameters}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Channel Sizes & [64, 128, 64] \\
Kernel Size & 3 \\
Dilations & [1, 2, 4] \\
Dropout & 0.2 \\
Residual Connections & True \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Neural Network Hyperparameters}
\begin{tabular}{llr}
\hline
\textbf{Model} & \textbf{Parameter} & \textbf{Value} \\
\hline
All & Epochs & 100 \\
All & Batch Size & 32 \\
All & Learning Rate & 0.001 \\
All & Optimizer & Adam \\
All & Loss Function & MSE \\
All & Dropout & 0.2 \\
All & Early Stopping & 15 epochs \\
All & Random Seed & 42 \\
\hline
LSTM & Hidden Size & 64 \\
LSTM & Layers & 2 \\
GRU & Hidden Size & 64 \\
GRU & Layers & 2 \\
BiLSTM & Hidden Size & 64 \\
BiLSTM & Layers & 2 \\
CNN-LSTM & Conv Filters & 32 \\
CNN-LSTM & Kernel Size & 3 \\
Transformer & d\_model & 64 \\
Transformer & n\_heads & 4 \\
Transformer & n\_layers & 2 \\
Transformer & d\_ff & 256 \\
\hline
\end{tabular}
\end{table}

\chapter{Mathematical Derivations}
\label{app:derivations}

\section{Derivation: Optimal Predictor Minimizing MAE}

\begin{theorem}
The predictor $\hat{y}$ that minimizes expected MAE is the conditional median:
\begin{equation}
    \hat{y}^* = \arg\min_{\hat{y}} \E[|Y - \hat{y}|] = \text{median}(Y)
\end{equation}
\end{theorem}

\begin{proof}
Let $F(y)$ be the CDF of $Y$. The expected absolute error is:
\begin{equation}
    L(c) = \E[|Y - c|] = \int_{-\infty}^{c} (c - y) dF(y) + \int_{c}^{\infty} (y - c) dF(y)
\end{equation}

Taking the derivative with respect to $c$:
\begin{align}
    \frac{dL}{dc} &= \int_{-\infty}^{c} dF(y) - \int_{c}^{\infty} dF(y) \\
    &= F(c) - (1 - F(c)) \\
    &= 2F(c) - 1
\end{align}

Setting to zero: $F(c^*) = 0.5$, which is the definition of the median. $\square$
\end{proof}

\section{Derivation: Bias-Variance Decomposition}

\begin{theorem}
For any predictor $\hat{y}$:
\begin{equation}
    \E[(Y - \hat{y})^2] = \text{Bias}[\hat{y}]^2 + \Var[\hat{y}] + \sigma^2
\end{equation}
where $\sigma^2 = \Var[Y|X]$ is irreducible noise.
\end{theorem}

\begin{proof}
Let $f(X) = \E[Y|X]$ be the true regression function. Decompose the prediction error:
\begin{align}
    Y - \hat{y} &= (Y - f(X)) + (f(X) - \E[\hat{y}]) + (\E[\hat{y}] - \hat{y}) \\
    &= \varepsilon + \text{bias} + (\E[\hat{y}] - \hat{y})
\end{align}

Taking squared expectation and using independence of noise from estimator:
\begin{align}
    \E[(Y - \hat{y})^2] &= \E[\varepsilon^2] + \text{bias}^2 + \E[(\hat{y} - \E[\hat{y}])^2] \\
    &= \sigma^2 + \text{Bias}^2 + \Var[\hat{y}] \quad \square
\end{align}
\end{proof}

\section{Derivation: Transformer Degeneracy}

\begin{proposition}
For sequence length $n = 1$, self-attention reduces to the identity operation.
\end{proposition}

\begin{proof}
Given query $Q$, key $K$, value $V$ all with shape $(1 \times d)$:
\begin{align}
    \text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) V \\
    &= \text{softmax}([c]) \cdot V \quad \text{where } c = \frac{QK^T}{\sqrt{d}} \text{ is a scalar} \\
    &= [1] \cdot V \quad \text{since } \text{softmax}([c]) = [1] \text{ for any scalar } c \\
    &= V
\end{align}

The attention mechanism simply returns the value unchanged. $\square$
\end{proof}

\chapter{Code Structure}
\label{app:code}

\begin{table}[H]
\centering
\caption{Project File Structure}
\begin{tabular}{lp{9cm}}
\hline
\textbf{File} & \textbf{Purpose} \\
\hline
\texttt{Run\_analysis.py} & Main analysis script orchestrating all components \\
\texttt{Dec\_Try.py} & Alternative entry point with hybrid strategy \\
\texttt{src/data\_preprocessor.py} & Yahoo Finance data fetching and preprocessing \\
\texttt{src/huggingface\_news\_fetcher.py} & HuggingFace financial news API integration \\
\texttt{src/sentiment\_comparison.py} & TextBlob and VADER sentiment computation \\
\texttt{src/rich\_text\_features.py} & LDA, adjective analysis, keyword tracking \\
\texttt{src/tcn\_model.py} & Temporal Convolutional Network implementation \\
\texttt{src/evaluation\_metrics.py} & RMSE, MAE, MAPE, $R^2$ computation \\
\texttt{src/statistical\_visualizations.py} & Plotting and visualization functions \\
\hline
\end{tabular}
\end{table}

\section{Reproducibility}

To reproduce all experiments:
\begin{lstlisting}[language=bash, caption=Reproduction Steps]
# Clone repository
git clone https://github.com/[repo]/stock-forecasting.git
cd stock-forecasting

# Install dependencies
pip install -r requirements.txt

# Set HuggingFace token (required for news data)
export HUGGINGFACE_TOKEN=your_token_here

# Run main analysis
python Run_analysis.py

# Results saved to results/enhanced/
\end{lstlisting}

Random seeds are fixed for reproducibility:
\begin{lstlisting}[language=Python, caption=Random Seed Configuration]
import numpy as np
import torch
import random

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
\end{lstlisting}

\chapter{Additional Visualizations}
\label{app:visualizations}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/04_linear_diagnostics.png}
    \caption{Linear Model Diagnostics. \textbf{Left:} Predicted vs actual plot shows near-perfect agreement along the diagonal. \textbf{Right:} Residual histogram is approximately normal with mean near zero. Slight heteroscedasticity visible at higher price levels indicates model performs slightly worse during the recent high-price regime (2020--2025).}
    \label{fig:linear_diagnostics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/05_tcn_diagnostics.png}
    \caption{TCN Model Diagnostics. The model captures overall trend but shows larger errors during volatile periods. The 2020 COVID crash and 2022 correction produce notable outliers in the residual distribution.}
    \label{fig:tcn_diagnostics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/06_model_comparison.png}
    \caption{Model Performance Comparison. Bar chart showing RMSE for all models (excluding Transformer for scale). Linear and SARIMAX achieve lowest errors. Neural networks cluster in \$7--\$12 range. TCN shows higher error due to training on full 26-year non-stationary data.}
    \label{fig:model_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/08_transformer_failure_analysis.png}
    \caption{Transformer Failure Analysis. \textbf{Left:} Scatter plot shows predictions clustering far from diagonal, with systematic under-prediction. \textbf{Right:} Error distribution is heavily skewed, indicating model learned training distribution mean but fails to generalize.}
    \label{fig:transformer_failure}
\end{figure}

\chapter{Data Availability}
\label{app:data}

\begin{table}[H]
\centering
\caption{Data Sources and Access}
\begin{tabular}{lll}
\hline
\textbf{Data} & \textbf{Source} & \textbf{Access} \\
\hline
Stock Prices & Yahoo Finance & \texttt{pip install yfinance} (public) \\
News Articles (2018--2023) & HuggingFace & API key required \\
Historical News (1999--2017) & CSV Archive & Included in repository \\
Related Stock Prices & Yahoo Finance & \texttt{pip install yfinance} (public) \\
Market Indices & Yahoo Finance & \texttt{pip install yfinance} (public) \\
\hline
\end{tabular}
\end{table}

\textbf{Data Availability Statement:} Stock price data is publicly available via Yahoo Finance. News data from HuggingFace requires an API key (free registration). Historical CSV news data is included in the project repository. All derived features, model outputs, and evaluation results are available for inspection and reproduction.

\end{document}
