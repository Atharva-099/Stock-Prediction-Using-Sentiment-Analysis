%% Chapter 9: Conclusion and Future Work
\chapter{Conclusion and Future Work}
\label{ch:conclusion}

\section{Summary of Achievements}

This research successfully developed a comprehensive stock price prediction system for Apple Inc. (AAPL) using sentiment analysis from financial news articles. The key achievements are:

\subsection{Research Aims Accomplished}

\begin{enumerate}
    \item \textbf{Aim 1 - Rolling Mean Quantification}: Tested windows of 3, 7, 14, 30 days; identified 7-day (\texttt{vader\_RM7}) as optimal
    
    \item \textbf{Aim 2 - Text Features}: Implemented LDA topic modeling, adjective extraction, and keyword analysis using \texttt{RichTextFeatureExtractor}
    
    \item \textbf{Aim 3 - Market Context}: Incorporated 27 features from related stocks (MSFT, GOOGL, AMZN) with 1-day lag to prevent lookahead bias
    
    \item \textbf{Aim 4 - Neural Networks}: Evaluated 9 architectures; 8/9 achieved positive $R^2$, with sklearn\_Linear achieving $R^2 = 0.9992$
    
    \item \textbf{Aim 5 - Documentation}: Complete code, logs, and visualizations preserved for reproducibility
    
    \item \textbf{Aim 6 - Temporal Validity}: Implemented walk-forward validation for SARIMAX and chronological train/test splits for all models
\end{enumerate}

\subsection{Performance Highlights}

\begin{table}[H]
\centering
\caption{Final Performance Summary}
\label{tab:final_summary}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Best Model R² & 0.9992 (sklearn\_Linear) \\
Best Model RMSE & \$1.83 \\
Best Model MAPE & 0.94\% \\
Ensemble R² & 0.9898 \\
Models > 0.95 R² & 3 \\
Models > 0.85 R² & 7 \\
Success Rate & 8/9 (89\%) \\
\bottomrule
\end{tabular}
\end{table}

\section{Key Contributions}

\subsection{Hybrid Meta-Learning Strategy}

We introduced a novel hybrid approach where:
\begin{enumerate}
    \item Foundational models (Linear, SARIMAX, TCN) train on full 26-year data
    \item Linear predictions serve as the 16th input feature for RNNs
    \item RNNs train on recent 5-year data to learn residual corrections
\end{enumerate}

This strategy improved GRU by +0.25 $R^2$, demonstrating effective meta-learning.

\subsection{Comprehensive Failure Analysis}

The detailed analysis of Transformer failure ($R^2 = -1.17$) provides valuable insights:
\begin{itemize}
    \item Architecture mismatch is worse than overfitting
    \item Reducing parameters made performance worse
    \item Self-attention requires proper sequence structure
\end{itemize}

\subsection{Large-Scale Data Integration}

We successfully integrated:
\begin{itemize}
    \item 26 years of stock price data (6,542 trading days)
    \item 57+ million financial news articles from HuggingFace
    \item Historical news archives from 1999-2017
\end{itemize}

\section{Theoretical Insights}

\subsection{Simplicity Can Win}

Linear Regression outperformed all deep learning models, demonstrating that:
\begin{itemize}
    \item Feature engineering matters more than model complexity
    \item Long-term trends in stock prices are approximately linear
    \item Complex architectures require proper task alignment
\end{itemize}

\subsection{Dataset Length Matters Differently}

\begin{table}[H]
\centering
\caption{Optimal Dataset Length by Model Type}
\label{tab:dataset_length}
\begin{tabular}{ll}
\toprule
\textbf{Model Type} & \textbf{Optimal Data} \\
\midrule
Linear, SARIMAX & 26 years (more is better) \\
TCN & 26 years (moderate amount) \\
RNNs (LSTM, GRU) & 5 years (recent is better) \\
Transformer & N/A (architecture mismatch) \\
\bottomrule
\end{tabular}
\end{table}

\section{Recommendations}

\subsection{For Practitioners}

\begin{enumerate}
    \item \textbf{Start with Linear Regression}: Despite its simplicity, it may be your best model
    \item \textbf{Invest in feature engineering}: Quality features > complex architectures
    \item \textbf{Use ensemble for robustness}: Weighted combination of diverse models
    \item \textbf{Avoid vanilla Transformers}: Unless reformulating as proper sequence task
    \item \textbf{Consider hybrid strategies}: Meta-learning can improve RNN performance
\end{enumerate}

\subsection{For Researchers}

\begin{enumerate}
    \item \textbf{Test specialized Transformers}: TFT, Informer, Autoformer for time series
    \item \textbf{Explore attention in RNNs}: Attention mechanisms without full Transformer
    \item \textbf{Investigate dynamic weighting}: Adaptive ensemble weights based on market regime
    \item \textbf{Multi-stock generalization}: Test if findings hold for other stocks
\end{enumerate}

\section{Future Work}

\subsection{Short-Term Improvements}

\begin{enumerate}
    \item \textbf{Time Series Transformers}: Implement PatchTST, Autoformer, Temporal Fusion Transformer
    \item \textbf{XGBoost/LightGBM}: Add gradient boosting methods for comparison
    \item \textbf{Attention RNNs}: Add attention layer to LSTM/GRU
    \item \textbf{Dynamic Ensemble Weights}: Adjust weights based on recent performance
\end{enumerate}

\subsection{Medium-Term Directions}

\begin{enumerate}
    \item \textbf{Multi-stock portfolio}: Extend to portfolio optimization
    \item \textbf{Event-driven features}: Extract specific event mentions from news
    \item \textbf{Real-time prediction}: Implement streaming prediction pipeline
    \item \textbf{Risk quantification}: Add uncertainty estimation to predictions
\end{enumerate}

\subsection{Long-Term Research Questions}

\begin{enumerate}
    \item Can sentiment analysis predict market regime changes?
    \item How do cross-market sentiments affect individual stocks?
    \item What is the optimal look-back window for sentiment relevance?
    \item Can LLMs improve sentiment extraction quality?
\end{enumerate}

\section{Reproducibility}

All experiments are fully reproducible using the provided code:

\begin{lstlisting}[language=bash, caption=Reproduction Steps]
# Step 1: Install dependencies
pip install -r requirements.txt

# Step 2: Fetch historical news (optional but recommended)
python fetch_news_1999_2025.py

# Step 3: Run complete analysis
python Run_analysis.py

# Outputs:
# - results/enhanced/enhanced_dataset_with_all_features.csv
# - results/enhanced/comprehensive_model_comparison.csv
# - results/enhanced/statistical/*.png (8 plots)
# - logs/full_pipeline.log
\end{lstlisting}

\section{Final Remarks}

This research demonstrates that combining traditional statistical methods with modern deep learning techniques can achieve exceptional stock price prediction accuracy. The key insight is that simpler models with well-engineered features often outperform complex architectures with poor feature alignment.

The hybrid strategy—using foundational model predictions as input features for neural networks—provides a principled way to leverage the strengths of both approaches. This meta-learning framework can be extended to other forecasting tasks beyond stock prices.

We hope this work contributes to the growing body of research at the intersection of natural language processing and financial forecasting, and provides practical guidance for practitioners seeking to implement sentiment-aware prediction systems.

\vspace{1cm}

\begin{center}
\textit{All code and data are available at:}\\
\texttt{github.com/[repository-url]}
\end{center}
