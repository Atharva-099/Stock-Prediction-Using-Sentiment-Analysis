%% Chapter 2: Data Collection and Preprocessing
\chapter{Data Collection and Preprocessing}
\label{ch:data_collection}

\section{Overview}

This chapter describes the comprehensive data collection and preprocessing pipeline used in our research. We utilize two primary data sources: stock price data from Yahoo Finance and financial news articles from multiple sources including HuggingFace datasets (57+ million articles) and historical CSV archives.

\begin{table}[H]
\centering
\caption{Data Sources Summary}
\label{tab:data_sources}
\begin{tabular}{llll}
\toprule
\textbf{Data Type} & \textbf{Source} & \textbf{Coverage} & \textbf{Records} \\
\midrule
Stock Prices & Yahoo Finance & 1999-2025 & 6,542 trading days \\
Financial News & HuggingFace & 2018-2023 & 57M+ articles \\
Historical News & CSV Archive & 1999-2017 & 685MB \\
Sentiment & TextBlob/VADER & Full range & Daily aggregates \\
\bottomrule
\end{tabular}
\end{table}

\section{Stock Price Data}

\subsection{Data Source: Yahoo Finance}

Stock price data for Apple Inc. (AAPL) was fetched using the Yahoo Finance API through the \texttt{yfinance} Python library. The data spans 26 years from January 1999 to January 2025.

\begin{lstlisting}[language=Python, caption=Stock Data Fetching Code]
from src.data_preprocessor import StockDataProcessor

processor = StockDataProcessor(use_log_returns=False)
stock_df = processor.fetch_stock_data(
    ticker='AAPL',
    start_date='1999-01-01',
    end_date='2025-01-01'
)
\end{lstlisting}

\subsection{Data Characteristics}

\begin{table}[H]
\centering
\caption{Stock Price Data Statistics}
\label{tab:stock_stats}
\begin{tabular}{lr}
\toprule
\textbf{Statistic} & \textbf{Value} \\
\midrule
Total Trading Days & 6,542 \\
Date Range & 1999-01-04 to 2024-12-31 \\
Minimum Price & \$0.25 \\
Maximum Price & \$260.10 \\
Mean Price & \$54.72 \\
Volatility ($\sigma$) & \$65.84 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Price Distribution Analysis}

Statistical tests reveal that stock prices do not follow a normal distribution:

\begin{itemize}
    \item \textbf{Shapiro-Wilk Test}: $p < 0.0001$ (reject normality)
    \item \textbf{Skewness}: 1.23 (positive skew indicating right-tailed distribution)
    \item \textbf{Kurtosis}: 0.54 (slightly leptokurtic)
\end{itemize}

Figure \ref{fig:distribution} shows the comprehensive distribution analysis including Q-Q plots, histograms, and kernel density estimation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/01_comprehensive_distribution.png}
    \caption{Comprehensive Distribution Analysis of AAPL Stock Prices (1999-2025). The figure includes: (a) Q-Q plot for normality assessment, (b) histogram with kernel density estimation, (c) distribution statistics including Shapiro-Wilk, Jarque-Bera, and Anderson-Darling test results.}
    \label{fig:distribution}
\end{figure}

\subsection{Time Series Diagnostics}

The time series exhibits clear non-stationarity with an upward trend over the 26-year period. Figure \ref{fig:time_series} presents the diagnostic plots.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/02_time_series_diagnostics.png}
    \caption{Time Series Diagnostics for AAPL Stock Prices. The figure shows: (a) price time series, (b) autocorrelation function (ACF), (c) partial autocorrelation function (PACF), and (d) seasonal decomposition.}
    \label{fig:time_series}
\end{figure}

\section{Financial News Data}

\subsection{HuggingFace Dataset}

The primary source of financial news is the HuggingFace dataset \texttt{Brianferrell787/financial-news-multisource}, which contains over 57 million financial news articles.

\begin{lstlisting}[language=Python, caption=HuggingFace News Fetching]
from src.huggingface_news_fetcher import HuggingFaceFinancialNewsDataset

hf_fetcher = HuggingFaceFinancialNewsDataset(hf_token=HUGGINGFACE_TOKEN)
articles_df = hf_fetcher.fetch_news_for_stock(
    ticker='AAPL',
    start_date='1999-01-01',
    end_date='2025-01-01',
    max_articles=5000
)
\end{lstlisting}

\subsection{CSV Historical Archive}

For earlier years (1999-2017) where HuggingFace coverage is limited, we use a historical CSV archive containing financial news:

\begin{lstlisting}[language=Python, caption=CSV Data Loading]
csv_path = 'data/news_articles/all_news_1999_2025.csv'
csv_data = pd.read_csv(csv_path)  # 685MB file

# Filter for AAPL-related articles
csv_data = csv_data[
    csv_data['text'].str.upper().str.contains('APPLE|AAPL')
]
\end{lstlisting}

\subsection{Data Merging Strategy}

To avoid duplicate coverage, we implement a date-based filtering strategy:

\begin{enumerate}
    \item \textbf{CSV Data}: 1999-2017 (before HuggingFace coverage)
    \item \textbf{HuggingFace Data}: 2018-2023 (primary source)
    \item \textbf{Google RSS Fallback}: 2020-2025 (recent news backup)
\end{enumerate}

\begin{lstlisting}[language=Python, caption=Data Merging Logic]
# Filter CSV to non-overlapping periods
csv_data = csv_data[
    (csv_data['date'] < pd.to_datetime('2018-01-01').date()) |
    (csv_data['date'] > pd.to_datetime('2020-06-10').date())
]

# Merge datasets
sentiment_df = sentiment_df.merge(
    csv_sentiment_df,
    on='Date',
    how='outer'
)
\end{lstlisting}

\section{Sentiment Computation}

\subsection{Sentiment Analysis Methods}

We employ two well-established sentiment analysis methods:

\subsubsection{TextBlob Sentiment}

TextBlob provides a simple API for sentiment analysis based on a pre-trained lexicon:

\begin{equation}
    \text{TextBlob}_{polarity} = \frac{\sum_{w \in \text{text}} \text{polarity}(w)}{|\text{text}|}
\end{equation}

where polarity ranges from $-1$ (negative) to $+1$ (positive).

\subsubsection{VADER Sentiment}

VADER (Valence Aware Dictionary and sEntiment Reasoner) is specifically designed for social media and financial text:

\begin{equation}
    \text{VADER}_{compound} = \frac{\sum_{w \in \text{text}} s(w) \cdot v(w)}{\sqrt{\left(\sum_{w \in \text{text}} s(w) \cdot v(w)\right)^2 + \alpha}}
\end{equation}

where $s(w)$ is the sentiment score, $v(w)$ is the sentiment valence, and $\alpha$ is a normalization constant.

\begin{lstlisting}[language=Python, caption=Sentiment Computation]
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

vader = SentimentIntensityAnalyzer()

for idx, row in daily_articles.iterrows():
    text = row['full_text']
    sentiment_scores.append({
        'date': row['date'],
        'textblob': TextBlob(text).sentiment.polarity,
        'vader': vader.polarity_scores(text)['compound']
    })
\end{lstlisting}

\subsection{Rolling Mean Aggregation}

Raw daily sentiment can be noisy. We apply rolling mean aggregation with multiple window sizes:

\begin{equation}
    \text{Sentiment}_{RM_w}(t) = \frac{1}{w} \sum_{i=0}^{w-1} \text{Sentiment}(t-i)
\end{equation}

where $w \in \{3, 7, 14, 30\}$ days.

\begin{lstlisting}[language=Python, caption=Rolling Mean Computation]
WINDOWS = [3, 7, 14, 30]

for window in WINDOWS:
    for col in ['textblob', 'vader']:
        sentiment_df[f'{col}_RM{window}'] = (
            sentiment_df[col].rolling(window=window, min_periods=1).mean()
        )
\end{lstlisting}

\section{Data Quality and Coverage}

\subsection{Sentiment Coverage Analysis}

\begin{table}[H]
\centering
\caption{Sentiment Data Coverage}
\label{tab:sentiment_coverage}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Percentage} \\
\midrule
Total Trading Days & 6,542 & 100\% \\
Days with Sentiment & 2,030 & 31.0\% \\
Missing Days (filled) & 4,512 & 69.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Missing Data Handling}

Missing sentiment values are handled using forward-fill followed by zero-fill:

\begin{lstlisting}[language=Python, caption=Missing Data Handling]
merged_df[sentiment_cols] = merged_df[sentiment_cols].fillna(method='ffill')
merged_df[sentiment_cols] = merged_df[sentiment_cols].fillna(0.0)
\end{lstlisting}

\section{Dataset Splitting Strategy}

We use a temporal split to maintain the time series nature of the data:

\begin{table}[H]
\centering
\caption{Dataset Splitting}
\label{tab:data_split}
\begin{tabular}{llrr}
\toprule
\textbf{Dataset} & \textbf{Split} & \textbf{Samples} & \textbf{Percentage} \\
\midrule
26-Year (Full) & Training & 4,579 & 70\% \\
26-Year (Full) & Testing & 1,963 & 30\% \\
\midrule
5-Year (Recent) & Training & 878 & 70\% \\
5-Year (Recent) & Testing & 377 & 30\% \\
\bottomrule
\end{tabular}
\end{table}

The split is performed using a simple chronological division to prevent lookahead bias:

\begin{lstlisting}[language=Python, caption=Temporal Data Split]
size_26y = int(len(target_26y) * 0.70)
train_26y, test_26y = target_26y[:size_26y], target_26y[size_26y:]
dates_test_26y = merged_df_26y['Date'].tolist()[size_26y:]
\end{lstlisting}

\section{Key Implementation Files}

The data collection pipeline is implemented in the following Python files:

\begin{table}[H]
\centering
\caption{Data Collection Implementation Files}
\label{tab:data_files}
\begin{tabular}{ll}
\toprule
\textbf{File} & \textbf{Purpose} \\
\midrule
\texttt{src/data\_preprocessor.py} & Stock data fetching and preprocessing \\
\texttt{src/huggingface\_news\_fetcher.py} & HuggingFace dataset interface \\
\texttt{advanced\_sentiment.py} & Multi-method sentiment computation \\
\texttt{fetch\_news\_1999\_2025.py} & Historical news fetching script \\
\bottomrule
\end{tabular}
\end{table}
