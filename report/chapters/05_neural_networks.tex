%% Chapter 5: Neural Network Models
\chapter{Neural Network Models}
\label{ch:neural_networks}

\section{Overview}

This chapter presents the recurrent neural network architectures evaluated in our research: LSTM, BiLSTM, GRU, and CNN-LSTM. These models are trained on 5-year recent data (2020-2025) using the hybrid strategy with Linear predictions as the 16th input feature.

\begin{table}[H]
\centering
\caption{Neural Network Models Performance}
\label{tab:nn_performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{RÂ²} & \textbf{RMSE (\$)} & \textbf{Features} \\
\midrule
CNN-LSTM & 5-year & 0.8939 & 7.34 & 16 (hybrid) \\
GRU & 5-year & 0.8856 & 7.63 & 16 (hybrid) \\
BiLSTM & 5-year & 0.8812 & 7.77 & 16 (hybrid) \\
LSTM & 5-year & 0.7109 & 12.12 & 16 (hybrid) \\
\bottomrule
\end{tabular}
\end{table}

\section{Why 5-Year Data for RNNs}

\subsection{The Non-Stationarity Problem}

Training RNNs on 26-year data introduces significant challenges:

\begin{enumerate}
    \item \textbf{Distribution shift}: Prices ranged from \$0.25 in 1999 to \$260 in 2025
    \item \textbf{Regime changes}: Multiple market regimes (dot-com, 2008 crisis, COVID)
    \item \textbf{Pattern obsolescence}: Market patterns from 1999-2010 may be irrelevant today
    \item \textbf{Gradient issues}: Long training sequences exacerbate vanishing/exploding gradients
\end{enumerate}

\subsection{5-Year Window Benefits}

By training RNNs on recent 5-year data:

\begin{itemize}
    \item Captures current market dynamics
    \item Reduces distribution shift (prices: \$100-\$260)
    \item Focuses on relevant patterns
    \item Training set: 878 samples (sufficient for RNNs)
\end{itemize}

\section{LSTM (Long Short-Term Memory)}

\subsection{Mathematical Formulation}

LSTM addresses the vanishing gradient problem through gating mechanisms:

\begin{definition}[LSTM Cell]
The LSTM cell at time step $t$ computes:

\textbf{Forget Gate:}
\begin{equation}
    f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\end{equation}

\textbf{Input Gate:}
\begin{equation}
    i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\end{equation}

\textbf{Candidate Cell State:}
\begin{equation}
    \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
\end{equation}

\textbf{Cell State Update:}
\begin{equation}
    C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\end{equation}

\textbf{Output Gate:}
\begin{equation}
    o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\end{equation}

\textbf{Hidden State:}
\begin{equation}
    h_t = o_t \odot \tanh(C_t)
\end{equation}
\end{definition}

where:
\begin{itemize}
    \item $\sigma(\cdot)$ is the sigmoid function
    \item $\odot$ denotes element-wise multiplication (Hadamard product)
    \item $W_*$ are weight matrices, $b_*$ are bias vectors
    \item $h_t$ is the hidden state, $C_t$ is the cell state
\end{itemize}

\subsection{Architecture}

\begin{lstlisting}[language=Python, caption=LSTM Architecture]
class LSTMModel(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,  # 16 features
            hidden_size=64,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        self.fc = nn.Linear(64, 1)
    
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])  # Last time step
\end{lstlisting}

\subsection{Training Configuration}

\begin{lstlisting}[language=Python, caption=LSTM Training]
set_seed(46)  # Specific seed to avoid early stopping

lstm_model = LSTMModel(len(dl_features)+1).to(device)  # 16 features
optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.001)
criterion_lstm = nn.MSELoss()

for epoch in range(150):  # Extended epochs for LSTM
    lstm_model.train()
    optimizer_lstm.zero_grad()
    outputs = lstm_model(X_tensor)
    loss = criterion_lstm(outputs, y_tensor)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)
    optimizer_lstm.step()
    
    # Early stopping with patience
    if patience_counter >= 25:
        break
\end{lstlisting}

\section{BiLSTM (Bidirectional LSTM)}

\subsection{Architecture}

BiLSTM processes sequences in both forward and backward directions:

\begin{equation}
    \overrightarrow{h}_t = \text{LSTM}_{\text{forward}}(x_t, \overrightarrow{h}_{t-1})
\end{equation}

\begin{equation}
    \overleftarrow{h}_t = \text{LSTM}_{\text{backward}}(x_t, \overleftarrow{h}_{t+1})
\end{equation}

\begin{equation}
    h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]
\end{equation}

\begin{lstlisting}[language=Python, caption=BiLSTM Architecture]
class BiLSTMModel(nn.Module):
    """Bidirectional LSTM"""
    def __init__(self, input_size):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size, 64, 2, 
            batch_first=True, 
            dropout=0.2, 
            bidirectional=True  # Key difference
        )
        self.fc = nn.Linear(128, 1)  # 64*2 = 128 (bidirectional)
    
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])
\end{lstlisting}

\subsection{Why Bidirectional Helps}

In financial time series with complete sequences (batch training):
\begin{itemize}
    \item Backward pass provides additional context
    \item Captures patterns visible only when looking back from future
    \item Output size doubles (128 vs 64) providing more expressiveness
\end{itemize}

\section{GRU (Gated Recurrent Unit)}

\subsection{Mathematical Formulation}

GRU is a simplified version of LSTM with fewer gates:

\begin{definition}[GRU Cell]
\textbf{Reset Gate:}
\begin{equation}
    r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
\end{equation}

\textbf{Update Gate:}
\begin{equation}
    z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
\end{equation}

\textbf{Candidate Hidden State:}
\begin{equation}
    \tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)
\end{equation}

\textbf{Hidden State Update:}
\begin{equation}
    h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{equation}
\end{definition}

GRU combines the forget and input gates into a single update gate, reducing parameters.

\begin{lstlisting}[language=Python, caption=GRU Architecture]
class GRUModel(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.gru = nn.GRU(
            input_size, 64, 2, 
            batch_first=True, 
            dropout=0.2
        )
        self.fc = nn.Linear(64, 1)
    
    def forward(self, x):
        out, _ = self.gru(x)
        return self.fc(out[:, -1, :])
\end{lstlisting}

\subsection{GRU Performance}

GRU showed the largest improvement with the hybrid strategy (+0.25 $R^2$), suggesting it is particularly effective at learning residual corrections.

\section{CNN-LSTM Hybrid}

\subsection{Architecture Motivation}

CNN-LSTM combines:
\begin{itemize}
    \item \textbf{CNN}: Extracts local patterns from features
    \item \textbf{LSTM}: Captures temporal dependencies
\end{itemize}

\subsection{Mathematical Formulation}

\textbf{CNN Layer} (1D Convolution):
\begin{equation}
    h_i = \text{ReLU}\left(\sum_{j=0}^{k-1} W_j \cdot x_{i+j} + b\right)
\end{equation}

\textbf{LSTM Layer}:
Standard LSTM processing of CNN output.

\begin{lstlisting}[language=Python, caption=CNN-LSTM Architecture]
class CNNLSTMModel(nn.Module):
    """CNN-LSTM Hybrid"""
    def __init__(self, input_size):
        super().__init__()
        # CNN for feature extraction
        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)
        # LSTM for sequence modeling
        self.lstm = nn.LSTM(32, 64, 1, batch_first=True)
        self.fc = nn.Linear(64, 1)
    
    def forward(self, x):
        # x shape: (batch, seq_len, features)
        x = x.permute(0, 2, 1)  # (batch, features, seq_len)
        x = torch.relu(self.conv1(x))  # Conv1d
        x = x.permute(0, 2, 1)  # (batch, seq_len, channels)
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])
\end{lstlisting}

\section{Training Helper Function}

All RNNs use a common training function with early stopping:

\begin{lstlisting}[language=Python, caption=Training Helper Function]
def train_and_eval(model_name, ModelClass, epochs=100):
    # Use len(dl_features)+1 to include Linear prediction feature
    model = ModelClass(len(dl_features)+1).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
        
        if patience_counter >= 15:  # Early stopping
            break
    
    model.eval()
    with torch.no_grad():
        pred_scaled = model(X_test_tensor).cpu().numpy().flatten()
    
    pred = scaler_y.inverse_transform(
        pred_scaled.reshape(-1, 1)
    ).flatten()
    metrics = compute_all_metrics(y_test, pred)
    
    return metrics
\end{lstlisting}

\section{Hyperparameters}

\begin{table}[H]
\centering
\caption{RNN Hyperparameters}
\label{tab:rnn_hyperparams}
\begin{tabular}{lr}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Hidden Size & 64 \\
Number of Layers & 2 (LSTM/BiLSTM/GRU), 1 (CNN-LSTM) \\
Dropout & 0.2 \\
Learning Rate & 0.001 \\
Epochs & 100-150 \\
Optimizer & Adam \\
Loss Function & MSE \\
Gradient Clipping & 1.0 \\
Early Stopping Patience & 15-25 \\
\bottomrule
\end{tabular}
\end{table}

\section{Impact of Hybrid Strategy}

\subsection{Before and After Comparison}

\begin{table}[H]
\centering
\caption{Hybrid Strategy Impact}
\label{tab:hybrid_impact}
\begin{tabular}{lrrl}
\toprule
\textbf{Model} & \textbf{Without Hybrid} & \textbf{With Hybrid} & \textbf{Change} \\
\midrule
LSTM & 0.7109 & 0.7109 & +0.00 \\
BiLSTM & 0.85 & 0.8812 & +0.03 \\
GRU & 0.64 & 0.8856 & \textbf{+0.25} \\
CNN-LSTM & 0.87 & 0.8939 & +0.02 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Why GRU Benefits Most}

GRU's simpler architecture (fewer parameters than LSTM) makes it more effective at learning the correction task:
\begin{itemize}
    \item Less prone to overfitting on the small 5-year dataset
    \item Update gate directly controls information flow
    \item More efficient gradient flow for residual learning
\end{itemize}

\section{Computational Requirements}

\begin{table}[H]
\centering
\caption{Model Computational Requirements}
\label{tab:compute_requirements}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Training Time} & \textbf{Device} \\
\midrule
LSTM & ~50K & ~2 min & CPU/GPU \\
BiLSTM & ~100K & ~3 min & CPU/GPU \\
GRU & ~40K & ~2 min & CPU/GPU \\
CNN-LSTM & ~35K & ~2 min & CPU/GPU \\
\bottomrule
\end{tabular}
\end{table}

\section{Summary}

\begin{itemize}
    \item RNNs are trained on 5-year data to avoid non-stationarity issues
    \item The hybrid strategy adds Linear predictions as the 16th feature
    \item GRU shows the largest improvement (+0.25 $R^2$) with hybrid strategy
    \item CNN-LSTM achieves the best RNN performance ($R^2 = 0.8939$)
    \item All models use gradient clipping (1.0) for stability
\end{itemize}
