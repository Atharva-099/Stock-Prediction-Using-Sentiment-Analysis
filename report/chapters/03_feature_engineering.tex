%% Chapter 3: Feature Engineering
\chapter{Feature Engineering}
\label{ch:feature_engineering}

\section{Overview}

Feature engineering is crucial for successful stock price prediction. We engineer 55 features across four categories: sentiment features, text features, market context features, and price-based features. Additionally, we introduce a novel 16th feature for the hybrid RNN strategy: predictions from the Linear model.

\begin{table}[H]
\centering
\caption{Feature Categories Summary}
\label{tab:feature_summary}
\begin{tabular}{lrl}
\toprule
\textbf{Category} & \textbf{Count} & \textbf{Description} \\
\midrule
Sentiment Features & 20 & TextBlob, VADER + rolling means \\
Text Features & 8 & LDA topics, adjectives, keywords \\
Market Context Features & 27 & Related stocks, market indices \\
Price Rolling Features & 8 & Close/Volume rolling means \\
\midrule
\textbf{Total} & \textbf{55} & Base features \\
\midrule
Hybrid Feature & +1 & Linear model predictions \\
\bottomrule
\end{tabular}
\end{table}

\section{Sentiment Features (20 Features)}

\subsection{Base Sentiment Scores}

Two sentiment analysis methods are applied to financial news:

\begin{definition}[TextBlob Polarity]
The TextBlob polarity score $p_{\text{TB}} \in [-1, 1]$ is computed as:
\begin{equation}
    p_{\text{TB}} = \frac{\sum_{w \in \text{words}} \text{polarity}(w) \cdot \text{subjectivity}(w)}{\sum_{w \in \text{words}} \text{subjectivity}(w)}
\end{equation}
\end{definition}

\begin{definition}[VADER Compound Score]
The VADER compound score $c_{\text{VA}} \in [-1, 1]$ is computed as:
\begin{equation}
    c_{\text{VA}} = \frac{x}{\sqrt{x^2 + \alpha}}
\end{equation}
where $x = \sum_{i} s_i$ is the sum of valence scores and $\alpha = 15$ is a normalization constant.
\end{definition}

\subsection{Rolling Mean Features}

For each base sentiment score, we compute rolling means with windows $w \in \{3, 7, 14, 30\}$ days:

\begin{equation}
    \text{RM}_w(t) = \frac{1}{\min(w, t+1)} \sum_{i=\max(0, t-w+1)}^{t} s_i
\end{equation}

This yields 10 features per sentiment method (1 raw + 4 rolling means × 2 methods = 10 features for HuggingFace data, plus 10 for CSV data = 20 total).

\subsection{Feature List}

\begin{table}[H]
\centering
\caption{Sentiment Feature Names}
\label{tab:sentiment_features}
\begin{tabular}{ll}
\toprule
\textbf{Feature Name} & \textbf{Description} \\
\midrule
\texttt{textblob} & Raw TextBlob polarity score \\
\texttt{textblob\_RM3} & 3-day rolling mean of TextBlob \\
\texttt{textblob\_RM7} & 7-day rolling mean of TextBlob \\
\texttt{textblob\_RM14} & 14-day rolling mean of TextBlob \\
\texttt{textblob\_RM30} & 30-day rolling mean of TextBlob \\
\texttt{vader} & Raw VADER compound score \\
\texttt{vader\_RM3} & 3-day rolling mean of VADER \\
\texttt{vader\_RM7} & 7-day rolling mean of VADER \\
\texttt{vader\_RM14} & 14-day rolling mean of VADER \\
\texttt{vader\_RM30} & 30-day rolling mean of VADER \\
\bottomrule
\end{tabular}
\end{table}

The 7-day rolling mean (\texttt{vader\_RM7}) was identified as the optimal sentiment feature through correlation analysis.

\section{Text Features (8 Features)}

\subsection{LDA Topic Modeling}

Latent Dirichlet Allocation (LDA) extracts latent topics from news text:

\begin{equation}
    p(\theta, z, w | \alpha, \beta) = p(\theta | \alpha) \prod_{n=1}^{N} p(z_n | \theta) p(w_n | z_n, \beta)
\end{equation}

where $\theta$ is the topic distribution, $z$ is the topic assignment, $w$ is the word, and $\alpha, \beta$ are hyperparameters.

We extract 5 topic weights per document:

\begin{lstlisting}[language=Python, caption=LDA Feature Extraction]
extractor = RichTextFeatureExtractor(
    max_features=15, 
    n_topics=5, 
    min_df=1, 
    max_df=1.0
)
extractor.fit_lda(texts)
\end{lstlisting}

\subsection{Adjective Features}

Financial news adjectives often carry sentiment (e.g., "strong earnings", "weak outlook"):

\begin{lstlisting}[language=Python, caption=Adjective Extraction]
text_features = extractor.extract_all_features(
    texts, 
    include_adjectives=True,
    include_keywords=True
)
\end{lstlisting}

\section{Market Context Features (27 Features)}

\subsection{Related Stock Features}

We incorporate price movements of related technology stocks: MSFT (Microsoft), GOOGL (Google), and AMZN (Amazon).

\begin{definition}[Lagged Features]
To prevent lookahead bias, all related stock features use a 1-day lag:
\begin{equation}
    X_{\text{related}}(t) = P_{\text{related}}(t-1)
\end{equation}
\end{definition}

\begin{lstlisting}[language=Python, caption=Related Stock Features]
engine = RelatedStocksFeatureEngine(
    related_tickers=['MSFT', 'GOOGL', 'AMZN']
)
related_features = engine.create_all_features(
    target_df=stock_df,
    target_ticker='AAPL',
    lag_days=1,  # Prevent lookahead bias
    include_relative=True,
    include_correlation=True,
    include_market_indices=True
)
\end{lstlisting}

\subsection{Feature Types}

\begin{table}[H]
\centering
\caption{Market Context Feature Types}
\label{tab:market_features}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Type} & \textbf{Description} \\
\midrule
Lagged Prices & \texttt{MSFT\_lag1}, \texttt{GOOGL\_lag1}, \texttt{AMZN\_lag1} \\
Relative Returns & Price change relative to previous day \\
Correlation & Rolling correlation with AAPL \\
Market Indices & Sector-level indicators \\
\bottomrule
\end{tabular}
\end{table}

\section{Price Rolling Features (8 Features)}

We compute rolling means of price and volume to capture short-term trends:

\begin{equation}
    \text{Close\_RM}_w(t) = \frac{1}{w} \sum_{i=t-w+1}^{t} \text{Close}_i
\end{equation}

\begin{equation}
    \text{Volume\_RM}_w(t) = \frac{1}{w} \sum_{i=t-w+1}^{t} \text{Volume}_i
\end{equation}

for windows $w \in \{3, 7, 14, 30\}$ days.

\begin{lstlisting}[language=Python, caption=Price Rolling Features]
WINDOWS = [3, 7, 14, 30]
for window in WINDOWS:
    merged_df[f'Close_RM{window}'] = merged_df['Close'].rolling(
        window=window, min_periods=1
    ).mean()
    merged_df[f'Volume_RM{window}'] = merged_df['Volume'].rolling(
        window=window, min_periods=1
    ).mean()
\end{lstlisting}

\section{The 16th Feature: Hybrid Strategy}

\subsection{Motivation}

Traditional approaches train neural networks to predict stock prices directly from features. This is challenging because:

\begin{enumerate}
    \item RNNs require learning both the general price-feature relationship AND specific patterns
    \item Training on 26 years of data introduces non-stationarity issues
    \item The prediction task has high variance due to multiple price regimes
\end{enumerate}

\subsection{Hybrid Approach}

Our hybrid strategy adds predictions from the Linear model (trained on 26-year data) as a 16th input feature:

\begin{equation}
    \mathbf{X}_{\text{hybrid}} = [\mathbf{X}_{\text{original}}, \hat{y}_{\text{linear}}]
\end{equation}

where $\mathbf{X}_{\text{original}} \in \mathbb{R}^{n \times 15}$ and $\hat{y}_{\text{linear}} \in \mathbb{R}^n$ is the Linear model prediction.

\begin{lstlisting}[language=Python, caption=Adding Linear Predictions as 16th Feature]
# Generate Linear model predictions for 5-year data
linear_pred_train_5y = lr.predict(scaler_X_5y.transform(X_train_5y))
linear_pred_test_5y = lr.predict(scaler_X_5y.transform(X_test_5y))

# Add as new feature
X_train_with_linear = np.concatenate([
    X_train_scaled,
    linear_pred_train_5y.reshape(-1, 1)
], axis=1)

X_test_with_linear = np.concatenate([
    X_test_scaled,
    linear_pred_test_5y.reshape(-1, 1)
], axis=1)
\end{lstlisting}

\subsection{Theoretical Justification}

The hybrid approach transforms the learning task from:

\begin{equation}
    \text{Learn: } f(\mathbf{X}) \rightarrow y
\end{equation}

to:

\begin{equation}
    \text{Learn: } g(\mathbf{X}, \hat{y}_{\text{linear}}) \rightarrow y - \hat{y}_{\text{linear}} + \hat{y}_{\text{linear}} = y
\end{equation}

The RNN now focuses on learning residual corrections:

\begin{equation}
    \text{Residual} = y - \hat{y}_{\text{linear}}
\end{equation}

This is a simpler task because:
\begin{itemize}
    \item The Linear model already captures the main price trend ($R^2 = 0.9992$)
    \item The RNN only needs to learn the error patterns
    \item The residuals have lower variance than the full price series
\end{itemize}

\subsection{Impact on Model Performance}

\begin{table}[H]
\centering
\caption{Hybrid Strategy Performance Improvement}
\label{tab:hybrid_improvement}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{R² (15 features)} & \textbf{R² (16 features)} & \textbf{Improvement} \\
\midrule
LSTM & 0.71 & 0.71 & +0.00 \\
BiLSTM & 0.85 & 0.88 & +0.03 \\
GRU & 0.64 & 0.89 & \textbf{+0.25} \\
CNN-LSTM & 0.87 & 0.89 & +0.02 \\
\bottomrule
\end{tabular}
\end{table}

GRU showed the largest improvement (+0.25 $R^2$) with the hybrid strategy, demonstrating that it effectively learns to correct Linear's predictions.

\section{Feature Correlation Analysis}

Figure \ref{fig:correlation} shows the correlation matrix between key features and the target (Close price).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/03_correlation_matrix.png}
    \caption{Feature Correlation Matrix showing relationships between sentiment features, price rolling means, and the target Close price. Strong correlations between rolling means and Close indicate effective feature engineering.}
    \label{fig:correlation}
\end{figure}

\section{Feature Scaling}

All features are scaled using MinMaxScaler to the range $[0, 1]$:

\begin{equation}
    X_{\text{scaled}} = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
\end{equation}

\begin{lstlisting}[language=Python, caption=Feature Scaling]
from sklearn.preprocessing import MinMaxScaler

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
\end{lstlisting}

\section{Implementation Files}

\begin{table}[H]
\centering
\caption{Feature Engineering Implementation Files}
\label{tab:feature_files}
\begin{tabular}{ll}
\toprule
\textbf{File} & \textbf{Purpose} \\
\midrule
\texttt{src/sentiment\_comparison.py} & Sentiment feature creation \\
\texttt{src/rich\_text\_features.py} & LDA, BOW, TF-IDF, adjectives \\
\texttt{src/related\_stocks\_features.py} & Market context features \\
\texttt{Run\_analysis.py} & Pipeline integration \\
\bottomrule
\end{tabular}
\end{table}
