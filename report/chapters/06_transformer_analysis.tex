%% Chapter 6: Transformer Analysis
\chapter{Transformer Analysis}
\label{ch:transformer_analysis}

\section{Overview}

This chapter provides a detailed analysis of why Transformer models failed catastrophically for our stock price prediction task, achieving a negative $R^2$ of $-1.17$. Understanding this failure is crucial for future research and for practitioners considering Transformer architectures for financial forecasting.

\begin{table}[H]
\centering
\caption{Transformer Variations Tested}
\label{tab:transformer_variations}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Attempt} & \textbf{d\_model} & \textbf{Heads} & \textbf{Layers} & \textbf{Params} & \textbf{R²} \\
\midrule
Original & 64 & 4 & 2 & ~52K & -1.17 \\
SmallTransformer & 32 & 2 & 1 & ~6K & -1.45 \\
TinyTransformer & 16 & 1 & 1 & ~2.5K & -1.88 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observation}: Reducing parameters made performance \textit{worse}, not better. This indicates the problem is \textit{not} overfitting.

\section{Transformer Architecture}

\subsection{Self-Attention Mechanism}

The core innovation of Transformers is the self-attention mechanism:

\begin{definition}[Scaled Dot-Product Attention]
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where:
\begin{itemize}
    \item $Q \in \mathbb{R}^{n \times d_k}$ = Query matrix
    \item $K \in \mathbb{R}^{n \times d_k}$ = Key matrix
    \item $V \in \mathbb{R}^{n \times d_v}$ = Value matrix
    \item $d_k$ = dimension of keys (scaling factor)
\end{itemize}
\end{definition}

\begin{definition}[Multi-Head Attention]
\begin{equation}
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
\end{equation}
where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$
\end{definition}

\subsection{Implementation}

\begin{lstlisting}[language=Python, caption=Transformer Architecture]
class TransformerModel(nn.Module):
    """Original Transformer architecture"""
    def __init__(self, input_size):
        super().__init__()
        self.input_proj = nn.Linear(input_size, 64)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=64, 
            nhead=4, 
            dim_feedforward=256, 
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=2
        )
        self.fc = nn.Linear(64, 1)
    
    def forward(self, x):
        x = self.input_proj(x)
        x = self.transformer(x)
        return self.fc(x[:, -1, :])  # Last position output
\end{lstlisting}

\section{Failure Analysis}

\subsection{Training Dynamics}

Training metrics show the model learns to fit training data well:

\begin{lstlisting}[caption=Training Log Analysis]
Epoch 20:  Loss = 0.017
Epoch 40:  Loss = 0.006
Epoch 60:  Loss = 0.004
Epoch 80:  Loss = 0.003
Epoch 100: Loss = 0.002  # Excellent convergence
\end{lstlisting}

\textbf{Test Performance}:
\begin{lstlisting}[caption=Test Results]
RMSE = $97.01  (vs Linear's $1.83 - 53x worse!)
R^2 = -1.17    (negative = worse than predicting mean)
\end{lstlisting}

This pattern—good training loss but catastrophic test performance—indicates a fundamental architecture mismatch, not mere overfitting.

\subsection{Root Cause 1: Task Mismatch}

Transformers are designed for sequence-to-sequence tasks:

\begin{table}[H]
\centering
\caption{Task Type Comparison}
\label{tab:task_mismatch}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Transformer Design} & \textbf{Our Task} \\
\midrule
Input & Sequence of tokens & Feature vector \\
Output & Sequence of tokens & Single price value \\
Attention & Token-to-token & Feature-to-feature (?) \\
Sequence length & 100s-1000s & 1 (unsqueezed) \\
\bottomrule
\end{tabular}
\end{table}

Our workaround of \texttt{.unsqueeze(1)} creates a fake sequence of length 1:

\begin{lstlisting}[language=Python, caption=Input Reshaping]
# Original: (batch_size, num_features) = (4579, 15)
# After unsqueeze: (batch_size, seq_len=1, num_features) = (4579, 1, 15)
X_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(1)
\end{lstlisting}

\textbf{Problem}: Self-attention between 1 time step and itself is meaningless.

\subsection{Root Cause 2: Architecture Mismatch}

\begin{table}[H]
\centering
\caption{Transformer Components Analysis}
\label{tab:arch_mismatch}
\begin{tabular}{lp{9cm}}
\toprule
\textbf{Component} & \textbf{Why It Fails} \\
\midrule
Self-Attention & Computes attention between ONE position and itself \\
Multi-Head & No benefit when sequence length = 1 \\
Positional Encoding & Meaningless for single position \\
Feed-Forward & Only component actually working \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Root Cause 3: Output Distribution Mismatch}

Analysis suggests the model outputs values that don't match the expected distribution:

\begin{enumerate}
    \item Model trained on scaled values in $[0, 1]$
    \item Model may output extreme values outside this range
    \item Inverse transform amplifies these errors dramatically
    \item RMSE of \$97 on \$150-200 stock = ~50\% error
\end{enumerate}

\section{What We Tried to Fix It}

\subsection{Attempt 1: Architecture Reduction}

\textbf{Hypothesis}: Model too complex for 4,579 samples.

\begin{lstlisting}[language=Python, caption=Reduced Architecture]
class SmallTransformer(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.input_proj = nn.Linear(input_size, 32)  # 64 -> 32
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=32,       # 64 -> 32
            nhead=2,          # 4 -> 2
            dim_feedforward=64,  # 256 -> 64
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=1  # 2 -> 1
        )
        self.fc = nn.Linear(32, 1)
\end{lstlisting}

\textbf{Result}: FAILED. $R^2$ got worse ($-1.17 \rightarrow -1.45 \rightarrow -1.88$).

\textbf{Conclusion}: Problem is NOT overfitting.

\subsection{Attempt 2: Scaler Reference Fix}

\textbf{Hypothesis}: Scaler was being overwritten by 5-year data processing.

\begin{lstlisting}[language=Python, caption=Deep Copy Scaler]
from copy import deepcopy
scaler_y_26y = deepcopy(scaler_y)  # Independent copy
\end{lstlisting}

\textbf{Result}: FAILED. No improvement.

\subsection{Attempt 3: Variable Preservation}

\textbf{Hypothesis}: 26-year data variables being overwritten.

\begin{lstlisting}[language=Python, caption=Variable Preservation]
# Save BEFORE 5-year processing
X_train_26y_saved = X_train_scaled.copy()
y_train_26y_saved = y_train_scaled.copy()
X_test_26y_saved = X_test_scaled.copy()
\end{lstlisting}

\textbf{Result}: FAILED. Still $R^2 = -1.17$.

\subsection{Attempt 4: Training Location}

\textbf{Hypothesis}: Training context matters.

\textbf{Action}: Moved Transformer training after all 5-year models.

\textbf{Result}: FAILED. No improvement.

\section{Why Other Models Succeed}

\begin{table}[H]
\centering
\caption{Model Mechanism Comparison}
\label{tab:model_comparison}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Mechanism} & \textbf{Why Works} \\
\midrule
Linear & $y = \sum w_i x_i + b$ & Direct feature-to-value \\
SARIMAX & $y_t = f(y_{t-1}, \ldots, X_t)$ & Time series autoregression \\
TCN & Dilated 1D convolutions & Features as pseudo-sequence \\
LSTM/GRU & Recurrent connections & Batch as sequence \\
Transformer & Self-attention & \textbf{No mechanism for single-step} \\
\bottomrule
\end{tabular}
\end{table}

\section{Transformer Failure Visualization}

Figure \ref{fig:transformer_failure} shows the comprehensive failure analysis for the Transformer model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/08_transformer_failure_analysis.png}
    \caption{Transformer Failure Analysis. The figure demonstrates the catastrophic prediction errors including: (a) Predicted vs Actual scatter showing extreme divergence, (b) Error distribution, (c) Time series comparison showing predictions completely missing the target, and (d) Summary of failure modes.}
    \label{fig:transformer_failure}
\end{figure}

\section{Interpretation of Negative R²}

A negative $R^2$ indicates the model performs worse than simply predicting the mean:

\begin{equation}
    R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
\end{equation}

For $R^2 = -1.17$:
\begin{equation}
    SS_{res} = 2.17 \times SS_{tot}
\end{equation}

The residual sum of squares is more than twice the total sum of squares—the predictions are actively harmful.

\section{What Would Actually Fix Transformer}

\subsection{Option 1: Time Series Transformers}

Specialized architectures designed for time series:

\begin{itemize}
    \item \textbf{Informer}: ProbSparse attention for long sequences
    \item \textbf{Autoformer}: Auto-correlation instead of self-attention
    \item \textbf{Temporal Fusion Transformer (TFT)}: Designed for tabular time series
    \item \textbf{PatchTST}: Treats time series as patches like Vision Transformer
\end{itemize}

\subsection{Option 2: Sequence Reformulation}

Transform the task into a proper sequence problem:

\begin{lstlisting}[language=Python, caption=Sequence Reformulation]
# Instead of: (batch, 1, features)
# Use windowed approach: (batch, window_size, features)
window_size = 30  # 30 days of history
X_windowed = create_windows(X, window_size)
# Shape: (batch, 30, features)
\end{lstlisting}

\subsection{Option 3: Feature Dimension as Sequence}

Treat features as sequence positions:

\begin{lstlisting}[language=Python, caption=Features as Sequence]
# Instead of: (batch, 1, 15_features)
# Transpose to: (batch, 15_features, 1)
# Self-attention between features
X_transposed = X.permute(0, 2, 1)
\end{lstlisting}

\section{Lessons Learned}

\begin{enumerate}
    \item \textbf{Architecture matters}: Not all neural networks are suitable for all tasks
    \item \textbf{Sequence length matters}: Transformers need actual sequences, not single vectors
    \item \textbf{Reducing parameters doesn't always help}: When the architecture is wrong, simplification makes it worse
    \item \textbf{Good training loss $\neq$ good generalization}: Especially with architecture mismatch
    \item \textbf{Simpler models can outperform complex ones}: Linear Regression ($R^2 = 0.9992$) vs Transformer ($R^2 = -1.17$)
\end{enumerate}

\section{Recommendations for Practitioners}

For stock price prediction with tabular features:

\begin{enumerate}
    \item \textbf{Start simple}: Linear Regression, Random Forest, XGBoost
    \item \textbf{Use RNNs carefully}: Train on recent data, use hybrid strategy
    \item \textbf{Avoid vanilla Transformers}: Unless reformulating as proper sequence task
    \item \textbf{Consider specialized architectures}: TFT, Autoformer if Transformer is required
\end{enumerate}

\section{Summary}

\begin{itemize}
    \item Transformer achieves $R^2 = -1.17$ (catastrophic failure)
    \item Root cause: Architecture designed for sequences, not feature vectors
    \item Reducing parameters made it \textit{worse}, not better
    \item Self-attention between 1 position is meaningless
    \item Specialized time series Transformers may work but require different approach
\end{itemize}
