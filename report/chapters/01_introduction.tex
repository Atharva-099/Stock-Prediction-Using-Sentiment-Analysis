%% Chapter 1: Introduction and Literature Review
\chapter{Introduction}
\label{ch:introduction}

\section{Background and Motivation}

Financial markets have long been a subject of intense study, with researchers and practitioners alike seeking to understand and predict stock price movements. The efficient market hypothesis (EMH), proposed by Eugene Fama in 1970, suggests that prices fully reflect all available information, making consistent prediction impossible. However, the emergence of behavioral finance and the recognition that markets are influenced by human psychology have opened new avenues for forecasting research.

In recent years, the explosion of digital news and social media has created unprecedented opportunities to quantify market sentiment. Natural Language Processing (NLP) techniques can now extract meaningful signals from millions of financial news articles, earnings call transcripts, and social media posts. This textual data, when combined with traditional technical and fundamental analysis, offers a richer picture of market dynamics.

This research addresses the fundamental question: \textit{Can sentiment extracted from financial news articles improve stock price prediction accuracy?} We focus on Apple Inc. (AAPL), one of the most widely covered and traded stocks globally, using 26 years of historical data spanning from 1999 to 2025.

\section{Research Objectives}

This study pursues six primary research aims:

\begin{enumerate}[label=\textbf{Aim \arabic*:}]
    \item \textbf{Rolling Mean Quantification}: Investigate the optimal rolling window sizes (3, 7, 14, 30 days) for sentiment feature aggregation
    \item \textbf{Text Feature Extraction}: Develop higher-dimensional text features using LDA topic modeling, adjective extraction, and keyword analysis
    \item \textbf{Market Context Integration}: Incorporate related stock movements (MSFT, GOOGL, AMZN) as contextual features with appropriate lag to prevent lookahead bias
    \item \textbf{Neural Network Architectures}: Evaluate multiple deep learning architectures including LSTM, BiLSTM, GRU, CNN-LSTM, TCN, and Transformer
    \item \textbf{Reproducibility}: Ensure complete documentation and reproducibility of all experiments
    \item \textbf{Temporal Validity}: Implement walk-forward validation to ensure predictions are temporally valid
\end{enumerate}

\section{Problem Statement}

Stock price prediction remains one of the most challenging problems in financial engineering due to several inherent difficulties:

\begin{itemize}
    \item \textbf{Non-stationarity}: Stock prices exhibit changing statistical properties over time
    \item \textbf{Noise}: Financial time series contain substantial random fluctuations
    \item \textbf{Regime changes}: Market behavior varies across economic cycles
    \item \textbf{Non-linearity}: Price movements often show complex, non-linear patterns
    \item \textbf{Information asymmetry}: Not all market participants have equal access to information
\end{itemize}

Our approach addresses these challenges through a hybrid strategy that combines the robustness of traditional statistical models with the pattern recognition capabilities of deep learning.

\section{Literature Review}

\subsection{Sentiment Analysis in Finance}

The application of sentiment analysis to financial forecasting has grown substantially since the seminal work of Tetlock (2007), who demonstrated that media pessimism predicts downward pressure on market prices. Subsequent research has expanded this foundation:

\begin{itemize}
    \item \textbf{Bollen et al. (2011)} showed that Twitter mood indicators improve prediction of the Dow Jones Industrial Average
    \item \textbf{Ding et al. (2015)} introduced deep learning for event-driven stock prediction using structured representations of news
    \item \textbf{Xu and Cohen (2018)} combined technical indicators with social media sentiment using attention mechanisms
\end{itemize}

\subsection{Traditional Time Series Models}

Autoregressive Integrated Moving Average (ARIMA) models and their extensions remain fundamental to financial time series analysis:

\begin{itemize}
    \item \textbf{Box and Jenkins (1970)} established the theoretical foundation for ARIMA modeling
    \item \textbf{SARIMAX} extends ARIMA with seasonal components and exogenous variables, making it suitable for incorporating sentiment features
    \item Walk-forward validation ensures temporal validity by training only on past data
\end{itemize}

\subsection{Deep Learning for Time Series}

Recent advances in deep learning have introduced powerful architectures for sequence modeling:

\begin{itemize}
    \item \textbf{LSTM (Hochreiter \& Schmidhuber, 1997)}: Long Short-Term Memory networks address the vanishing gradient problem in RNNs
    \item \textbf{GRU (Cho et al., 2014)}: Gated Recurrent Units offer a simplified alternative to LSTM with comparable performance
    \item \textbf{TCN (Bai et al., 2018)}: Temporal Convolutional Networks use dilated causal convolutions for efficient long-range dependency modeling
    \item \textbf{Transformer (Vaswani et al., 2017)}: Self-attention mechanisms enable parallel processing of sequences
\end{itemize}

\subsection{Ensemble Methods}

Combining multiple models often improves prediction accuracy and robustness:

\begin{itemize}
    \item \textbf{Model averaging} reduces variance by combining predictions from diverse models
    \item \textbf{Stacking} uses a meta-learner to optimally weight component model predictions
    \item Our approach uses weighted ensemble: 40\% Linear + 30\% SARIMAX + 30\% TCN
\end{itemize}

\section{Contribution Summary}

This research makes several novel contributions:

\begin{enumerate}
    \item \textbf{Hybrid Strategy}: We introduce a meta-learning approach where predictions from foundational models (trained on 26-year data) serve as input features for neural networks (trained on 5-year data)
    
    \item \textbf{16th Feature Innovation}: Linear model predictions are incorporated as a 16th input feature, enabling RNNs to learn residual corrections rather than full predictions
    
    \item \textbf{Comprehensive Model Comparison}: We evaluate 9 distinct architectures under consistent experimental conditions
    
    \item \textbf{Failure Analysis}: We provide detailed analysis of why Transformer models fail for this specific task
    
    \item \textbf{Large-Scale Dataset}: We utilize 57+ million articles from HuggingFace datasets spanning 26 years
\end{enumerate}

\section{Report Organization}

The remainder of this report is organized as follows:

\begin{itemize}
    \item \textbf{Chapter 2} describes data collection from Yahoo Finance and HuggingFace
    \item \textbf{Chapter 3} details the feature engineering pipeline (55 features)
    \item \textbf{Chapter 4} presents foundational models (SARIMAX, TCN, Linear)
    \item \textbf{Chapter 5} covers neural network architectures (LSTM, BiLSTM, GRU, CNN-LSTM)
    \item \textbf{Chapter 6} analyzes Transformer failure
    \item \textbf{Chapter 7} discusses ensemble methods
    \item \textbf{Chapter 8} presents results and discussion
    \item \textbf{Chapter 9} concludes with future work
\end{itemize}
