%% Chapter 7: Ensemble Methods
\chapter{Ensemble Methods}
\label{ch:ensemble_methods}

\section{Overview}

Ensemble methods combine predictions from multiple models to achieve better performance than any single model. Our Enhanced Ensemble combines the three foundational models with optimized weights.

\begin{table}[H]
\centering
\caption{Ensemble Performance}
\label{tab:ensemble_perf}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{Weight} & \textbf{Individual R²} & \textbf{Contribution} \\
\midrule
sklearn\_Linear & 40\% & 0.9992 & Long-term trends \\
SARIMAX & 30\% & 0.9984 & Time series patterns \\
TCN & 30\% & 0.8969 & Non-linear patterns \\
\midrule
\textbf{Ensemble} & 100\% & \textbf{0.9898} & Combined strength \\
\bottomrule
\end{tabular}
\end{table}

\section{Theoretical Foundation}

\subsection{Weighted Averaging}

The ensemble prediction is a weighted average:

\begin{equation}
    \hat{y}_{\text{ensemble}} = \sum_{i=1}^{M} w_i \hat{y}_i = w_{\text{Linear}} \hat{y}_{\text{Linear}} + w_{\text{SARIMAX}} \hat{y}_{\text{SARIMAX}} + w_{\text{TCN}} \hat{y}_{\text{TCN}}
\end{equation}

where $\sum_{i=1}^{M} w_i = 1$ and $w_i \geq 0$.

\subsection{Bias-Variance Decomposition}

The expected error of an ensemble can be decomposed as:

\begin{equation}
    \mathbb{E}[(y - \hat{y}_{\text{ens}})^2] = \text{Bias}^2 + \text{Variance} + \text{Noise}
\end{equation}

Averaging diverse models reduces variance while maintaining low bias:

\begin{equation}
    \text{Var}(\bar{\hat{y}}) = \frac{1}{M^2} \sum_{i=1}^{M} \text{Var}(\hat{y}_i) + \frac{1}{M^2} \sum_{i \neq j} \text{Cov}(\hat{y}_i, \hat{y}_j)
\end{equation}

When model predictions are uncorrelated (diverse), the variance term shrinks.

\section{Model Diversity Analysis}

\subsection{Why These Three Models?}

Each model captures different aspects of the price-feature relationship:

\begin{table}[H]
\centering
\caption{Model Diversity Analysis}
\label{tab:model_diversity}
\begin{tabular}{llll}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{Strengths} & \textbf{Weaknesses} \\
\midrule
Linear & Statistical & Long-term trends & Sudden changes \\
SARIMAX & Time Series & Seasonality, cycles & Computationally slow \\
TCN & Deep Learning & Non-linear patterns & Needs large data \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Prediction Correlation}

Low correlation between model errors indicates diversity:

\begin{equation}
    \rho(\varepsilon_i, \varepsilon_j) = \frac{\text{Cov}(\varepsilon_i, \varepsilon_j)}{\sigma_{\varepsilon_i} \sigma_{\varepsilon_j}}
\end{equation}

where $\varepsilon_i = y - \hat{y}_i$ is the prediction error for model $i$.

\section{Weight Optimization}

\subsection{Weight Selection Rationale}

Weights were assigned based on individual model performance:

\begin{equation}
    w_i = \frac{R^2_i}{\sum_{j=1}^{M} R^2_j} \times \text{adjustment}
\end{equation}

\begin{itemize}
    \item \textbf{Linear (40\%)}: Highest $R^2 = 0.9992$, most reliable
    \item \textbf{SARIMAX (30\%)}: Second highest $R^2 = 0.9984$, different approach
    \item \textbf{TCN (30\%)}: Lower $R^2 = 0.8969$ but captures non-linearities
\end{itemize}

\subsection{Implementation}

\begin{lstlisting}[language=Python, caption=Ensemble Implementation]
# Weights: Linear=40%, SARIMAX=30%, TCN=30%
ensemble_pred_26y = (
    0.40 * y_pred_lr +      # Linear predictions
    0.30 * pred_sarimax +   # SARIMAX predictions  
    0.30 * pred_tcn         # TCN predictions
)

ensemble_metrics = compute_all_metrics(y_test_26y, ensemble_pred_26y)
# R² = 0.9898, RMSE = $6.66
\end{lstlisting}

\section{Ensemble Performance Analysis}

\subsection{Comparison with Components}

\begin{table}[H]
\centering
\caption{Ensemble vs Component Models}
\label{tab:ensemble_comparison}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{RMSE (\$)} & \textbf{MAPE (\%)} & \textbf{R²} \\
\midrule
sklearn\_Linear & 1.83 & 0.94 & 0.9992 \\
SARIMAX & 2.66 & 1.18 & 0.9984 \\
TCN & 21.16 & 11.04 & 0.8969 \\
\textbf{Ensemble} & \textbf{6.66} & \textbf{3.45} & \textbf{0.9898} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Why Ensemble is Slightly Lower Than Linear}

The ensemble $R^2 = 0.9898$ is lower than Linear's $R^2 = 0.9992$ because:

\begin{enumerate}
    \item \textbf{TCN drags down}: Including TCN ($R^2 = 0.8969$) reduces overall accuracy
    \item \textbf{Trade-off}: Diversity vs. raw performance
    \item \textbf{Robustness}: Ensemble is more robust to regime changes
\end{enumerate}

However, the ensemble provides benefits:
\begin{itemize}
    \item More stable predictions during market volatility
    \item Reduced risk of single-model failure
    \item Better generalization potential
\end{itemize}

\section{Complementary Error Analysis}

\subsection{When Models Disagree}

The ensemble benefits when models make complementary errors:

\begin{itemize}
    \item Linear overshoots $\rightarrow$ SARIMAX undershoots $\rightarrow$ Average closer
    \item TCN overfits $\rightarrow$ Linear stabilizes
    \item SARIMAX lags $\rightarrow$ TCN reacts faster
\end{itemize}

\subsection{Error Correlation}

\begin{equation}
    \hat{y}_{\text{ensemble}} = \frac{1}{M} \sum_{i=1}^{M} \hat{y}_i = \frac{1}{M} \sum_{i=1}^{M} (y + \varepsilon_i) = y + \frac{1}{M} \sum_{i=1}^{M} \varepsilon_i
\end{equation}

When $\sum \varepsilon_i \approx 0$ (errors cancel), ensemble prediction $\approx$ true value.

\section{Alternative Ensemble Strategies}

\subsection{Stacking (Meta-Learning)}

Instead of fixed weights, train a meta-learner:

\begin{equation}
    \hat{y}_{\text{stack}} = g(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_M)
\end{equation}

where $g$ is a learned function (e.g., another regression model).

\subsection{Boosting}

Sequential training where each model focuses on previous errors:

\begin{equation}
    F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)
\end{equation}

Not applicable here as our models are trained independently.

\subsection{Dynamic Weighting}

Adjust weights based on recent performance:

\begin{equation}
    w_i(t) = \frac{\exp(-\alpha \cdot \text{RecentError}_i)}{\sum_j \exp(-\alpha \cdot \text{RecentError}_j)}
\end{equation}

This could adapt to changing market conditions.

\section{Practical Considerations}

\subsection{Computational Cost}

\begin{table}[H]
\centering
\caption{Ensemble Computational Requirements}
\label{tab:ensemble_compute}
\begin{tabular}{lrr}
\toprule
\textbf{Model} & \textbf{Training Time} & \textbf{Inference Time} \\
\midrule
Linear & < 1 sec & < 1 ms \\
SARIMAX & ~10 min (walk-forward) & ~100 ms \\
TCN & ~2 min & < 10 ms \\
\midrule
\textbf{Ensemble Total} & ~12 min & ~110 ms \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Maintenance}

Each component model needs periodic retraining as new data arrives.

\section{Summary}

\begin{itemize}
    \item Enhanced Ensemble achieves $R^2 = 0.9898$ (exceeds target of 0.95)
    \item Weights: 40\% Linear + 30\% SARIMAX + 30\% TCN
    \item Model diversity provides robustness
    \item Slightly lower than best individual model but more stable
    \item Real-world deployment should consider dynamic weighting
\end{itemize}
