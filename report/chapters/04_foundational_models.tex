%% Chapter 4: Foundational Models
\chapter{Foundational Models}
\label{ch:foundational_models}

\section{Overview}

This chapter presents the three foundational models that form the backbone of our forecasting system: SARIMAX, Temporal Convolutional Network (TCN), and sklearn Linear Regression. These models are trained on the full 26-year dataset (1999-2025) comprising 4,579 training samples.

\begin{table}[H]
\centering
\caption{Foundational Models Summary}
\label{tab:foundational_summary}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{R²} & \textbf{RMSE (\$)} & \textbf{MAPE (\%)} \\
\midrule
sklearn\_Linear & 0.9992 & 1.83 & 0.94 \\
SARIMAX & 0.9984 & 2.66 & 1.18 \\
TCN & 0.8969 & 21.16 & 11.04 \\
\bottomrule
\end{tabular}
\end{table}

These models serve as foundational because they:
\begin{enumerate}
    \item Capture long-term price-feature relationships
    \item Provide stable, high-accuracy baseline predictions
    \item Supply the 16th feature for hybrid RNN training
    \item Form the basis for ensemble methods
\end{enumerate}

\section{SARIMAX Model}

\subsection{Mathematical Formulation}

SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous variables) extends the classical ARIMA model:

\begin{definition}[ARIMA(p,d,q)]
The ARIMA model is defined by:
\begin{equation}
    \phi(B)(1-B)^d y_t = \theta(B) \varepsilon_t
\end{equation}
where $B$ is the backshift operator, $\phi(B)$ is the AR polynomial, $\theta(B)$ is the MA polynomial, $d$ is the differencing order, and $\varepsilon_t$ is white noise.
\end{definition}

\begin{definition}[SARIMAX]
SARIMAX adds exogenous variables to ARIMA:
\begin{equation}
    y_t = c + \sum_{i=1}^{p} \phi_i y_{t-i} + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j} + \sum_{k=1}^{r} \beta_k X_{k,t} + \varepsilon_t
\end{equation}
where:
\begin{itemize}
    \item $y_t$ = stock price at time $t$
    \item $\phi_i$ = autoregressive coefficients (order $p$)
    \item $\theta_j$ = moving average coefficients (order $q$)
    \item $\beta_k$ = exogenous variable coefficients
    \item $X_{k,t}$ = exogenous variables (sentiment features)
    \item $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$ = error term
\end{itemize}
\end{definition}

\subsection{Model Configuration}

After grid search, the optimal order was determined to be $(p, d, q) = (2, 1, 1)$:

\begin{lstlisting}[language=Python, caption=SARIMAX Configuration]
from statsmodels.tsa.statespace.sarimax import SARIMAX

BEST_ORDER = (2, 1, 1)  # (AR=2, Integration=1, MA=1)

model = SARIMAX(
    history, 
    exog=np.array(history_exog).reshape(len(history_exog), -1),
    order=BEST_ORDER, 
    enforce_stationarity=False, 
    enforce_invertibility=False
)
model_fit = model.fit(disp=False, maxiter=50)
\end{lstlisting}

\subsection{Walk-Forward Validation}

To ensure temporal validity, we use walk-forward validation:

\begin{algorithm}[H]
\caption{Walk-Forward Validation for SARIMAX}
\begin{algorithmic}[1]
\State Initialize history $\leftarrow$ training data
\State Initialize predictions $\leftarrow []$
\For{each test point $t$}
    \State Fit SARIMAX on history
    \State $\hat{y}_t \leftarrow$ one-step forecast with exog$_t$
    \State Append $\hat{y}_t$ to predictions
    \State Append $(y_t, X_t)$ to history
\EndFor
\State \Return predictions
\end{algorithmic}
\end{algorithm}

\begin{lstlisting}[language=Python, caption=Walk-Forward Implementation]
history = list(train_26y)
history_exog = list(exog_train)
predictions_sarimax = []

for t in range(len(test_26y)):
    try:
        model = SARIMAX(history, 
                       exog=np.array(history_exog).reshape(-1, 1),
                       order=BEST_ORDER)
        model_fit = model.fit(disp=False, maxiter=50)
        yhat = model_fit.forecast(steps=1, 
                                  exog=exog_test[t].reshape(1, -1))[0]
    except:
        yhat = history[-1]  # Fallback to last known value
    
    predictions_sarimax.append(yhat)
    history.append(test_26y[t])
    history_exog.append(exog_test[t])
\end{lstlisting}

\subsection{Exogenous Variable Selection}

The best sentiment feature (\texttt{vader\_RM7}) was used as the exogenous variable:

\begin{equation}
    X_t = \text{vader\_RM7}(t) = \frac{1}{7} \sum_{i=t-6}^{t} \text{vader}(i)
\end{equation}

\subsection{Performance and Diagnostics}

SARIMAX achieved $R^2 = 0.9984$ with RMSE = \$2.66.

Figure \ref{fig:sarimax_diagnostics} shows the residual diagnostics for the SARIMAX model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/04_sarimax_diagnostics.png}
    \caption{SARIMAX Model Diagnostics including: (a) Actual vs Predicted plot, (b) Residual distribution with normality tests, (c) Residual autocorrelation, (d) Residuals over time, (e) Q-Q plot, and (f) Durbin-Watson test for autocorrelation.}
    \label{fig:sarimax_diagnostics}
\end{figure}

\section{Temporal Convolutional Network (TCN)}

\subsection{Architecture Overview}

TCN uses dilated causal convolutions to capture long-range dependencies efficiently:

\begin{definition}[Dilated Causal Convolution]
The dilated convolution operation is:
\begin{equation}
    F(s) = (x *_d f)(s) = \sum_{i=0}^{k-1} f(i) \cdot x_{s - d \cdot i}
\end{equation}
where:
\begin{itemize}
    \item $x$ = input sequence
    \item $f$ = filter/kernel of size $k$
    \item $d$ = dilation factor
    \item $s$ = output position
\end{itemize}
\end{definition}

The dilation factor increases exponentially at each layer: $d = 2^l$ for layer $l$.

\subsection{Receptive Field}

The receptive field of a TCN grows exponentially with depth:

\begin{equation}
    \text{Receptive Field} = 1 + 2(k-1) \sum_{l=0}^{L-1} 2^l = 1 + 2(k-1)(2^L - 1)
\end{equation}

For our configuration ($k=3$, $L=3$ layers):
\begin{equation}
    \text{RF} = 1 + 2(3-1)(2^3 - 1) = 1 + 4 \times 7 = 29 \text{ time steps}
\end{equation}

\subsection{Network Architecture}

\begin{lstlisting}[language=Python, caption=TCN Architecture]
class TemporalBlock(nn.Module):
    """Single TCN temporal block with:
    - Dilated causal convolution
    - Weight normalization
    - ReLU activation
    - Dropout
    - Residual connection
    """
    def __init__(self, n_inputs, n_outputs, kernel_size, 
                 stride, dilation, padding, dropout=0.2):
        super().__init__()
        self.conv1 = weight_norm(nn.Conv1d(
            n_inputs, n_outputs, kernel_size,
            stride=stride, padding=padding, dilation=dilation
        ))
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        
        self.conv2 = weight_norm(nn.Conv1d(
            n_outputs, n_outputs, kernel_size,
            stride=stride, padding=padding, dilation=dilation
        ))
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1)
        self.relu = nn.ReLU()

class TCNForecaster(nn.Module):
    def __init__(self, input_size, hidden_channels=[64, 128, 64],
                 kernel_size=3, dropout=0.2, output_size=1):
        super().__init__()
        self.tcn = TemporalConvNet(
            input_size, hidden_channels, kernel_size, dropout
        )
        self.linear = nn.Linear(hidden_channels[-1], output_size)
\end{lstlisting}

\subsection{Training Configuration}

\begin{table}[H]
\centering
\caption{TCN Hyperparameters}
\label{tab:tcn_hyperparams}
\begin{tabular}{lr}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Hidden Channels & [64, 128, 64] \\
Kernel Size & 3 \\
Dropout & 0.2 \\
Learning Rate & 0.001 \\
Epochs & 60 \\
Optimizer & Adam \\
Loss Function & MSE \\
Gradient Clipping & 1.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{lstlisting}[language=Python, caption=TCN Training]
tcn_model = TCNForecaster(
    input_size=len(dl_features),
    output_size=1,
    hidden_channels=[64, 128, 64],
    kernel_size=3,
    dropout=0.2
).to(device)

optimizer = torch.optim.Adam(tcn_model.parameters(), lr=0.001)
criterion = nn.MSELoss()

for epoch in range(60):
    tcn_model.train()
    optimizer.zero_grad()
    outputs = tcn_model(X_tensor)
    loss = criterion(outputs, y_tensor)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(tcn_model.parameters(), max_norm=1.0)
    optimizer.step()
\end{lstlisting}

\subsection{TCN Diagnostics}

Figure \ref{fig:tcn_diagnostics} shows the comprehensive diagnostics for TCN predictions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/05_tcn_diagnostics.png}
    \caption{TCN Model Diagnostics including prediction accuracy, residual analysis, and error distribution. TCN achieves $R^2 = 0.8969$ on the 26-year test set.}
    \label{fig:tcn_diagnostics}
\end{figure}

\subsection{Parameter Count}

The TCN model has approximately 144,000 trainable parameters:

\begin{lstlisting}[language=Python, caption=Parameter Counting]
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

total_params = count_parameters(tcn_model)
# Output: ~144,000 parameters
\end{lstlisting}

\section{sklearn Linear Regression}

\subsection{Mathematical Formulation}

Linear regression finds the optimal weights $\mathbf{w}$ that minimize the squared error:

\begin{equation}
    \hat{y} = \mathbf{X} \mathbf{w} + b = \sum_{i=1}^{p} w_i x_i + b
\end{equation}

The optimal solution is given by the normal equations:

\begin{equation}
    \mathbf{w}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
\end{equation}

\subsection{Why Linear Regression Works So Well}

Linear regression achieves $R^2 = 0.9992$ because:

\begin{enumerate}
    \item \textbf{Long-term trends}: Stock prices exhibit strong linear trends over extended periods (26 years)
    \item \textbf{Feature quality}: Our 55 engineered features capture relevant price drivers
    \item \textbf{Price rolling means}: Features like \texttt{Close\_RM7} are highly correlated with Close
    \item \textbf{Large training set}: 4,579 samples provide robust estimation
\end{enumerate}

\subsection{Implementation}

\begin{lstlisting}[language=Python, caption=Linear Regression Implementation]
from sklearn.linear_model import LinearRegression

# Training
lr = LinearRegression()
lr.fit(X_train_scaled, y_train_scaled)

# Prediction
y_pred_lr_scaled = lr.predict(X_test_scaled)
y_pred_lr = scaler_y.inverse_transform(
    y_pred_lr_scaled.reshape(-1, 1)
).flatten()

# Metrics
lr_26y_metrics = compute_all_metrics(y_test, y_pred_lr)
# R² = 0.9992, RMSE = $1.83
\end{lstlisting}

\subsection{Diagnostic Analysis}

Figure \ref{fig:linear_diagnostics} presents comprehensive diagnostics for the Linear model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/07_linear_diagnostics.png}
    \caption{sklearn\_Linear Model Diagnostics. The model achieves exceptional performance with $R^2 = 0.9992$, demonstrating that stock prices over long periods can be well-approximated by linear relationships with properly engineered features.}
    \label{fig:linear_diagnostics}
\end{figure}

\section{Why These Models are Foundational}

\subsection{Transfer Learning Perspective}

The foundational models serve a transfer learning role:

\begin{enumerate}
    \item \textbf{Knowledge capture}: They learn the fundamental price-feature relationships from 26 years of data
    \item \textbf{Transfer to RNNs}: Their predictions encode this knowledge as the 16th feature
    \item \textbf{Meta-learning}: RNNs learn to correct foundational model errors rather than predict from scratch
\end{enumerate}

\subsection{Robustness to Non-Stationarity}

Unlike RNNs, foundational models handle non-stationarity effectively:

\begin{table}[H]
\centering
\caption{Model Robustness to Non-Stationarity}
\label{tab:nonstationarity}
\begin{tabular}{lp{9cm}}
\toprule
\textbf{Model} & \textbf{Non-Stationarity Handling} \\
\midrule
Linear & Captures long-term equilibrium relationships \\
SARIMAX & Explicit differencing (d=1) removes trends \\
TCN & Dilated convolutions adapt to local patterns \\
\bottomrule
\end{tabular}
\end{table}

\section{Summary of Foundational Models}

\begin{itemize}
    \item \textbf{sklearn\_Linear}: Best single model ($R^2 = 0.9992$), provides 16th feature
    \item \textbf{SARIMAX}: Time series specialist ($R^2 = 0.9984$), uses walk-forward validation
    \item \textbf{TCN}: Deep learning baseline ($R^2 = 0.8969$), captures non-linear patterns
\end{itemize}

These three models form the foundation for:
\begin{enumerate}
    \item The weighted ensemble (40\% Linear + 30\% SARIMAX + 30\% TCN)
    \item The hybrid RNN strategy (Linear predictions as 16th feature)
\end{enumerate}
