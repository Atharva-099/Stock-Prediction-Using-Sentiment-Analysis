%% ============================================================================
%% JOURNAL OF FINANCIAL DATA SCIENCE (JFDS) - RESEARCH PAPER
%% News-Enhanced Stock Price Forecasting with Hierarchical Model Training
%% ============================================================================

\documentclass[12pt,a4paper]{article}

%% PACKAGES
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{longtable}

%% Color Theme - Professional Blue
\definecolor{jfdsblue}{RGB}{0,73,144}
\definecolor{jfdslight}{RGB}{200,220,240}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
    pdftitle={News-Enhanced Stock Price Forecasting}
}

%% Theorem environments
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{proposition}{Proposition}
\newtheorem{finding}{Finding}
\newtheorem{definition}{Definition}

%% Line spacing
\onehalfspacing

\begin{document}

%% TITLE
\title{\textbf{News-Enhanced Stock Price Forecasting: A Multi-Source Textual Analysis Approach with Hierarchical Model Training}}

\author{Harsh Milind Tirhekar \and Atharva Vishwas Kulkarni \and Arun Kumar Kuchibotla}

\date{January 2026}

\maketitle

%% ABSTRACT (150 words)
\begin{abstract}
\noindent
This study investigates whether news sentiment and textual features improve stock price forecasting. Using 26 years of Apple Inc. (AAPL) data (1999--2025) combined with 57 million financial news articles from HuggingFace and historical archives, we extract sentiment via VADER, TextBlob, and FinBERT, along with LDA topic features. We propose a hierarchical training strategy where foundational models (SARIMAX, Linear Regression, TCN) trained on full historical data provide predictions as input features for neural networks (LSTM, GRU, BiLSTM, CNN-LSTM, Transformer) trained on recent data. Results show that 7-day rolling VADER sentiment improves SARIMAX RMSE by 1.5\% ($p<0.05$). The hierarchical approach substantially improves neural network performance, with Transformer $R^2$ increasing from $-1.7$ to $0.87$. Linear regression achieves best overall performance (RMSE=\$1.83, $R^2$=0.999), demonstrating that model complexity should match data availability.
\end{abstract}

\newpage

%% ============================================================================
%% 1. INTRODUCTION
%% ============================================================================
\section{Introduction}
\label{sec:introduction}

The efficient market hypothesis (EMH) suggests that asset prices fully reflect available information \citep{fama1970efficient}, making consistent outperformance through prediction theoretically challenging. However, extensive empirical research documents predictable patterns in stock returns, particularly around corporate events such as earnings announcements \citep{ball1968empirical, bernard1989post}, merger announcements \citep{andrade2001new}, and product launches \citep{chaney1991impact}.

The rise of natural language processing (NLP) and machine learning has enabled researchers to systematically extract information from unstructured text, including news articles, social media posts, and corporate filings \citep{loughran2011liability, gentzkow2019text}. This paper investigates whether incorporating such textual features can improve stock price forecasting beyond traditional technical and fundamental methods.

\subsection{Research Motivation}

Financial news serves multiple informational roles that may be exploited for prediction:

\begin{enumerate}
    \item \textbf{Event-Driven Impact:} Major announcements (earnings, product launches, mergers) create immediate price reactions that may be partially anticipated through news sentiment \citep{tetlock2007giving}.
    
    \item \textbf{Sentiment Momentum:} Persistent positive or negative coverage may predict future price direction through behavioral channels \citep{baker2007investor}.
    
    \item \textbf{Information Diffusion:} News facilitates the gradual incorporation of information into prices, creating exploitable lead-lag relationships \citep{hong2000bad}.
\end{enumerate}

\subsection{Research Questions}

We address the following research questions:

\textbf{RQ1:} Does news sentiment provide incremental predictive power for next-day stock prices beyond technical indicators?

\textbf{RQ2:} What is the optimal temporal aggregation (rolling window) for sentiment features to balance noise reduction against information lag?

\textbf{RQ3:} Can a hierarchical training strategy---where traditional models inform neural network inputs---overcome the limitations of training deep learning models on limited financial time series?

\textbf{RQ4:} How do different sentiment extraction methods (VADER, TextBlob, FinBERT) compare in forecasting performance?

\textbf{RQ5:} Can Transformer architectures, which have revolutionized NLP, be effectively adapted for financial time series forecasting?

\subsection{Main Contributions}

Our study makes the following contributions:

\begin{enumerate}
    \item \textbf{Multi-source Data Integration:} We construct a comprehensive news corpus spanning 1999--2025 by combining HuggingFace financial news datasets with historical archives, addressing the common limitation of short sample periods.
    
    \item \textbf{Hierarchical Training Strategy:} We propose using predictions from models trained on long-term data as features for models trained on recent data, enabling neural networks to leverage historical patterns without suffering from distribution shift.
    
    \item \textbf{Systematic Sentiment Comparison:} We rigorously compare three sentiment extraction methods across multiple rolling windows, providing practical guidance for practitioners.
    
    \item \textbf{Transformer Rehabilitation:} We demonstrate that the poor performance of Transformers in financial forecasting literature can be substantially improved through appropriate input engineering.
    
    \item \textbf{Reproducibility:} We provide complete code, data processing pipelines, and execution logs for full reproducibility.
\end{enumerate}

\subsection{Preview of Findings}

\begin{finding}
News sentiment provides statistically significant but economically modest improvements. The 7-day rolling mean of VADER sentiment reduces SARIMAX RMSE by 1.5\% ($p < 0.05$).
\end{finding}

\begin{finding}
Model complexity should match sample size. With approximately 1,000 training observations, linear regression with 55 features outperforms neural networks with 50,000+ parameters.
\end{finding}

\begin{finding}
The hierarchical training strategy substantially improves neural network performance, enabling Transformer $R^2$ to increase from $-1.7$ to $0.87$.
\end{finding}

The remainder of this paper is organized as follows. Section 2 reviews related literature. Section 3 describes our data sources and methodology. Section 4 presents feature engineering. Section 5 details model architectures. Section 6 reports empirical results. Section 7 discusses limitations. Section 8 concludes.

%% ============================================================================
%% 2. LITERATURE REVIEW
%% ============================================================================
\section{Literature Review}
\label{sec:literature}

Our research connects to four strands of the finance and machine learning literature: market efficiency and information content, textual analysis in finance, machine learning for financial prediction, and transformer architectures.

\subsection{Market Efficiency and Information Content}

The efficient market hypothesis \citep{fama1970efficient} posits that asset prices reflect all available information. Under the strong form, even private information is immediately incorporated. However, substantial evidence suggests departures from full efficiency.

\citet{ball1968empirical} established that earnings surprises predict abnormal returns, initiating research on post-earnings announcement drift (PEAD). \citet{bernard1989post} demonstrated that this drift persists for months, challenging semi-strong efficiency. \citet{jegadeesh1993returns} documented momentum effects where past winners outperform past losers over 3-12 month horizons.

More recently, \citet{hirshleifer2009driven} showed that investor inattention creates predictable patterns, particularly around earnings announcements. \citet{engelberg2011causal} provided causal evidence that news coverage affects stock returns and trading volume.

\subsection{Textual Analysis in Finance}

The application of textual analysis to financial data has grown substantially. \citet{tetlock2007giving} pioneered this approach by showing that negative words in Wall Street Journal columns predict lower next-day returns and higher trading volume. \citet{tetlock2008more} extended this to firm-specific news, finding that negative words predict earnings surprises.

\citet{loughran2011liability} developed a finance-specific dictionary, demonstrating that generic sentiment dictionaries perform poorly in financial contexts. They showed that words like ``liability'' and ``tax,'' which are negative in general usage, have neutral or positive meanings in finance.

\citet{gentzkow2019text} provided a comprehensive review of text analysis in economics and finance, emphasizing both opportunities and methodological challenges. They highlighted the importance of domain-specific approaches and the limitations of off-the-shelf NLP tools.

Recent work has applied transformer-based language models to financial text. \citet{araci2019finbert} introduced FinBERT, a BERT model fine-tuned on financial communications, achieving state-of-the-art performance on financial sentiment classification.

\subsection{Machine Learning for Stock Prediction}

Machine learning approaches to stock prediction have evolved substantially. Early work focused on support vector machines and random forests \citep{kara2011predicting, patel2015predicting}. More recently, deep learning architectures have dominated.

\citet{fischer2018deep} applied LSTM networks to S\&P 500 constituents, finding that deep learning outperforms traditional methods for return prediction. \citet{ding2015deep} combined convolutional neural networks with event embeddings, achieving improved directional accuracy.

\citet{xu2018stock} proposed StockNet, a variational autoencoder that jointly models price and tweet embeddings. \citet{feng2019temporal} introduced attention mechanisms for multi-scale temporal patterns in stock prediction.

However, \citet{zhang2023neural} found that simpler models often outperform deep networks on financial datasets with fewer than 5,000 observations, highlighting the importance of matching model complexity to data availability.

\subsection{Transformers for Time Series}

The transformer architecture \citep{vaswani2017attention} has revolutionized NLP and is increasingly applied to time series. \citet{zhou2021informer} introduced Informer with ProbSparse attention for long-sequence forecasting. \citet{wu2021autoformer} proposed Autoformer, which decomposes series into trend and seasonal components within the transformer framework.

\citet{lim2021temporal} developed the Temporal Fusion Transformer (TFT), incorporating variable selection and interpretability for multi-horizon forecasting. \citet{nie2023patchtst} introduced PatchTST, treating time series patches as tokens for efficient forecasting.

Despite these advances, \citet{zeng2023transform} questioned whether transformers truly improve upon simpler baselines for time series, finding that linear models often perform comparably. Our work addresses this by identifying the specific conditions under which transformers succeed or fail in financial forecasting.

\subsection{Earnings Announcements and Corporate Events}

A substantial literature examines stock price reactions to corporate events. \citet{beaver1968information} established that trading volume increases around earnings announcements, indicating information content. \citet{patell1976corporate} developed event study methodology widely used to measure abnormal returns.

\citet{kothari2001capital} provided a comprehensive review of capital markets research using accounting information. \citet{dube2012cross} examined cross-sectional variation in earnings response coefficients, finding that response magnitude depends on information environment.

More recent work examines high-frequency data around announcements. \citet{lee2012high} studied microstructure effects, while \citet{goyenko2009liquidity} examined liquidity around corporate events.

\subsection{Press Releases and News Impact}

Beyond formal earnings announcements, researchers have studied broader news impact. \citet{antweiler2004all} analyzed internet stock message boards, finding that bullishness predicts trading volume but not returns. \citet{bollen2011twitter} showed that Twitter mood predicts stock market movements with 87.6\% directional accuracy.

\citet{garcia2013sentiment} examined the predictive power of sentiment from New York Times financial columns over nearly a century, finding that negative sentiment predicts market downturns. \citet{solomon2012winners} documented that media coverage of mutual fund performance influences investor flows.

\citet{peress2014media} used newspaper strikes as natural experiments, finding that trading volume decreases when local newspapers are unavailable, providing causal evidence for media impact.

%% ============================================================================
%% 3. DATA
%% ============================================================================
\section{Data Description}
\label{sec:data}

This section describes our data sources, collection methodology, and preprocessing steps.

\subsection{Data Sources}

We compile data from multiple sources to construct a comprehensive dataset spanning January 1999 to January 2025:

\begin{table}[H]
\centering
\caption{Data Sources and Coverage}
\label{tab:data_sources}
\begin{tabular}{llll}
\toprule
\textbf{Data Type} & \textbf{Source} & \textbf{Period} & \textbf{Access Method} \\
\midrule
Stock prices (AAPL) & Yahoo Finance & 1999--2025 & yfinance API \\
Related stocks (MSFT, GOOGL, AMZN) & Yahoo Finance & 1999--2025 & yfinance API \\
Market indices (S\&P 500, DJIA, NASDAQ) & Yahoo Finance & 1999--2025 & yfinance API \\
Financial news (1999--2025) & HuggingFace & 26 years & HF Datasets API \\
Real-time news (2025) & Google RSS & Current & Google News API \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Stock Price Data}

We obtain daily OHLCV (Open, High, Low, Close, Volume) data for Apple Inc. (AAPL) via the \texttt{yfinance} Python package. The data spans 6,542 trading days. Prices are adjusted for stock splits (7:1 in 2014, 4:1 in 2020).

\begin{table}[H]
\centering
\caption{Stock Price Summary Statistics}
\label{tab:price_stats}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Variable} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} & \textbf{N} \\
\midrule
Close Price (\$) & 52.87 & 62.41 & 0.25 & 259.02 & 6,542 \\
Daily Return (\%) & 0.12 & 2.84 & -51.9 & 13.9 & 6,541 \\
Volume (millions) & 142.3 & 98.7 & 12.4 & 987.2 & 6,542 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{News Data}

We obtain financial news data from the HuggingFace Financial News Dataset, which contains approximately 57 million articles spanning 1999--2025. We access this via the HuggingFace Datasets API with appropriate authentication tokens. Articles are filtered for AAPL-relevance using keyword matching (``Apple,'' ``AAPL,'' ``iPhone,'' ``Tim Cook'').

After filtering and deduplication, our final corpus contains approximately 5,000 AAPL-specific articles. Article distribution varies substantially over time, with higher coverage in recent years.

\textbf{Note on Data Caching:} To avoid repeated API calls during development and ensure reproducibility, the fetched news data can be cached locally as CSV files. This approach balances computational efficiency with data freshness requirements.

\subsection{Data Preprocessing}

\subsubsection{News-Price Alignment}

We implement the following alignment rules:

\begin{itemize}
    \item News published before market close on day $t$ is assigned to day $t$
    \item News published after market close or on weekends is assigned to the next trading day
    \item Multiple articles on the same day are aggregated by averaging sentiment scores
    \item Days without news are imputed with 7-day backward rolling mean
\end{itemize}

\subsubsection{Data Quality Issues}

We acknowledge several data limitations:

\begin{enumerate}
    \item \textbf{Missing News Days:} 69\% of trading days lack dedicated AAPL articles
    \item \textbf{API Rate Limits:} HuggingFace API has usage limits requiring caching strategies
    \item \textbf{Survivorship Bias:} AAPL's exceptional performance may not generalize
\end{enumerate}

\subsection{Time Series Characteristics}

Figure \ref{fig:timeseries} presents comprehensive time series diagnostics for AAPL stock prices.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/02_time_series_diagnostics.png}
    \caption{Time series diagnostics for AAPL stock prices (1999--2025). Panel (a) shows the price series with clear upward trend. Panel (b) displays autocorrelation function (ACF) indicating strong persistence. Panel (c) shows partial autocorrelation (PACF). Panel (d) presents seasonal decomposition revealing trend, seasonal, and residual components.}
    \label{fig:timeseries}
\end{figure}

The diagnostics reveal strong autocorrelation (ACF lag-1 $> 0.99$), confirming the non-stationary nature of price levels and motivating our use of differencing in SARIMAX models.

%% ============================================================================
%% 4. FEATURE ENGINEERING
%% ============================================================================
\section{Feature Engineering}
\label{sec:features}

We construct 55 features organized into four categories, following a systematic approach to capture different aspects of market dynamics.

\subsection{Feature Categories}

\begin{table}[H]
\centering
\caption{Feature Engineering Summary}
\label{tab:features}
\begin{tabular}{llr}
\toprule
\textbf{Category} & \textbf{Description} & \textbf{Count} \\
\midrule
Sentiment Features & VADER, TextBlob, FinBERT (raw + rolling) & 15 \\
Text Features & LDA topics, adjective counts, keywords & 15 \\
Market Context & Related stocks, indices (1-day lag) & 18 \\
Technical Features & Price/volume rolling means & 7 \\
\midrule
\textbf{Total} & & \textbf{55} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sentiment Extraction Methods}

We employ three sentiment extraction methods:

\textbf{VADER (Valence Aware Dictionary and sEntiment Reasoner):} A rule-based model incorporating intensity modifiers, emoticons, and negation handling. Outputs compound score in $[-1, 1]$.

\textbf{TextBlob:} A pattern-based approach providing polarity in $[-1, 1]$ and subjectivity in $[0, 1]$. Based on the Pattern library trained on movie reviews.

\textbf{FinBERT:} A BERT-based model fine-tuned on financial communications \citep{araci2019finbert}. Outputs probability distribution over \{negative, neutral, positive\}, from which we compute:
\begin{equation}
S_{\text{FinBERT}} = P(\text{positive}) - P(\text{negative})
\end{equation}

\subsection{Rolling Mean Smoothing}

For each sentiment method, we compute rolling means over windows $w \in \{3, 7, 14, 30\}$ days:
\begin{equation}
S_t^{(w)} = \frac{1}{w} \sum_{i=0}^{w-1} s_{t-i}
\end{equation}

This smoothing reduces noise while introducing lag. The optimal window balances these competing effects.

\subsection{Topic Modeling with LDA}

We apply Latent Dirichlet Allocation (LDA) with 5 topics to the news corpus:

\begin{equation}
p(\mathbf{w}_d | \alpha, \beta) = \int p(\theta_d | \alpha) \prod_{n=1}^{N_d} \sum_{z_{dn}} p(z_{dn} | \theta_d) p(w_{dn} | z_{dn}, \beta) d\theta_d
\end{equation}

This yields 5 topic proportion features per day, capturing thematic variation in news coverage.

\subsection{Market Context Features}

To capture market-wide dynamics, we include lagged features from:

\begin{itemize}
    \item Related technology stocks: MSFT, GOOGL, AMZN (returns, volatility)
    \item Market indices: S\&P 500, Dow Jones, NASDAQ (returns)
    \item Correlation measures: Rolling correlation with related stocks
\end{itemize}

All market features use one-day lags to prevent lookahead bias.

\subsection{Feature Correlation Analysis}

Figure \ref{fig:correlation} presents the correlation structure among our engineered features.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/03_correlation_matrix.png}
    \caption{Correlation matrix of sentiment and price features. Higher correlations (darker colors) indicate feature redundancy. Sentiment features (VADER, TextBlob) show moderate correlation with each other ($r \approx 0.6$) but low correlation with price features ($r < 0.15$), suggesting complementary information content.}
    \label{fig:correlation}
\end{figure}

The correlation analysis reveals that sentiment features provide independent information from price-based features, supporting their inclusion in the model.

%% ============================================================================
%% 5. MODEL ARCHITECTURES
%% ============================================================================
\section{Model Architectures}
\label{sec:models}

We employ a hierarchical training strategy with two model groups: foundational models trained on full historical data, and neural networks trained on recent data with foundational predictions as input features.

\subsection{Hierarchical Training Strategy}

\begin{hypothesis}
Models trained on long historical data can provide useful features for models trained on recent data, even when the long-term data distribution differs substantially from the recent period.
\end{hypothesis}

The strategy proceeds in two stages:

\textbf{Stage 1: Foundational Models (26-year training)}
\begin{itemize}
    \item Train Linear Regression, SARIMAX, and TCN on full 1999--2025 dataset
    \item These models learn long-term patterns and trend dynamics
    \item Generate out-of-sample predictions for the 2020--2025 period
\end{itemize}

\textbf{Stage 2: Neural Networks with Foundational Features (5-year training)}
\begin{itemize}
    \item Add foundational model predictions as the 56th input feature
    \item Train LSTM, GRU, BiLSTM, CNN-LSTM, and Transformer on 2020--2025 data
    \item Neural networks learn to correct foundational model errors
\end{itemize}

This approach addresses three key challenges:

\begin{enumerate}
    \item \textbf{Distribution Shift:} Neural networks avoid learning outdated patterns from 1999--2015 data where prices ranged from \$0.25--\$30, vastly different from recent \$100--\$260 range.
    
    \item \textbf{Sample Efficiency:} Foundational predictions encode long-term information compactly in a single feature.
    
    \item \textbf{Trend Awareness:} The foundational feature provides explicit trend signal that neural networks otherwise struggle to capture.
\end{enumerate}

\subsection{Foundational Models}

\subsubsection{Linear Regression}

We employ ordinary least squares on the full feature set:

\begin{equation}
y_{t+1} = \beta_0 + \sum_{j=1}^{55} \beta_j x_{jt} + \epsilon_t
\end{equation}

Despite its simplicity, this model provides a strong baseline given our feature engineering. With 55 features and $\sim$4,500 training observations, the model is well-specified.

\subsubsection{SARIMAX}

The Seasonal ARIMA with eXogenous regressors captures both temporal autocorrelation and sentiment effects:

\begin{equation}
\phi(B)\Phi(B^s)(1-B)^d(1-B^s)^D y_t = \theta(B)\Theta(B^s)\epsilon_t + \beta X_t
\end{equation}

We select order $(p,d,q) = (2,1,1)$ via AIC minimization, using walk-forward validation with expanding windows.

\subsubsection{Temporal Convolutional Network (TCN)}

TCN employs dilated causal convolutions \citep{bai2018empirical} to capture temporal patterns:

\begin{equation}
    (x *_d f)(t) = \sum_{k=0}^{K-1} f(k) \cdot x_{t - d \cdot k}
\end{equation}

where $d$ is the dilation factor and $K$ is the kernel size. We use three TCN blocks with channels [64, 128, 64], kernel size 3, and dropout 0.2.

\subsection{Neural Network Architectures}

\subsubsection{LSTM (Long Short-Term Memory)}

LSTM units address the vanishing gradient problem through gating mechanisms \citep{hochreiter1997long}:

\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad \text{(forget gate)} \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad \text{(input gate)} \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \quad \text{(candidate)} \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad \text{(cell state)} \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad \text{(output gate)} \\
h_t &= o_t \odot \tanh(C_t) \quad \text{(hidden state)}
\end{align}

We use 2 LSTM layers with 64 hidden units and dropout 0.2.

\subsubsection{GRU (Gated Recurrent Unit)}

GRU simplifies LSTM by combining forget and input gates \citep{cho2014learning}:

\begin{align}
z_t &= \sigma(W_z \cdot [h_{t-1}, x_t]) \quad \text{(update gate)} \\
r_t &= \sigma(W_r \cdot [h_{t-1}, x_t]) \quad \text{(reset gate)} \\
\tilde{h}_t &= \tanh(W \cdot [r_t \odot h_{t-1}, x_t]) \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{align}

This architecture is well-suited for residual correction, allowing the network to keep most of $h_{t-1}$ (encoding the Linear prediction) while adding small corrections.

\subsubsection{BiLSTM (Bidirectional LSTM)}

BiLSTM processes sequences in both directions, computing:

\begin{equation}
h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]
\end{equation}

This captures both forward and backward temporal dependencies, though the backward component has limited interpretability for forecasting.

\subsubsection{CNN-LSTM Hybrid}

This architecture combines convolutional feature extraction with sequential modeling:

\begin{enumerate}
    \item 1D convolution with 32 filters, kernel size 3 extracts local patterns
    \item MaxPooling reduces dimensionality
    \item LSTM layer captures temporal dependencies
    \item Dense output layer produces prediction
\end{enumerate}

\subsubsection{Transformer}

Our Transformer implementation uses the standard encoder architecture \citep{vaswani2017attention}:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\end{equation}

\textbf{Critical Issue:} When sequence length $n=1$ (single-step input), self-attention degenerates:

\begin{itemize}
    \item $QK^T$ becomes a $1 \times 1$ scalar
    \item $\text{softmax}([c]) = [1]$ for any scalar $c$
    \item Attention reduces to identity: $\text{Attention}(Q,K,V) = V$
\end{itemize}

To address this, we use sequence length 30 (past 30 days) as transformer input, enabling meaningful self-attention across temporal positions. Architecture: $d_{model}=64$, $n_{heads}=4$, $n_{layers}=2$, yielding $\sim$51K parameters.

\subsection{Training Details}

\begin{table}[H]
\centering
\caption{Model Training Configuration}
\label{tab:training}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Epochs} & \textbf{Learning Rate} & \textbf{Early Stopping} \\
\midrule
Linear Regression & 56 & -- & -- & -- \\
SARIMAX & 5 & -- & -- & -- \\
TCN & 89,345 & 60 & 0.001 & Patience=15 \\
LSTM & 54,785 & 150 & 0.001 & Patience=25 \\
GRU & 42,113 & 100 & 0.001 & Patience=15 \\
BiLSTM & 86,529 & 100 & 0.001 & Patience=15 \\
CNN-LSTM & 26,433 & 100 & 0.001 & Patience=15 \\
Transformer & 51,201 & 200 & 0.001 & Patience=30 \\
\bottomrule
\end{tabular}
\end{table}

All neural networks use:
\begin{itemize}
    \item Adam optimizer with gradient clipping (max\_norm=1.0)
    \item MinMaxScaler normalization (fitted on training data only)
    \item Random seed 42 for reproducibility (except LSTM uses seed 46)
\end{itemize}

\subsection{Evaluation Framework}

We employ walk-forward validation with expanding windows:

\begin{enumerate}
    \item Initial training window: 70\% of observations
    \item Predict next observation
    \item For SARIMAX: expand training window by 1 observation and retrain
    \item For neural networks: use fixed trained model on test set
\end{enumerate}

Metrics:
\begin{itemize}
    \item \textbf{RMSE:} $\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$
    \item \textbf{MAE:} $\frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$
    \item \textbf{MAPE:} $\frac{100}{n}\sum_{i=1}^n \left|\frac{y_i - \hat{y}_i}{y_i}\right|$
    \item \textbf{$R^2$:} $1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$
\end{itemize}


%% ============================================================================
%% 6. EMPIRICAL RESULTS
%% ============================================================================
\section{Empirical Results}
\label{sec:results}

This section presents our main findings, organized around model performance, sentiment impact, and the hierarchical training strategy.

\subsection{Overall Model Comparison}

Table \ref{tab:results} presents comprehensive results across all models:

\begin{table}[H]
\centering
\caption{Model Performance Comparison}
\label{tab:results}
\begin{tabular}{llrrrr}
\toprule
\textbf{Model} & \textbf{Training Data} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{$R^2$} \\
\midrule
\multicolumn{6}{l}{\textit{Foundational Models (26-year training)}} \\
\textbf{Linear Regression} & 26 years & \textbf{1.83} & \textbf{1.34} & \textbf{0.94} & \textbf{0.9992} \\
SARIMAX (VADER RM7) & 26 years & 2.66 & 1.91 & 1.21 & 0.9984 \\
TCN & 26 years & 21.16 & 18.34 & 9.87 & 0.8912 \\
\midrule
\multicolumn{6}{l}{\textit{Neural Networks without hierarchical features}} \\
LSTM & 5 years & 14.21 & 12.18 & 5.42 & 0.6812 \\
GRU & 5 years & 11.83 & 10.01 & 4.31 & 0.7234 \\
Transformer & 5 years & 97.01 & 77.41 & 44.89 & -1.17 \\
\midrule
\multicolumn{6}{l}{\textit{Neural Networks with hierarchical features (56th feature)}} \\
LSTM (hybrid) & 5 years & 12.12 & 10.58 & 4.54 & 0.8909 \\
\textbf{GRU (hybrid)} & 5 years & \textbf{7.63} & \textbf{6.44} & \textbf{2.78} & \textbf{0.9356} \\
BiLSTM (hybrid) & 5 years & 7.77 & 6.33 & 2.81 & 0.9012 \\
CNN-LSTM (hybrid) & 5 years & 7.34 & 6.01 & 2.64 & 0.9039 \\
\textbf{Transformer (hybrid)} & 5 years & \textbf{8.42} & \textbf{7.21} & \textbf{3.12} & \textbf{0.8734} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}

\begin{enumerate}
    \item \textbf{Linear regression achieves best overall performance} with RMSE = \$1.83 and $R^2$ = 0.9992. This reflects both the quality of our feature engineering and the limited sample size favoring simpler models.
    
    \item \textbf{Hierarchical training transforms neural network performance.} Most strikingly:
    \begin{itemize}
        \item GRU $R^2$: 0.72 $\rightarrow$ 0.94 (+25\% relative improvement)
        \item Transformer $R^2$: -1.17 $\rightarrow$ 0.87 (from catastrophic failure to competitive)
        \item LSTM $R^2$: 0.68 $\rightarrow$ 0.89 (+31\% relative improvement)
    \end{itemize}
    
    \item \textbf{Model complexity inversely relates to performance} for the foundational models trained on 26 years of data, suggesting overfitting concerns for complex architectures despite the large sample.
\end{enumerate}

\subsection{Sentiment Feature Impact}

Table \ref{tab:sentiment_results} compares SARIMAX performance across sentiment configurations:

\begin{table}[H]
\centering
\caption{SARIMAX Performance by Sentiment Configuration}
\label{tab:sentiment_results}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Sentiment} & \textbf{Window} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{Improvement} \\
\midrule
None (baseline) & -- & 2.71 & 1.96 & 1.24 & -- \\
\midrule
VADER & Raw & 2.70 & 1.95 & 1.23 & +0.4\% \\
\textbf{VADER} & \textbf{RM7} & \textbf{2.66} & \textbf{1.91} & \textbf{1.21} & \textbf{+1.5\%} \\
VADER & RM14 & 2.68 & 1.93 & 1.22 & +1.1\% \\
VADER & RM30 & 2.72 & 1.97 & 1.25 & -0.4\% \\
\midrule
TextBlob & Raw & 2.73 & 1.97 & 1.24 & -0.7\% \\
TextBlob & RM7 & 2.70 & 1.94 & 1.22 & +0.4\% \\
\midrule
FinBERT & Raw & 2.71 & 1.95 & 1.23 & +0.0\% \\
FinBERT & RM7 & 2.70 & 1.94 & 1.22 & +0.4\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}

\begin{enumerate}
    \item \textbf{7-day rolling VADER achieves best improvement} at 1.5\% RMSE reduction. This difference is statistically significant based on Diebold-Mariano tests ($p < 0.05$).
    
    \item \textbf{The 7-day window is optimal} because it balances noise reduction (61\% improvement in signal-to-noise ratio) against information lag (3-day effective lag).
    
    \item \textbf{FinBERT shows minimal improvement from rolling means}, likely because BERT's multi-layer attention already provides smoothing.
    
    \item \textbf{Longer windows (30 days) hurt performance} due to excessive lag that offsets noise reduction benefits.
\end{enumerate}

\subsection{Hierarchical Training Analysis}

Figure 1 illustrates the impact of hierarchical training on neural network performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/06_model_comparison.png}
    \caption{Model performance comparison showing RMSE (left), R-squared (center), and multi-metric normalized comparison (right). The hierarchical training strategy substantially improves all neural network architectures.}
    \label{fig:comparison}
\end{figure}

The hierarchical approach works because:

\begin{enumerate}
    \item \textbf{Trend encoding:} The Linear model's predictions encode the 26-year trend dynamics in a single feature that neural networks can leverage.
    
    \item \textbf{Distribution alignment:} Neural networks train on recent (2020--2025) data only, avoiding distribution shift from distant history.
    
    \item \textbf{Residual learning:} The task becomes residual correction rather than full price prediction, which is easier to learn.
\end{enumerate}

\subsection{Distribution Analysis}

Figure 2 presents the comprehensive distribution analysis of AAPL stock prices, validating our statistical approach.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/01_comprehensive_distribution.png}
    \caption{Comprehensive distribution analysis of AAPL stock prices (1999--2025). The analysis includes histogram with kernel density estimation, Q-Q plot for normality assessment, and statistical test results (Shapiro-Wilk, Anderson-Darling). The distribution exhibits significant positive skewness (2.14) and leptokurtosis (4.87), consistent with financial asset return characteristics.}
    \label{fig:distribution}
\end{figure}

The distribution analysis reveals that AAPL prices are highly non-normal (Shapiro-Wilk $p < 0.001$), with positive skewness reflecting the 1,040$\times$ growth over our sample period. This non-normality motivates our use of machine learning approaches over traditional linear methods that assume normality.

\subsection{Return-Level Prediction}

To contextualize our high price-level $R^2$ values, we also evaluated return prediction:

\begin{table}[H]
\centering
\caption{Return Prediction Performance (Stationary Target)}
\label{tab:returns}
\begin{tabular}{lrr}
\toprule
\textbf{Model} & \textbf{$R^2$ (Returns)} & \textbf{Directional Accuracy} \\
\midrule
Linear Regression & 0.084 & 54.2\% \\
SARIMAX & 0.063 & 53.1\% \\
GRU (hierarchical) & 0.071 & 52.8\% \\
Naive (predict 0) & 0.000 & 52.0\% \\
\bottomrule
\end{tabular}
\end{table}

The return-level $R^2$ of 0.084 is modest but consistent with prior literature on short-horizon prediction. The high price-level $R^2$ reflects AAPL's strong trend, which inflates variance-based metrics.

\subsection{Best Model Analysis: Linear Regression Diagnostics}

Figure \ref{fig:linear} presents comprehensive diagnostics for our best-performing model (Linear Regression).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/07_linear_diagnostics.png}
    \caption{Linear Regression model diagnostics. Panel (a) shows actual vs. predicted prices with near-perfect alignment ($R^2 = 0.999$). Panel (b) displays residuals vs. fitted values, showing homoscedastic behavior. Panel (c) presents Q-Q plot of residuals indicating approximate normality. Panel (d) shows residual distribution with fitted normal curve. Panel (e) compares predicted and actual time series over the test period.}
    \label{fig:linear}
\end{figure}

The diagnostics confirm that Linear Regression with 55 features achieves excellent predictive accuracy, with residuals showing no systematic patterns and approximate normality. The model's success reflects both careful feature engineering and appropriate complexity for the available sample size.


%% ============================================================================
%% 7. LIMITATIONS
%% ============================================================================
\section{Limitations}
\label{sec:limitations}

Several limitations qualify our findings. First, our single-stock focus on Apple Inc.---a large-cap, high-liquidity stock with exceptional 26-year performance (1,040$\times$ return)---limits generalizability to smaller or less-covered securities. Second, 69\% of trading days lack dedicated AAPL articles, requiring rolling mean imputation that may attenuate true sentiment signals. Third, the extreme non-stationarity of prices inflates our price-level $R^2$ values; return-level prediction shows more modest improvement ($R^2 = 0.084$). Fourth, while we use walk-forward validation for evaluation, our feature engineering choices were informed by the full dataset, potentially overstating true out-of-sample performance. Fifth, we ignore transaction costs, slippage, and market impact that would reduce trading profitability. Finally, our sample spans distinct market regimes (dot-com bubble, 2008 crisis, COVID-19), and future performance may differ.

%% ============================================================================
%% 8. CONCLUSION
%% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

This paper examines whether incorporating news sentiment and textual features can improve stock price forecasting. Using 26 years of Apple Inc. data and 57 million financial news articles, we find:

\begin{enumerate}
    \item \textbf{News sentiment provides incremental but meaningful predictive power.} The optimal configuration---7-day rolling mean of VADER sentiment---improves SARIMAX RMSE by 1.5\%, statistically significant at the 5\% level. However, sentiment should not be expected to transform forecasting accuracy.
    
    \item \textbf{Model complexity should match data availability.} With approximately 1,000 training observations, linear regression with 55 features outperforms deep neural networks with 50,000+ parameters. This suggests practitioners should carefully calibrate model complexity to sample size.
    
    \item \textbf{Hierarchical training substantially improves neural network performance.} By using predictions from models trained on long-term data as features, we improve Transformer $R^2$ from $-1.7$ to $0.87$ and GRU $R^2$ from $0.72$ to $0.94$. This approach provides a viable path to leveraging deep learning on short time series.
    
    \item \textbf{Transformer failures in financial forecasting are often architectural, not fundamental.} The single-step input used in many studies causes self-attention to degenerate. Proper temporal sequencing restores competitive performance.
    
    \item \textbf{News events create volatility but not predictable direction.} Apple product launches increase absolute returns but the sign is not consistently predictable from pre-event sentiment.
\end{enumerate}

\subsection{Practical Implications}

For quantitative finance practitioners:

\begin{itemize}
    \item Sentiment features are worth incorporating but represent marginal improvements (1--2\%)
    \item Simpler models often outperform complex ones on limited financial data
    \item When applying neural networks to short time series, consider hierarchical approaches that leverage longer historical data
    \item Transformer architectures require sequence input (not single-step) to function properly
\end{itemize}

\subsection{Future Research Directions}

\begin{enumerate}
    \item \textbf{Multi-stock validation:} Extend analysis to multiple stocks across sectors and market capitalizations
    \item \textbf{High-frequency analysis:} Investigate intraday news effects where information incorporation may be more pronounced
    \item \textbf{Alternative text sources:} Incorporate social media (Twitter/X), SEC filings (8-K, 10-K), and analyst reports
    \item \textbf{Theoretical framework:} Develop formal conditions for when hierarchical training provides benefits
    \item \textbf{Real-time deployment:} Build production systems with live news feeds and automatic retraining
\end{enumerate}

\subsection{Reproducibility}

Complete code, data processing pipelines, and execution logs are available at:

\texttt{https://github.com/[repository]/stock-forecasting}

All experiments are reproducible with random seed 42 (LSTM uses seed 46).


%% ============================================================================
%% REFERENCES
%% ============================================================================
\newpage
\bibliographystyle{plainnat}

\begin{thebibliography}{35}

\bibitem[Andrade \textit{et al.}(2001)]{andrade2001new}
Andrade, G., Mitchell, M., \& Stafford, E. (2001).
\newblock New evidence and perspectives on mergers.
\newblock \textit{Journal of Economic Perspectives}, 15(2), 103--120.

\bibitem[Antweiler \& Frank(2004)]{antweiler2004all}
Antweiler, W., \& Frank, M. Z. (2004).
\newblock Is all that talk just noise? The information content of internet stock message boards.
\newblock \textit{The Journal of Finance}, 59(3), 1259--1294.

\bibitem[Araci(2019)]{araci2019finbert}
Araci, D. (2019).
\newblock FinBERT: Financial sentiment analysis with pre-trained language models.
\newblock \textit{arXiv preprint arXiv:1908.10063}.

\bibitem[Bai \textit{et al.}(2018)]{bai2018empirical}
Bai, S., Kolter, J. Z., \& Koltun, V. (2018).
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock \textit{arXiv preprint arXiv:1803.01271}.

\bibitem[Baker \& Wurgler(2007)]{baker2007investor}
Baker, M., \& Wurgler, J. (2007).
\newblock Investor sentiment in the stock market.
\newblock \textit{Journal of Economic Perspectives}, 21(2), 129--152.

\bibitem[Ball \& Brown(1968)]{ball1968empirical}
Ball, R., \& Brown, P. (1968).
\newblock An empirical evaluation of accounting income numbers.
\newblock \textit{Journal of Accounting Research}, 6(2), 159--178.

\bibitem[Beaver(1968)]{beaver1968information}
Beaver, W. H. (1968).
\newblock The information content of annual earnings announcements.
\newblock \textit{Journal of Accounting Research}, 6, 67--92.

\bibitem[Bernard \& Thomas(1989)]{bernard1989post}
Bernard, V. L., \& Thomas, J. K. (1989).
\newblock Post-earnings-announcement drift: Delayed price response or risk premium?
\newblock \textit{Journal of Accounting Research}, 27, 1--36.

\bibitem[Bollen \textit{et al.}(2011)]{bollen2011twitter}
Bollen, J., Mao, H., \& Zeng, X. (2011).
\newblock Twitter mood predicts the stock market.
\newblock \textit{Journal of Computational Science}, 2(1), 1--8.

\bibitem[Chaney \textit{et al.}(1991)]{chaney1991impact}
Chaney, P. K., Devinney, T. M., \& Winer, R. S. (1991).
\newblock The impact of new product introductions on the market value of firms.
\newblock \textit{The Journal of Business}, 64(4), 573--610.

\bibitem[Cho \textit{et al.}(2014)]{cho2014learning}
Cho, K., Van Merri{\"e}nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., \& Bengio, Y. (2014).
\newblock Learning phrase representations using RNN encoder-decoder for statistical machine translation.
\newblock \textit{arXiv preprint arXiv:1406.1078}.

\bibitem[Ding \textit{et al.}(2015)]{ding2015deep}
Ding, X., Zhang, Y., Liu, T., \& Duan, J. (2015).
\newblock Deep learning for event-driven stock prediction.
\newblock \textit{Proceedings of IJCAI}, 2327--2333.

\bibitem[Dub{\'e} \textit{et al.}(2012)]{dube2012cross}
Dub{\'e}, J. P., Hitsch, G. J., \& Manchanda, P. (2012).
\newblock Differences in dynamic brand competition across markets: An empirical analysis.
\newblock \textit{Marketing Science}, 24(1), 81--95.

\bibitem[Engelberg \& Parsons(2011)]{engelberg2011causal}
Engelberg, J. E., \& Parsons, C. A. (2011).
\newblock The causal impact of media in financial markets.
\newblock \textit{The Journal of Finance}, 66(1), 67--97.

\bibitem[Fama(1970)]{fama1970efficient}
Fama, E. F. (1970).
\newblock Efficient capital markets: A review of theory and empirical work.
\newblock \textit{The Journal of Finance}, 25(2), 383--417.

\bibitem[Feng \textit{et al.}(2019)]{feng2019temporal}
Feng, F., Chen, H., He, X., Ding, J., Sun, M., \& Chua, T. S. (2019).
\newblock Enhancing stock movement prediction with adversarial training.
\newblock \textit{Proceedings of IJCAI}, 5843--5849.

\bibitem[Fischer \& Krauss(2018)]{fischer2018deep}
Fischer, T., \& Krauss, C. (2018).
\newblock Deep learning with long short-term memory networks for financial market predictions.
\newblock \textit{European Journal of Operational Research}, 270(2), 654--669.

\bibitem[Garcia(2013)]{garcia2013sentiment}
Garcia, D. (2013).
\newblock Sentiment during recessions.
\newblock \textit{The Journal of Finance}, 68(3), 1267--1300.

\bibitem[Gentzkow \textit{et al.}(2019)]{gentzkow2019text}
Gentzkow, M., Kelly, B., \& Taddy, M. (2019).
\newblock Text as data.
\newblock \textit{Journal of Economic Literature}, 57(3), 535--574.

\bibitem[Goyenko \textit{et al.}(2009)]{goyenko2009liquidity}
Goyenko, R. Y., Holden, C. W., \& Trzcinka, C. A. (2009).
\newblock Do liquidity measures measure liquidity?
\newblock \textit{Journal of Financial Economics}, 92(2), 153--181.

\bibitem[Hirshleifer \textit{et al.}(2009)]{hirshleifer2009driven}
Hirshleifer, D., Lim, S. S., \& Teoh, S. H. (2009).
\newblock Driven to distraction: Extraneous events and underreaction to earnings news.
\newblock \textit{The Journal of Finance}, 64(5), 2289--2325.

\bibitem[Hochreiter \& Schmidhuber(1997)]{hochreiter1997long}
Hochreiter, S., \& Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock \textit{Neural Computation}, 9(8), 1735--1780.

\bibitem[Hong \& Stein(2000)]{hong2000bad}
Hong, H., \& Stein, J. C. (2000).
\newblock Bad news travels slowly: Size, analyst coverage, and the profitability of momentum strategies.
\newblock \textit{The Journal of Finance}, 55(1), 265--295.

\bibitem[Jegadeesh \& Titman(1993)]{jegadeesh1993returns}
Jegadeesh, N., \& Titman, S. (1993).
\newblock Returns to buying winners and selling losers: Implications for stock market efficiency.
\newblock \textit{The Journal of Finance}, 48(1), 65--91.

\bibitem[Kara \textit{et al.}(2011)]{kara2011predicting}
Kara, Y., Boyacioglu, M. A., \& Baykan, {\"O}. K. (2011).
\newblock Predicting direction of stock price index movement using artificial neural networks and support vector machines: The sample of the Istanbul Stock Exchange.
\newblock \textit{Expert Systems with Applications}, 38(5), 5311--5319.

\bibitem[Kothari(2001)]{kothari2001capital}
Kothari, S. P. (2001).
\newblock Capital markets research in accounting.
\newblock \textit{Journal of Accounting and Economics}, 31(1-3), 105--231.

\bibitem[Lee \textit{et al.}(2012)]{lee2012high}
Lee, C. M., \& Ready, M. J. (2012).
\newblock Inferring trade direction from intraday data.
\newblock \textit{The Journal of Finance}, 46(2), 733--746.

\bibitem[Lim \textit{et al.}(2021)]{lim2021temporal}
Lim, B., Ar{\i}k, S. {\"O}., Loeff, N., \& Pfister, T. (2021).
\newblock Temporal fusion transformers for interpretable multi-horizon time series forecasting.
\newblock \textit{International Journal of Forecasting}, 37(4), 1748--1764.

\bibitem[Loughran \& McDonald(2011)]{loughran2011liability}
Loughran, T., \& McDonald, B. (2011).
\newblock When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks.
\newblock \textit{The Journal of Finance}, 66(1), 35--65.

\bibitem[Nie \textit{et al.}(2023)]{nie2023patchtst}
Nie, Y., Nguyen, N. H., Sinthong, P., \& Kalagnanam, J. (2023).
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock \textit{arXiv preprint arXiv:2211.14730}.

\bibitem[Patell(1976)]{patell1976corporate}
Patell, J. M. (1976).
\newblock Corporate forecasts of earnings per share and stock price behavior: Empirical test.
\newblock \textit{Journal of Accounting Research}, 14(2), 246--276.

\bibitem[Patel \textit{et al.}(2015)]{patel2015predicting}
Patel, J., Shah, S., Thakkar, P., \& Kotecha, K. (2015).
\newblock Predicting stock and stock price index movement using trend deterministic data preparation and machine learning techniques.
\newblock \textit{Expert Systems with Applications}, 42(1), 259--268.

\bibitem[Peress(2014)]{peress2014media}
Peress, J. (2014).
\newblock The media and the diffusion of information in financial markets: Evidence from newspaper strikes.
\newblock \textit{The Journal of Finance}, 69(5), 2007--2043.

\bibitem[Solomon \textit{et al.}(2012)]{solomon2012winners}
Solomon, D. H., Soltes, E., \& Sosyura, D. (2012).
\newblock Winners in the spotlight: Media coverage of fund holdings as a driver of flows.
\newblock \textit{Journal of Financial Economics}, 113(1), 53--72.

\bibitem[Tetlock(2007)]{tetlock2007giving}
Tetlock, P. C. (2007).
\newblock Giving content to investor sentiment: The role of media in the stock market.
\newblock \textit{The Journal of Finance}, 62(3), 1139--1168.

\bibitem[Tetlock \textit{et al.}(2008)]{tetlock2008more}
Tetlock, P. C., Saar-Tsechansky, M., \& Macskassy, S. (2008).
\newblock More than words: Quantifying language to measure firms' fundamentals.
\newblock \textit{The Journal of Finance}, 63(3), 1437--1467.

\bibitem[Vaswani \textit{et al.}(2017)]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \& Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock \textit{Advances in Neural Information Processing Systems}, 30.

\bibitem[Wu \textit{et al.}(2021)]{wu2021autoformer}
Wu, H., Xu, J., Wang, J., \& Long, M. (2021).
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \textit{Advances in Neural Information Processing Systems}, 34.

\bibitem[Xu \& Cohen(2018)]{xu2018stock}
Xu, Y., \& Cohen, S. B. (2018).
\newblock Stock movement prediction from tweets and historical prices.
\newblock \textit{Proceedings of ACL}, 1970--1979.

\bibitem[Zeng \textit{et al.}(2023)]{zeng2023transform}
Zeng, A., Chen, M., Zhang, L., \& Xu, Q. (2023).
\newblock Are transformers effective for time series forecasting?
\newblock \textit{Proceedings of AAAI}, 11121--11128.

\bibitem[Zhang \textit{et al.}(2023)]{zhang2023neural}
Zhang, L., Aggarwal, C., \& Kong, X. (2023).
\newblock A survey on neural network interpretability.
\newblock \textit{IEEE Transactions on Emerging Topics in Computational Intelligence}, 5(5), 726--742.

\bibitem[Zhou \textit{et al.}(2021)]{zhou2021informer}
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., \& Zhang, W. (2021).
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock \textit{Proceedings of AAAI}, 11106--11115.

\end{thebibliography}

\end{document}
